{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "denoising_Train_Unet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "fb421a8e04ba428395907f599f266033": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7f3f85ec6edc481499f4fa2eef36523b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_57ac1b7b3ae84b129993a072a39989e1",
              "IPY_MODEL_20eb8d148932485a8ee52c45b1dc9e42",
              "IPY_MODEL_0df90cd5582048b7ae3885a5e15a6500"
            ]
          }
        },
        "7f3f85ec6edc481499f4fa2eef36523b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57ac1b7b3ae84b129993a072a39989e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_2ec7c53db6494635b7cb6c644965a5c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4206f90c70d24196a6009f1491e9e995"
          }
        },
        "20eb8d148932485a8ee52c45b1dc9e42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_70719c96d0904b7baaae5c15e00bb664",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 36988158,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 36988158,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_439cec8ba7c8445890798a48c4ead29c"
          }
        },
        "0df90cd5582048b7ae3885a5e15a6500": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_3e21ef5fd8a6419c8890ad543088a1f8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 35.3M/35.3M [00:00&lt;00:00, 105MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3f337f52730746d9b5a441a4acb522d9"
          }
        },
        "2ec7c53db6494635b7cb6c644965a5c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4206f90c70d24196a6009f1491e9e995": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "70719c96d0904b7baaae5c15e00bb664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "439cec8ba7c8445890798a48c4ead29c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3e21ef5fd8a6419c8890ad543088a1f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3f337f52730746d9b5a441a4acb522d9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "negxzpN_iEth",
        "outputId": "65f29122-de08-4534-a822-a3ba07f99eed"
      },
      "source": [
        "!git clone https://github.com/Nomow/02456-deep-learning-image-denoising.git\n",
        "!pip install -U albumentations\n",
        "!pip install git+https://github.com/qubvel/segmentation_models.pytorch\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '02456-deep-learning-image-denoising'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 62 (delta 28), reused 44 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (62/62), done.\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 15.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Collecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.6 MB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (3.10.0.2)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.1)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.2.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
            "Installing collected packages: opencv-python-headless, qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.1.0 opencv-python-headless-4.5.4.60 qudida-0.0.4\n",
            "Collecting git+https://github.com/qubvel/segmentation_models.pytorch\n",
            "  Cloning https://github.com/qubvel/segmentation_models.pytorch to /tmp/pip-req-build-2spymett\n",
            "  Running command git clone -q https://github.com/qubvel/segmentation_models.pytorch /tmp/pip-req-build-2spymett\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.2.1) (0.11.1+cu111)\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.6 MB/s \n",
            "\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n",
            "  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 28.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.1) (1.10.0+cu111)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.1) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.1) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.1) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.1) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.1) (1.15.0)\n",
            "Building wheels for collected packages: segmentation-models-pytorch, efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for segmentation-models-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segmentation-models-pytorch: filename=segmentation_models_pytorch-0.2.1-py3-none-any.whl size=88599 sha256=3a7be47508273cbed2553daf1b11570a8daecc7fce78eade23c58ad7a32dbf5e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2kz7q9m9/wheels/fa/c5/a8/1e8af6cb04a0974db8a4a156ebd2fdd1d99ad2558d3fce49d4\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12421 sha256=987b9b68102b766d5c8f389cc8565bd86a83cd4923357b3285214160794651db\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=132d6526a515ac8cdcd523dbc017ecb515c237ec3460acc70a90befd26af5ece\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n",
            "Successfully built segmentation-models-pytorch efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.2.1 timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mnl3m6v2iQyP",
        "outputId": "ddf5621c-0501-482e-f9d9-0b56fc7e5d7f"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7FYrrn0ikFf",
        "outputId": "4ec0a219-b727-4317-ff34-5e9e8b99efa4"
      },
      "source": [
        "%cd /content/02456-deep-learning-image-denoising\n",
        "from pathlib import Path\n",
        "from dataset import AutoEncoderDataset\n",
        "from utils import train\n",
        "from utils import val\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as albu\n",
        "import segmentation_models_pytorch as smp\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations.pytorch\n",
        "import numpy as np\n",
        "\n",
        "root_path = Path('/content/gdrive/MyDrive/dermatology_dataset') \n",
        "train_path = root_path / 'train/'\n",
        "val_path = root_path / 'val/'\n",
        "test_path = root_path / 'test/'\n",
        "save_path = root_path / \"experiment_unet_no_dropout\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/02456-deep-learning-image-denoising\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY192aPyoxuE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "fb421a8e04ba428395907f599f266033",
            "7f3f85ec6edc481499f4fa2eef36523b",
            "57ac1b7b3ae84b129993a072a39989e1",
            "20eb8d148932485a8ee52c45b1dc9e42",
            "0df90cd5582048b7ae3885a5e15a6500",
            "2ec7c53db6494635b7cb6c644965a5c7",
            "4206f90c70d24196a6009f1491e9e995",
            "70719c96d0904b7baaae5c15e00bb664",
            "439cec8ba7c8445890798a48c4ead29c",
            "3e21ef5fd8a6419c8890ad543088a1f8",
            "3f337f52730746d9b5a441a4acb522d9"
          ]
        },
        "outputId": "a3b8f18d-8af3-46d2-f987-be12d34ddbad"
      },
      "source": [
        "img_max_size = 512\n",
        "batch_print = 1\n",
        "architecture = smp.Unet\n",
        "encoder = 'timm-regnetx_016'\n",
        "lr = 0.01\n",
        "lr_step_size = 70\n",
        "nb_epochs = 290\n",
        "batch_size = 15\n",
        "val_epoch = 5\n",
        "weight_path = os.path.join(\"/content/weights\", encoder + \"-\" + architecture.__name__)\n",
        "\n",
        "\n",
        "net = architecture(encoder_name=encoder,  # UnetPlusPlus\n",
        "    encoder_weights=\"imagenet\", \n",
        "    in_channels=3, \n",
        "    classes=3,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_016-65ca972a.pth\" to /root/.cache/torch/hub/checkpoints/regnetx_016-65ca972a.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fb421a8e04ba428395907f599f266033",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/35.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Elc6DwbbxsM3"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  net.cuda()\n",
        "  device = \"cuda:0\"\n",
        "else:\n",
        "  device = \"cpu\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8t1EeaSx1gD"
      },
      "source": [
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_step_size, gamma=0.1, last_epoch=-1, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXLGLMLJmcmE"
      },
      "source": [
        "train_transform = [\n",
        "    albu.Blur(blur_limit=10, always_apply=False, p=0.6),\n",
        "    albu.MultiplicativeNoise(multiplier=(0.7, 1.3), p=0.6),\n",
        "    albu.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.4, brightness_by_max=True, always_apply=False, p=0.6),\n",
        "    albu.Downscale(scale_min=0.25, scale_max=0.25, interpolation=0, always_apply=False, p=0.3),\n",
        "    albu.ISONoise(color_shift=(0.08, 0.15), intensity=(0.2, 0.3), always_apply=False, p=0.6),\n",
        "    albu.RandomToneCurve(scale=0.3, p=0.6)\n",
        "]\n",
        "\n",
        "train_transforms = albu.Compose(train_transform)\n",
        "\n",
        "\n",
        "preprocess_transform = [\n",
        "    albu.LongestMaxSize(max_size=img_max_size, always_apply=True),\n",
        "    albu.PadIfNeeded(min_height=img_max_size, min_width=img_max_size, always_apply=True, border_mode=0),    \n",
        "    albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
        "    albumentations.pytorch.transforms.ToTensorV2()]\n",
        "\n",
        "preprocess_transforms = albu.Compose(preprocess_transform)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmEGD3Glik75"
      },
      "source": [
        "train_dataset = AutoEncoderDataset(train_path, preprocess_transforms, train_transforms)\n",
        "val_dataset = AutoEncoderDataset(val_path, preprocess_transforms, train_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09N_fbf2rb0o"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIR6nXT2llM_",
        "outputId": "6b62e99f-35ca-48cd-fa00-f1d033ed2c6c"
      },
      "source": [
        "logs = {}\n",
        "logs[\"train\"] = []\n",
        "logs[\"val\"] = []\n",
        "best_loss = np.inf\n",
        "best_idx = 0\n",
        "for i in range(nb_epochs):\n",
        "  print(\"epoch: \", i + 1)\n",
        "  net = train(net, criterion, optimizer, scheduler, train_loader, batch_print, device)\n",
        "  if((i % val_epoch) == 0):\n",
        "    train_loss = val(net, criterion, train_loader, device)\n",
        "    tmp_train = {\"epoch\" : i, \"loss\" : train_loss}\n",
        "    logs[\"train\"].append(tmp_train)\n",
        "    print(\"train loss: \", train_loss)\n",
        "\n",
        "    \n",
        "    val_loss = val(net, criterion, val_loader, device)\n",
        "    tmp_val = {\"epoch\" : i, \"loss\" : val_loss}\n",
        "    logs[\"val\"].append(tmp_val)\n",
        "    print(\"val loss: \", val_loss)\n",
        "    if(val_loss < best_loss):\n",
        "      best_loss = val_loss\n",
        "      torch.save(net.state_dict(), os.path.join(save_path, \"best.pth\"))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[   18] loss: 0.219\n",
            "[   19] loss: 0.646\n",
            "epoch:  46\n",
            "[    1] loss: 0.199\n",
            "[    2] loss: 0.210\n",
            "[    3] loss: 0.177\n",
            "[    4] loss: 0.165\n",
            "[    5] loss: 0.219\n",
            "[    6] loss: 0.173\n",
            "[    7] loss: 0.141\n",
            "[    8] loss: 0.260\n",
            "[    9] loss: 0.221\n",
            "[   10] loss: 0.186\n",
            "[   11] loss: 0.185\n",
            "[   12] loss: 0.230\n",
            "[   13] loss: 0.191\n",
            "[   14] loss: 0.187\n",
            "[   15] loss: 0.196\n",
            "[   16] loss: 0.163\n",
            "[   17] loss: 0.159\n",
            "[   18] loss: 0.176\n",
            "[   19] loss: 0.310\n",
            "train loss:  0.15573946641319814\n",
            "val loss:  0.21410376578569412\n",
            "epoch:  47\n",
            "[    1] loss: 0.189\n",
            "[    2] loss: 0.126\n",
            "[    3] loss: 0.148\n",
            "[    4] loss: 0.117\n",
            "[    5] loss: 0.169\n",
            "[    6] loss: 0.199\n",
            "[    7] loss: 0.124\n",
            "[    8] loss: 0.193\n",
            "[    9] loss: 0.231\n",
            "[   10] loss: 0.170\n",
            "[   11] loss: 0.168\n",
            "[   12] loss: 0.145\n",
            "[   13] loss: 0.165\n",
            "[   14] loss: 0.182\n",
            "[   15] loss: 0.203\n",
            "[   16] loss: 0.207\n",
            "[   17] loss: 0.122\n",
            "[   18] loss: 0.180\n",
            "[   19] loss: 0.114\n",
            "epoch:  48\n",
            "[    1] loss: 0.131\n",
            "[    2] loss: 0.196\n",
            "[    3] loss: 0.160\n",
            "[    4] loss: 0.173\n",
            "[    5] loss: 0.155\n",
            "[    6] loss: 0.177\n",
            "[    7] loss: 0.177\n",
            "[    8] loss: 0.158\n",
            "[    9] loss: 0.112\n",
            "[   10] loss: 0.203\n",
            "[   11] loss: 0.173\n",
            "[   12] loss: 0.147\n",
            "[   13] loss: 0.182\n",
            "[   14] loss: 0.203\n",
            "[   15] loss: 0.246\n",
            "[   16] loss: 0.143\n",
            "[   17] loss: 0.173\n",
            "[   18] loss: 0.186\n",
            "[   19] loss: 0.476\n",
            "epoch:  49\n",
            "[    1] loss: 0.180\n",
            "[    2] loss: 0.138\n",
            "[    3] loss: 0.138\n",
            "[    4] loss: 0.176\n",
            "[    5] loss: 0.158\n",
            "[    6] loss: 0.142\n",
            "[    7] loss: 0.218\n",
            "[    8] loss: 0.221\n",
            "[    9] loss: 0.121\n",
            "[   10] loss: 0.190\n",
            "[   11] loss: 0.124\n",
            "[   12] loss: 0.135\n",
            "[   13] loss: 0.141\n",
            "[   14] loss: 0.214\n",
            "[   15] loss: 0.220\n",
            "[   16] loss: 0.287\n",
            "[   17] loss: 0.168\n",
            "[   18] loss: 0.120\n",
            "[   19] loss: 0.128\n",
            "epoch:  50\n",
            "[    1] loss: 0.158\n",
            "[    2] loss: 0.161\n",
            "[    3] loss: 0.198\n",
            "[    4] loss: 0.122\n",
            "[    5] loss: 0.129\n",
            "[    6] loss: 0.177\n",
            "[    7] loss: 0.139\n",
            "[    8] loss: 0.173\n",
            "[    9] loss: 0.132\n",
            "[   10] loss: 0.153\n",
            "[   11] loss: 0.197\n",
            "[   12] loss: 0.121\n",
            "[   13] loss: 0.116\n",
            "[   14] loss: 0.243\n",
            "[   15] loss: 0.132\n",
            "[   16] loss: 0.182\n",
            "[   17] loss: 0.266\n",
            "[   18] loss: 0.233\n",
            "[   19] loss: 0.591\n",
            "epoch:  51\n",
            "[    1] loss: 0.158\n",
            "[    2] loss: 0.133\n",
            "[    3] loss: 0.161\n",
            "[    4] loss: 0.177\n",
            "[    5] loss: 0.133\n",
            "[    6] loss: 0.161\n",
            "[    7] loss: 0.163\n",
            "[    8] loss: 0.152\n",
            "[    9] loss: 0.148\n",
            "[   10] loss: 0.143\n",
            "[   11] loss: 0.142\n",
            "[   12] loss: 0.118\n",
            "[   13] loss: 0.143\n",
            "[   14] loss: 0.149\n",
            "[   15] loss: 0.154\n",
            "[   16] loss: 0.158\n",
            "[   17] loss: 0.161\n",
            "[   18] loss: 0.131\n",
            "[   19] loss: 0.329\n",
            "train loss:  0.13379840758245656\n",
            "val loss:  0.1931322067975998\n",
            "epoch:  52\n",
            "[    1] loss: 0.181\n",
            "[    2] loss: 0.124\n",
            "[    3] loss: 0.183\n",
            "[    4] loss: 0.131\n",
            "[    5] loss: 0.136\n",
            "[    6] loss: 0.185\n",
            "[    7] loss: 0.184\n",
            "[    8] loss: 0.165\n",
            "[    9] loss: 0.146\n",
            "[   10] loss: 0.159\n",
            "[   11] loss: 0.135\n",
            "[   12] loss: 0.171\n",
            "[   13] loss: 0.199\n",
            "[   14] loss: 0.149\n",
            "[   15] loss: 0.177\n",
            "[   16] loss: 0.146\n",
            "[   17] loss: 0.093\n",
            "[   18] loss: 0.103\n",
            "[   19] loss: 0.340\n",
            "epoch:  53\n",
            "[    1] loss: 0.255\n",
            "[    2] loss: 0.133\n",
            "[    3] loss: 0.165\n",
            "[    4] loss: 0.173\n",
            "[    5] loss: 0.171\n",
            "[    6] loss: 0.161\n",
            "[    7] loss: 0.215\n",
            "[    8] loss: 0.192\n",
            "[    9] loss: 0.172\n",
            "[   10] loss: 0.180\n",
            "[   11] loss: 0.104\n",
            "[   12] loss: 0.118\n",
            "[   13] loss: 0.115\n",
            "[   14] loss: 0.212\n",
            "[   15] loss: 0.209\n",
            "[   16] loss: 0.174\n",
            "[   17] loss: 0.179\n",
            "[   18] loss: 0.154\n",
            "[   19] loss: 0.361\n",
            "epoch:  54\n",
            "[    1] loss: 0.145\n",
            "[    2] loss: 0.182\n",
            "[    3] loss: 0.184\n",
            "[    4] loss: 0.103\n",
            "[    5] loss: 0.144\n",
            "[    6] loss: 0.121\n",
            "[    7] loss: 0.135\n",
            "[    8] loss: 0.137\n",
            "[    9] loss: 0.181\n",
            "[   10] loss: 0.223\n",
            "[   11] loss: 0.140\n",
            "[   12] loss: 0.131\n",
            "[   13] loss: 0.173\n",
            "[   14] loss: 0.174\n",
            "[   15] loss: 0.122\n",
            "[   16] loss: 0.263\n",
            "[   17] loss: 0.194\n",
            "[   18] loss: 0.180\n",
            "[   19] loss: 0.116\n",
            "epoch:  55\n",
            "[    1] loss: 0.137\n",
            "[    2] loss: 0.112\n",
            "[    3] loss: 0.165\n",
            "[    4] loss: 0.243\n",
            "[    5] loss: 0.143\n",
            "[    6] loss: 0.217\n",
            "[    7] loss: 0.182\n",
            "[    8] loss: 0.270\n",
            "[    9] loss: 0.141\n",
            "[   10] loss: 0.127\n",
            "[   11] loss: 0.210\n",
            "[   12] loss: 0.121\n",
            "[   13] loss: 0.154\n",
            "[   14] loss: 0.137\n",
            "[   15] loss: 0.183\n",
            "[   16] loss: 0.157\n",
            "[   17] loss: 0.132\n",
            "[   18] loss: 0.162\n",
            "[   19] loss: 0.510\n",
            "epoch:  56\n",
            "[    1] loss: 0.175\n",
            "[    2] loss: 0.120\n",
            "[    3] loss: 0.133\n",
            "[    4] loss: 0.185\n",
            "[    5] loss: 0.157\n",
            "[    6] loss: 0.142\n",
            "[    7] loss: 0.139\n",
            "[    8] loss: 0.214\n",
            "[    9] loss: 0.217\n",
            "[   10] loss: 0.126\n",
            "[   11] loss: 0.182\n",
            "[   12] loss: 0.180\n",
            "[   13] loss: 0.212\n",
            "[   14] loss: 0.157\n",
            "[   15] loss: 0.173\n",
            "[   16] loss: 0.121\n",
            "[   17] loss: 0.137\n",
            "[   18] loss: 0.176\n",
            "[   19] loss: 0.129\n",
            "train loss:  0.12210377875496359\n",
            "val loss:  0.20444694347679615\n",
            "epoch:  57\n",
            "[    1] loss: 0.146\n",
            "[    2] loss: 0.204\n",
            "[    3] loss: 0.133\n",
            "[    4] loss: 0.121\n",
            "[    5] loss: 0.156\n",
            "[    6] loss: 0.108\n",
            "[    7] loss: 0.110\n",
            "[    8] loss: 0.126\n",
            "[    9] loss: 0.209\n",
            "[   10] loss: 0.203\n",
            "[   11] loss: 0.116\n",
            "[   12] loss: 0.130\n",
            "[   13] loss: 0.154\n",
            "[   14] loss: 0.128\n",
            "[   15] loss: 0.222\n",
            "[   16] loss: 0.171\n",
            "[   17] loss: 0.150\n",
            "[   18] loss: 0.139\n",
            "[   19] loss: 0.680\n",
            "epoch:  58\n",
            "[    1] loss: 0.124\n",
            "[    2] loss: 0.144\n",
            "[    3] loss: 0.200\n",
            "[    4] loss: 0.206\n",
            "[    5] loss: 0.173\n",
            "[    6] loss: 0.234\n",
            "[    7] loss: 0.129\n",
            "[    8] loss: 0.187\n",
            "[    9] loss: 0.154\n",
            "[   10] loss: 0.160\n",
            "[   11] loss: 0.145\n",
            "[   12] loss: 0.152\n",
            "[   13] loss: 0.159\n",
            "[   14] loss: 0.166\n",
            "[   15] loss: 0.121\n",
            "[   16] loss: 0.163\n",
            "[   17] loss: 0.113\n",
            "[   18] loss: 0.182\n",
            "[   19] loss: 0.277\n",
            "epoch:  59\n",
            "[    1] loss: 0.186\n",
            "[    2] loss: 0.149\n",
            "[    3] loss: 0.143\n",
            "[    4] loss: 0.122\n",
            "[    5] loss: 0.266\n",
            "[    6] loss: 0.179\n",
            "[    7] loss: 0.108\n",
            "[    8] loss: 0.174\n",
            "[    9] loss: 0.257\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.156\n",
            "[   12] loss: 0.134\n",
            "[   13] loss: 0.137\n",
            "[   14] loss: 0.137\n",
            "[   15] loss: 0.199\n",
            "[   16] loss: 0.160\n",
            "[   17] loss: 0.158\n",
            "[   18] loss: 0.191\n",
            "[   19] loss: 0.216\n",
            "epoch:  60\n",
            "[    1] loss: 0.171\n",
            "[    2] loss: 0.097\n",
            "[    3] loss: 0.176\n",
            "[    4] loss: 0.141\n",
            "[    5] loss: 0.171\n",
            "[    6] loss: 0.120\n",
            "[    7] loss: 0.193\n",
            "[    8] loss: 0.164\n",
            "[    9] loss: 0.133\n",
            "[   10] loss: 0.176\n",
            "[   11] loss: 0.202\n",
            "[   12] loss: 0.132\n",
            "[   13] loss: 0.113\n",
            "[   14] loss: 0.162\n",
            "[   15] loss: 0.229\n",
            "[   16] loss: 0.118\n",
            "[   17] loss: 0.147\n",
            "[   18] loss: 0.149\n",
            "[   19] loss: 0.315\n",
            "epoch:  61\n",
            "[    1] loss: 0.180\n",
            "[    2] loss: 0.127\n",
            "[    3] loss: 0.189\n",
            "[    4] loss: 0.175\n",
            "[    5] loss: 0.157\n",
            "[    6] loss: 0.135\n",
            "[    7] loss: 0.108\n",
            "[    8] loss: 0.186\n",
            "[    9] loss: 0.121\n",
            "[   10] loss: 0.122\n",
            "[   11] loss: 0.182\n",
            "[   12] loss: 0.147\n",
            "[   13] loss: 0.142\n",
            "[   14] loss: 0.148\n",
            "[   15] loss: 0.120\n",
            "[   16] loss: 0.163\n",
            "[   17] loss: 0.143\n",
            "[   18] loss: 0.174\n",
            "[   19] loss: 0.230\n",
            "train loss:  0.1280841866015073\n",
            "val loss:  0.1849763486534357\n",
            "epoch:  62\n",
            "[    1] loss: 0.129\n",
            "[    2] loss: 0.171\n",
            "[    3] loss: 0.144\n",
            "[    4] loss: 0.157\n",
            "[    5] loss: 0.142\n",
            "[    6] loss: 0.142\n",
            "[    7] loss: 0.102\n",
            "[    8] loss: 0.108\n",
            "[    9] loss: 0.175\n",
            "[   10] loss: 0.173\n",
            "[   11] loss: 0.208\n",
            "[   12] loss: 0.143\n",
            "[   13] loss: 0.119\n",
            "[   14] loss: 0.130\n",
            "[   15] loss: 0.161\n",
            "[   16] loss: 0.122\n",
            "[   17] loss: 0.139\n",
            "[   18] loss: 0.121\n",
            "[   19] loss: 0.119\n",
            "epoch:  63\n",
            "[    1] loss: 0.197\n",
            "[    2] loss: 0.122\n",
            "[    3] loss: 0.131\n",
            "[    4] loss: 0.179\n",
            "[    5] loss: 0.100\n",
            "[    6] loss: 0.149\n",
            "[    7] loss: 0.114\n",
            "[    8] loss: 0.124\n",
            "[    9] loss: 0.129\n",
            "[   10] loss: 0.114\n",
            "[   11] loss: 0.158\n",
            "[   12] loss: 0.173\n",
            "[   13] loss: 0.162\n",
            "[   14] loss: 0.149\n",
            "[   15] loss: 0.154\n",
            "[   16] loss: 0.153\n",
            "[   17] loss: 0.230\n",
            "[   18] loss: 0.148\n",
            "[   19] loss: 0.177\n",
            "epoch:  64\n",
            "[    1] loss: 0.175\n",
            "[    2] loss: 0.177\n",
            "[    3] loss: 0.170\n",
            "[    4] loss: 0.146\n",
            "[    5] loss: 0.118\n",
            "[    6] loss: 0.178\n",
            "[    7] loss: 0.170\n",
            "[    8] loss: 0.174\n",
            "[    9] loss: 0.154\n",
            "[   10] loss: 0.168\n",
            "[   11] loss: 0.173\n",
            "[   12] loss: 0.173\n",
            "[   13] loss: 0.118\n",
            "[   14] loss: 0.155\n",
            "[   15] loss: 0.134\n",
            "[   16] loss: 0.159\n",
            "[   17] loss: 0.141\n",
            "[   18] loss: 0.104\n",
            "[   19] loss: 1.249\n",
            "epoch:  65\n",
            "[    1] loss: 0.149\n",
            "[    2] loss: 0.159\n",
            "[    3] loss: 0.166\n",
            "[    4] loss: 0.297\n",
            "[    5] loss: 0.374\n",
            "[    6] loss: 0.270\n",
            "[    7] loss: 0.234\n",
            "[    8] loss: 0.181\n",
            "[    9] loss: 0.241\n",
            "[   10] loss: 0.141\n",
            "[   11] loss: 0.164\n",
            "[   12] loss: 0.260\n",
            "[   13] loss: 0.157\n",
            "[   14] loss: 0.234\n",
            "[   15] loss: 0.155\n",
            "[   16] loss: 0.158\n",
            "[   17] loss: 0.139\n",
            "[   18] loss: 0.154\n",
            "[   19] loss: 0.233\n",
            "epoch:  66\n",
            "[    1] loss: 0.172\n",
            "[    2] loss: 0.135\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.104\n",
            "[    5] loss: 0.100\n",
            "[    6] loss: 0.113\n",
            "[    7] loss: 0.164\n",
            "[    8] loss: 0.165\n",
            "[    9] loss: 0.208\n",
            "[   10] loss: 0.205\n",
            "[   11] loss: 0.159\n",
            "[   12] loss: 0.239\n",
            "[   13] loss: 0.135\n",
            "[   14] loss: 0.236\n",
            "[   15] loss: 0.143\n",
            "[   16] loss: 0.119\n",
            "[   17] loss: 0.155\n",
            "[   18] loss: 0.180\n",
            "[   19] loss: 0.626\n",
            "train loss:  0.13590019486625404\n",
            "val loss:  0.211398271843791\n",
            "epoch:  67\n",
            "[    1] loss: 0.178\n",
            "[    2] loss: 0.217\n",
            "[    3] loss: 0.133\n",
            "[    4] loss: 0.176\n",
            "[    5] loss: 0.204\n",
            "[    6] loss: 0.128\n",
            "[    7] loss: 0.204\n",
            "[    8] loss: 0.123\n",
            "[    9] loss: 0.140\n",
            "[   10] loss: 0.238\n",
            "[   11] loss: 0.110\n",
            "[   12] loss: 0.221\n",
            "[   13] loss: 0.124\n",
            "[   14] loss: 0.159\n",
            "[   15] loss: 0.231\n",
            "[   16] loss: 0.164\n",
            "[   17] loss: 0.136\n",
            "[   18] loss: 0.122\n",
            "[   19] loss: 0.829\n",
            "epoch:  68\n",
            "[    1] loss: 0.132\n",
            "[    2] loss: 0.238\n",
            "[    3] loss: 0.148\n",
            "[    4] loss: 0.123\n",
            "[    5] loss: 0.251\n",
            "[    6] loss: 0.135\n",
            "[    7] loss: 0.123\n",
            "[    8] loss: 0.175\n",
            "[    9] loss: 0.176\n",
            "[   10] loss: 0.163\n",
            "[   11] loss: 0.148\n",
            "[   12] loss: 0.235\n",
            "[   13] loss: 0.113\n",
            "[   14] loss: 0.140\n",
            "[   15] loss: 0.193\n",
            "[   16] loss: 0.140\n",
            "[   17] loss: 0.192\n",
            "[   18] loss: 0.149\n",
            "[   19] loss: 0.159\n",
            "epoch:  69\n",
            "[    1] loss: 0.171\n",
            "[    2] loss: 0.126\n",
            "[    3] loss: 0.141\n",
            "[    4] loss: 0.159\n",
            "[    5] loss: 0.231\n",
            "[    6] loss: 0.218\n",
            "[    7] loss: 0.246\n",
            "[    8] loss: 0.135\n",
            "[    9] loss: 0.227\n",
            "[   10] loss: 0.106\n",
            "[   11] loss: 0.124\n",
            "[   12] loss: 0.186\n",
            "[   13] loss: 0.132\n",
            "[   14] loss: 0.195\n",
            "[   15] loss: 0.147\n",
            "[   16] loss: 0.216\n",
            "[   17] loss: 0.185\n",
            "[   18] loss: 0.183\n",
            "[   19] loss: 0.268\n",
            "epoch:  70\n",
            "[    1] loss: 0.118\n",
            "[    2] loss: 0.130\n",
            "[    3] loss: 0.094\n",
            "[    4] loss: 0.135\n",
            "[    5] loss: 0.201\n",
            "[    6] loss: 0.209\n",
            "[    7] loss: 0.137\n",
            "[    8] loss: 0.134\n",
            "[    9] loss: 0.206\n",
            "[   10] loss: 0.104\n",
            "[   11] loss: 0.146\n",
            "[   12] loss: 0.125\n",
            "[   13] loss: 0.149\n",
            "[   14] loss: 0.168\n",
            "[   15] loss: 0.141\n",
            "[   16] loss: 0.135\n",
            "[   17] loss: 0.144\n",
            "[   18] loss: 0.125\n",
            "[   19] loss: 0.494\n",
            "epoch:  71\n",
            "[    1] loss: 0.130\n",
            "[    2] loss: 0.115\n",
            "[    3] loss: 0.136\n",
            "[    4] loss: 0.087\n",
            "[    5] loss: 0.122\n",
            "[    6] loss: 0.163\n",
            "[    7] loss: 0.081\n",
            "[    8] loss: 0.134\n",
            "[    9] loss: 0.100\n",
            "[   10] loss: 0.132\n",
            "[   11] loss: 0.131\n",
            "[   12] loss: 0.141\n",
            "[   13] loss: 0.127\n",
            "[   14] loss: 0.154\n",
            "[   15] loss: 0.111\n",
            "[   16] loss: 0.141\n",
            "[   17] loss: 0.174\n",
            "[   18] loss: 0.136\n",
            "[   19] loss: 0.244\n",
            "train loss:  0.1097919741535888\n",
            "val loss:  0.20831044018268585\n",
            "epoch:  72\n",
            "[    1] loss: 0.127\n",
            "[    2] loss: 0.136\n",
            "[    3] loss: 0.139\n",
            "[    4] loss: 0.167\n",
            "[    5] loss: 0.207\n",
            "[    6] loss: 0.171\n",
            "[    7] loss: 0.122\n",
            "[    8] loss: 0.125\n",
            "[    9] loss: 0.189\n",
            "[   10] loss: 0.117\n",
            "[   11] loss: 0.142\n",
            "[   12] loss: 0.107\n",
            "[   13] loss: 0.109\n",
            "[   14] loss: 0.184\n",
            "[   15] loss: 0.100\n",
            "[   16] loss: 0.148\n",
            "[   17] loss: 0.109\n",
            "[   18] loss: 0.157\n",
            "[   19] loss: 0.454\n",
            "epoch:  73\n",
            "[    1] loss: 0.101\n",
            "[    2] loss: 0.113\n",
            "[    3] loss: 0.152\n",
            "[    4] loss: 0.097\n",
            "[    5] loss: 0.177\n",
            "[    6] loss: 0.148\n",
            "[    7] loss: 0.107\n",
            "[    8] loss: 0.147\n",
            "[    9] loss: 0.124\n",
            "[   10] loss: 0.116\n",
            "[   11] loss: 0.106\n",
            "[   12] loss: 0.084\n",
            "[   13] loss: 0.115\n",
            "[   14] loss: 0.142\n",
            "[   15] loss: 0.185\n",
            "[   16] loss: 0.133\n",
            "[   17] loss: 0.153\n",
            "[   18] loss: 0.171\n",
            "[   19] loss: 0.321\n",
            "epoch:  74\n",
            "[    1] loss: 0.096\n",
            "[    2] loss: 0.115\n",
            "[    3] loss: 0.135\n",
            "[    4] loss: 0.135\n",
            "[    5] loss: 0.193\n",
            "[    6] loss: 0.144\n",
            "[    7] loss: 0.135\n",
            "[    8] loss: 0.125\n",
            "[    9] loss: 0.123\n",
            "[   10] loss: 0.129\n",
            "[   11] loss: 0.099\n",
            "[   12] loss: 0.187\n",
            "[   13] loss: 0.140\n",
            "[   14] loss: 0.142\n",
            "[   15] loss: 0.105\n",
            "[   16] loss: 0.146\n",
            "[   17] loss: 0.149\n",
            "[   18] loss: 0.129\n",
            "[   19] loss: 0.122\n",
            "epoch:  75\n",
            "[    1] loss: 0.113\n",
            "[    2] loss: 0.093\n",
            "[    3] loss: 0.122\n",
            "[    4] loss: 0.120\n",
            "[    5] loss: 0.149\n",
            "[    6] loss: 0.106\n",
            "[    7] loss: 0.092\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.135\n",
            "[   10] loss: 0.086\n",
            "[   11] loss: 0.147\n",
            "[   12] loss: 0.115\n",
            "[   13] loss: 0.119\n",
            "[   14] loss: 0.108\n",
            "[   15] loss: 0.115\n",
            "[   16] loss: 0.141\n",
            "[   17] loss: 0.115\n",
            "[   18] loss: 0.159\n",
            "[   19] loss: 0.657\n",
            "epoch:  76\n",
            "[    1] loss: 0.084\n",
            "[    2] loss: 0.121\n",
            "[    3] loss: 0.157\n",
            "[    4] loss: 0.117\n",
            "[    5] loss: 0.130\n",
            "[    6] loss: 0.113\n",
            "[    7] loss: 0.156\n",
            "[    8] loss: 0.121\n",
            "[    9] loss: 0.128\n",
            "[   10] loss: 0.091\n",
            "[   11] loss: 0.137\n",
            "[   12] loss: 0.113\n",
            "[   13] loss: 0.105\n",
            "[   14] loss: 0.120\n",
            "[   15] loss: 0.139\n",
            "[   16] loss: 0.182\n",
            "[   17] loss: 0.168\n",
            "[   18] loss: 0.113\n",
            "[   19] loss: 0.178\n",
            "train loss:  0.11131750367691412\n",
            "val loss:  0.1826956868171692\n",
            "epoch:  77\n",
            "[    1] loss: 0.127\n",
            "[    2] loss: 0.135\n",
            "[    3] loss: 0.127\n",
            "[    4] loss: 0.102\n",
            "[    5] loss: 0.107\n",
            "[    6] loss: 0.137\n",
            "[    7] loss: 0.106\n",
            "[    8] loss: 0.142\n",
            "[    9] loss: 0.121\n",
            "[   10] loss: 0.121\n",
            "[   11] loss: 0.114\n",
            "[   12] loss: 0.102\n",
            "[   13] loss: 0.095\n",
            "[   14] loss: 0.096\n",
            "[   15] loss: 0.156\n",
            "[   16] loss: 0.097\n",
            "[   17] loss: 0.118\n",
            "[   18] loss: 0.163\n",
            "[   19] loss: 0.086\n",
            "epoch:  78\n",
            "[    1] loss: 0.115\n",
            "[    2] loss: 0.093\n",
            "[    3] loss: 0.134\n",
            "[    4] loss: 0.101\n",
            "[    5] loss: 0.203\n",
            "[    6] loss: 0.113\n",
            "[    7] loss: 0.136\n",
            "[    8] loss: 0.150\n",
            "[    9] loss: 0.093\n",
            "[   10] loss: 0.159\n",
            "[   11] loss: 0.138\n",
            "[   12] loss: 0.164\n",
            "[   13] loss: 0.100\n",
            "[   14] loss: 0.135\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.136\n",
            "[   17] loss: 0.138\n",
            "[   18] loss: 0.134\n",
            "[   19] loss: 0.397\n",
            "epoch:  79\n",
            "[    1] loss: 0.097\n",
            "[    2] loss: 0.169\n",
            "[    3] loss: 0.100\n",
            "[    4] loss: 0.147\n",
            "[    5] loss: 0.132\n",
            "[    6] loss: 0.125\n",
            "[    7] loss: 0.127\n",
            "[    8] loss: 0.113\n",
            "[    9] loss: 0.121\n",
            "[   10] loss: 0.137\n",
            "[   11] loss: 0.119\n",
            "[   12] loss: 0.137\n",
            "[   13] loss: 0.098\n",
            "[   14] loss: 0.139\n",
            "[   15] loss: 0.106\n",
            "[   16] loss: 0.126\n",
            "[   17] loss: 0.132\n",
            "[   18] loss: 0.122\n",
            "[   19] loss: 0.418\n",
            "epoch:  80\n",
            "[    1] loss: 0.200\n",
            "[    2] loss: 0.102\n",
            "[    3] loss: 0.101\n",
            "[    4] loss: 0.093\n",
            "[    5] loss: 0.101\n",
            "[    6] loss: 0.184\n",
            "[    7] loss: 0.101\n",
            "[    8] loss: 0.138\n",
            "[    9] loss: 0.104\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.176\n",
            "[   12] loss: 0.089\n",
            "[   13] loss: 0.106\n",
            "[   14] loss: 0.084\n",
            "[   15] loss: 0.103\n",
            "[   16] loss: 0.170\n",
            "[   17] loss: 0.106\n",
            "[   18] loss: 0.214\n",
            "[   19] loss: 0.385\n",
            "epoch:  81\n",
            "[    1] loss: 0.108\n",
            "[    2] loss: 0.134\n",
            "[    3] loss: 0.146\n",
            "[    4] loss: 0.127\n",
            "[    5] loss: 0.117\n",
            "[    6] loss: 0.142\n",
            "[    7] loss: 0.108\n",
            "[    8] loss: 0.180\n",
            "[    9] loss: 0.121\n",
            "[   10] loss: 0.106\n",
            "[   11] loss: 0.092\n",
            "[   12] loss: 0.088\n",
            "[   13] loss: 0.129\n",
            "[   14] loss: 0.223\n",
            "[   15] loss: 0.088\n",
            "[   16] loss: 0.158\n",
            "[   17] loss: 0.133\n",
            "[   18] loss: 0.241\n",
            "[   19] loss: 0.126\n",
            "train loss:  0.10589001793414354\n",
            "val loss:  0.1919909119606018\n",
            "epoch:  82\n",
            "[    1] loss: 0.105\n",
            "[    2] loss: 0.148\n",
            "[    3] loss: 0.099\n",
            "[    4] loss: 0.130\n",
            "[    5] loss: 0.127\n",
            "[    6] loss: 0.105\n",
            "[    7] loss: 0.138\n",
            "[    8] loss: 0.090\n",
            "[    9] loss: 0.204\n",
            "[   10] loss: 0.113\n",
            "[   11] loss: 0.165\n",
            "[   12] loss: 0.130\n",
            "[   13] loss: 0.141\n",
            "[   14] loss: 0.099\n",
            "[   15] loss: 0.091\n",
            "[   16] loss: 0.095\n",
            "[   17] loss: 0.193\n",
            "[   18] loss: 0.119\n",
            "[   19] loss: 0.215\n",
            "epoch:  83\n",
            "[    1] loss: 0.113\n",
            "[    2] loss: 0.149\n",
            "[    3] loss: 0.109\n",
            "[    4] loss: 0.115\n",
            "[    5] loss: 0.118\n",
            "[    6] loss: 0.132\n",
            "[    7] loss: 0.097\n",
            "[    8] loss: 0.121\n",
            "[    9] loss: 0.161\n",
            "[   10] loss: 0.147\n",
            "[   11] loss: 0.109\n",
            "[   12] loss: 0.103\n",
            "[   13] loss: 0.095\n",
            "[   14] loss: 0.140\n",
            "[   15] loss: 0.151\n",
            "[   16] loss: 0.106\n",
            "[   17] loss: 0.124\n",
            "[   18] loss: 0.117\n",
            "[   19] loss: 0.629\n",
            "epoch:  84\n",
            "[    1] loss: 0.094\n",
            "[    2] loss: 0.112\n",
            "[    3] loss: 0.091\n",
            "[    4] loss: 0.130\n",
            "[    5] loss: 0.079\n",
            "[    6] loss: 0.155\n",
            "[    7] loss: 0.113\n",
            "[    8] loss: 0.100\n",
            "[    9] loss: 0.113\n",
            "[   10] loss: 0.131\n",
            "[   11] loss: 0.139\n",
            "[   12] loss: 0.124\n",
            "[   13] loss: 0.169\n",
            "[   14] loss: 0.122\n",
            "[   15] loss: 0.154\n",
            "[   16] loss: 0.142\n",
            "[   17] loss: 0.132\n",
            "[   18] loss: 0.137\n",
            "[   19] loss: 0.304\n",
            "epoch:  85\n",
            "[    1] loss: 0.145\n",
            "[    2] loss: 0.174\n",
            "[    3] loss: 0.113\n",
            "[    4] loss: 0.172\n",
            "[    5] loss: 0.149\n",
            "[    6] loss: 0.105\n",
            "[    7] loss: 0.126\n",
            "[    8] loss: 0.096\n",
            "[    9] loss: 0.150\n",
            "[   10] loss: 0.113\n",
            "[   11] loss: 0.170\n",
            "[   12] loss: 0.130\n",
            "[   13] loss: 0.258\n",
            "[   14] loss: 0.086\n",
            "[   15] loss: 0.155\n",
            "[   16] loss: 0.123\n",
            "[   17] loss: 0.122\n",
            "[   18] loss: 0.111\n",
            "[   19] loss: 0.157\n",
            "epoch:  86\n",
            "[    1] loss: 0.092\n",
            "[    2] loss: 0.099\n",
            "[    3] loss: 0.146\n",
            "[    4] loss: 0.120\n",
            "[    5] loss: 0.158\n",
            "[    6] loss: 0.092\n",
            "[    7] loss: 0.153\n",
            "[    8] loss: 0.104\n",
            "[    9] loss: 0.112\n",
            "[   10] loss: 0.156\n",
            "[   11] loss: 0.118\n",
            "[   12] loss: 0.135\n",
            "[   13] loss: 0.104\n",
            "[   14] loss: 0.131\n",
            "[   15] loss: 0.118\n",
            "[   16] loss: 0.147\n",
            "[   17] loss: 0.118\n",
            "[   18] loss: 0.140\n",
            "[   19] loss: 0.515\n",
            "train loss:  0.1011149517589194\n",
            "val loss:  0.1914273388683796\n",
            "epoch:  87\n",
            "[    1] loss: 0.092\n",
            "[    2] loss: 0.138\n",
            "[    3] loss: 0.101\n",
            "[    4] loss: 0.107\n",
            "[    5] loss: 0.091\n",
            "[    6] loss: 0.160\n",
            "[    7] loss: 0.122\n",
            "[    8] loss: 0.133\n",
            "[    9] loss: 0.161\n",
            "[   10] loss: 0.085\n",
            "[   11] loss: 0.122\n",
            "[   12] loss: 0.083\n",
            "[   13] loss: 0.106\n",
            "[   14] loss: 0.093\n",
            "[   15] loss: 0.145\n",
            "[   16] loss: 0.130\n",
            "[   17] loss: 0.166\n",
            "[   18] loss: 0.120\n",
            "[   19] loss: 0.197\n",
            "epoch:  88\n",
            "[    1] loss: 0.110\n",
            "[    2] loss: 0.136\n",
            "[    3] loss: 0.118\n",
            "[    4] loss: 0.132\n",
            "[    5] loss: 0.134\n",
            "[    6] loss: 0.118\n",
            "[    7] loss: 0.095\n",
            "[    8] loss: 0.108\n",
            "[    9] loss: 0.126\n",
            "[   10] loss: 0.152\n",
            "[   11] loss: 0.155\n",
            "[   12] loss: 0.120\n",
            "[   13] loss: 0.097\n",
            "[   14] loss: 0.154\n",
            "[   15] loss: 0.148\n",
            "[   16] loss: 0.097\n",
            "[   17] loss: 0.181\n",
            "[   18] loss: 0.118\n",
            "[   19] loss: 0.384\n",
            "epoch:  89\n",
            "[    1] loss: 0.142\n",
            "[    2] loss: 0.110\n",
            "[    3] loss: 0.159\n",
            "[    4] loss: 0.140\n",
            "[    5] loss: 0.114\n",
            "[    6] loss: 0.133\n",
            "[    7] loss: 0.188\n",
            "[    8] loss: 0.136\n",
            "[    9] loss: 0.163\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.183\n",
            "[   12] loss: 0.114\n",
            "[   13] loss: 0.151\n",
            "[   14] loss: 0.099\n",
            "[   15] loss: 0.107\n",
            "[   16] loss: 0.102\n",
            "[   17] loss: 0.135\n",
            "[   18] loss: 0.228\n",
            "[   19] loss: 0.458\n",
            "epoch:  90\n",
            "[    1] loss: 0.087\n",
            "[    2] loss: 0.149\n",
            "[    3] loss: 0.131\n",
            "[    4] loss: 0.101\n",
            "[    5] loss: 0.116\n",
            "[    6] loss: 0.145\n",
            "[    7] loss: 0.116\n",
            "[    8] loss: 0.218\n",
            "[    9] loss: 0.116\n",
            "[   10] loss: 0.146\n",
            "[   11] loss: 0.108\n",
            "[   12] loss: 0.114\n",
            "[   13] loss: 0.115\n",
            "[   14] loss: 0.148\n",
            "[   15] loss: 0.172\n",
            "[   16] loss: 0.102\n",
            "[   17] loss: 0.102\n",
            "[   18] loss: 0.146\n",
            "[   19] loss: 0.103\n",
            "epoch:  91\n",
            "[    1] loss: 0.100\n",
            "[    2] loss: 0.197\n",
            "[    3] loss: 0.162\n",
            "[    4] loss: 0.157\n",
            "[    5] loss: 0.076\n",
            "[    6] loss: 0.086\n",
            "[    7] loss: 0.131\n",
            "[    8] loss: 0.100\n",
            "[    9] loss: 0.157\n",
            "[   10] loss: 0.110\n",
            "[   11] loss: 0.111\n",
            "[   12] loss: 0.117\n",
            "[   13] loss: 0.104\n",
            "[   14] loss: 0.118\n",
            "[   15] loss: 0.121\n",
            "[   16] loss: 0.120\n",
            "[   17] loss: 0.122\n",
            "[   18] loss: 0.127\n",
            "[   19] loss: 0.292\n",
            "train loss:  0.10655907700386118\n",
            "val loss:  0.20142197608947754\n",
            "epoch:  92\n",
            "[    1] loss: 0.163\n",
            "[    2] loss: 0.133\n",
            "[    3] loss: 0.138\n",
            "[    4] loss: 0.100\n",
            "[    5] loss: 0.117\n",
            "[    6] loss: 0.081\n",
            "[    7] loss: 0.119\n",
            "[    8] loss: 0.127\n",
            "[    9] loss: 0.092\n",
            "[   10] loss: 0.124\n",
            "[   11] loss: 0.114\n",
            "[   12] loss: 0.104\n",
            "[   13] loss: 0.119\n",
            "[   14] loss: 0.096\n",
            "[   15] loss: 0.151\n",
            "[   16] loss: 0.085\n",
            "[   17] loss: 0.128\n",
            "[   18] loss: 0.102\n",
            "[   19] loss: 0.279\n",
            "epoch:  93\n",
            "[    1] loss: 0.115\n",
            "[    2] loss: 0.098\n",
            "[    3] loss: 0.159\n",
            "[    4] loss: 0.137\n",
            "[    5] loss: 0.124\n",
            "[    6] loss: 0.105\n",
            "[    7] loss: 0.145\n",
            "[    8] loss: 0.129\n",
            "[    9] loss: 0.079\n",
            "[   10] loss: 0.162\n",
            "[   11] loss: 0.152\n",
            "[   12] loss: 0.130\n",
            "[   13] loss: 0.126\n",
            "[   14] loss: 0.125\n",
            "[   15] loss: 0.100\n",
            "[   16] loss: 0.128\n",
            "[   17] loss: 0.077\n",
            "[   18] loss: 0.078\n",
            "[   19] loss: 0.205\n",
            "epoch:  94\n",
            "[    1] loss: 0.110\n",
            "[    2] loss: 0.108\n",
            "[    3] loss: 0.108\n",
            "[    4] loss: 0.151\n",
            "[    5] loss: 0.110\n",
            "[    6] loss: 0.141\n",
            "[    7] loss: 0.127\n",
            "[    8] loss: 0.119\n",
            "[    9] loss: 0.123\n",
            "[   10] loss: 0.110\n",
            "[   11] loss: 0.129\n",
            "[   12] loss: 0.132\n",
            "[   13] loss: 0.138\n",
            "[   14] loss: 0.144\n",
            "[   15] loss: 0.123\n",
            "[   16] loss: 0.104\n",
            "[   17] loss: 0.187\n",
            "[   18] loss: 0.130\n",
            "[   19] loss: 0.177\n",
            "epoch:  95\n",
            "[    1] loss: 0.128\n",
            "[    2] loss: 0.087\n",
            "[    3] loss: 0.106\n",
            "[    4] loss: 0.144\n",
            "[    5] loss: 0.103\n",
            "[    6] loss: 0.175\n",
            "[    7] loss: 0.103\n",
            "[    8] loss: 0.107\n",
            "[    9] loss: 0.124\n",
            "[   10] loss: 0.135\n",
            "[   11] loss: 0.147\n",
            "[   12] loss: 0.150\n",
            "[   13] loss: 0.129\n",
            "[   14] loss: 0.094\n",
            "[   15] loss: 0.097\n",
            "[   16] loss: 0.103\n",
            "[   17] loss: 0.168\n",
            "[   18] loss: 0.131\n",
            "[   19] loss: 0.612\n",
            "epoch:  96\n",
            "[    1] loss: 0.136\n",
            "[    2] loss: 0.121\n",
            "[    3] loss: 0.139\n",
            "[    4] loss: 0.122\n",
            "[    5] loss: 0.112\n",
            "[    6] loss: 0.090\n",
            "[    7] loss: 0.100\n",
            "[    8] loss: 0.147\n",
            "[    9] loss: 0.129\n",
            "[   10] loss: 0.149\n",
            "[   11] loss: 0.147\n",
            "[   12] loss: 0.097\n",
            "[   13] loss: 0.070\n",
            "[   14] loss: 0.115\n",
            "[   15] loss: 0.127\n",
            "[   16] loss: 0.157\n",
            "[   17] loss: 0.138\n",
            "[   18] loss: 0.113\n",
            "[   19] loss: 0.260\n",
            "train loss:  0.10467370139325366\n",
            "val loss:  0.18233379162847996\n",
            "epoch:  97\n",
            "[    1] loss: 0.135\n",
            "[    2] loss: 0.125\n",
            "[    3] loss: 0.088\n",
            "[    4] loss: 0.223\n",
            "[    5] loss: 0.148\n",
            "[    6] loss: 0.133\n",
            "[    7] loss: 0.101\n",
            "[    8] loss: 0.134\n",
            "[    9] loss: 0.109\n",
            "[   10] loss: 0.120\n",
            "[   11] loss: 0.206\n",
            "[   12] loss: 0.145\n",
            "[   13] loss: 0.116\n",
            "[   14] loss: 0.136\n",
            "[   15] loss: 0.109\n",
            "[   16] loss: 0.110\n",
            "[   17] loss: 0.098\n",
            "[   18] loss: 0.079\n",
            "[   19] loss: 0.085\n",
            "epoch:  98\n",
            "[    1] loss: 0.083\n",
            "[    2] loss: 0.117\n",
            "[    3] loss: 0.124\n",
            "[    4] loss: 0.136\n",
            "[    5] loss: 0.117\n",
            "[    6] loss: 0.105\n",
            "[    7] loss: 0.102\n",
            "[    8] loss: 0.129\n",
            "[    9] loss: 0.116\n",
            "[   10] loss: 0.106\n",
            "[   11] loss: 0.144\n",
            "[   12] loss: 0.093\n",
            "[   13] loss: 0.081\n",
            "[   14] loss: 0.134\n",
            "[   15] loss: 0.092\n",
            "[   16] loss: 0.129\n",
            "[   17] loss: 0.136\n",
            "[   18] loss: 0.117\n",
            "[   19] loss: 0.272\n",
            "epoch:  99\n",
            "[    1] loss: 0.101\n",
            "[    2] loss: 0.128\n",
            "[    3] loss: 0.079\n",
            "[    4] loss: 0.103\n",
            "[    5] loss: 0.145\n",
            "[    6] loss: 0.138\n",
            "[    7] loss: 0.126\n",
            "[    8] loss: 0.148\n",
            "[    9] loss: 0.116\n",
            "[   10] loss: 0.108\n",
            "[   11] loss: 0.096\n",
            "[   12] loss: 0.149\n",
            "[   13] loss: 0.122\n",
            "[   14] loss: 0.142\n",
            "[   15] loss: 0.112\n",
            "[   16] loss: 0.130\n",
            "[   17] loss: 0.173\n",
            "[   18] loss: 0.145\n",
            "[   19] loss: 0.072\n",
            "epoch:  100\n",
            "[    1] loss: 0.124\n",
            "[    2] loss: 0.200\n",
            "[    3] loss: 0.115\n",
            "[    4] loss: 0.125\n",
            "[    5] loss: 0.142\n",
            "[    6] loss: 0.105\n",
            "[    7] loss: 0.120\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.116\n",
            "[   10] loss: 0.108\n",
            "[   11] loss: 0.086\n",
            "[   12] loss: 0.103\n",
            "[   13] loss: 0.126\n",
            "[   14] loss: 0.130\n",
            "[   15] loss: 0.103\n",
            "[   16] loss: 0.132\n",
            "[   17] loss: 0.123\n",
            "[   18] loss: 0.120\n",
            "[   19] loss: 0.905\n",
            "epoch:  101\n",
            "[    1] loss: 0.137\n",
            "[    2] loss: 0.147\n",
            "[    3] loss: 0.085\n",
            "[    4] loss: 0.167\n",
            "[    5] loss: 0.109\n",
            "[    6] loss: 0.108\n",
            "[    7] loss: 0.140\n",
            "[    8] loss: 0.094\n",
            "[    9] loss: 0.117\n",
            "[   10] loss: 0.134\n",
            "[   11] loss: 0.130\n",
            "[   12] loss: 0.098\n",
            "[   13] loss: 0.173\n",
            "[   14] loss: 0.110\n",
            "[   15] loss: 0.095\n",
            "[   16] loss: 0.122\n",
            "[   17] loss: 0.120\n",
            "[   18] loss: 0.229\n",
            "[   19] loss: 0.243\n",
            "train loss:  0.10338060215444249\n",
            "val loss:  0.1724250316619873\n",
            "epoch:  102\n",
            "[    1] loss: 0.109\n",
            "[    2] loss: 0.150\n",
            "[    3] loss: 0.107\n",
            "[    4] loss: 0.166\n",
            "[    5] loss: 0.111\n",
            "[    6] loss: 0.133\n",
            "[    7] loss: 0.110\n",
            "[    8] loss: 0.110\n",
            "[    9] loss: 0.138\n",
            "[   10] loss: 0.090\n",
            "[   11] loss: 0.167\n",
            "[   12] loss: 0.101\n",
            "[   13] loss: 0.154\n",
            "[   14] loss: 0.105\n",
            "[   15] loss: 0.088\n",
            "[   16] loss: 0.122\n",
            "[   17] loss: 0.120\n",
            "[   18] loss: 0.115\n",
            "[   19] loss: 1.153\n",
            "epoch:  103\n",
            "[    1] loss: 0.145\n",
            "[    2] loss: 0.164\n",
            "[    3] loss: 0.081\n",
            "[    4] loss: 0.093\n",
            "[    5] loss: 0.091\n",
            "[    6] loss: 0.115\n",
            "[    7] loss: 0.093\n",
            "[    8] loss: 0.140\n",
            "[    9] loss: 0.095\n",
            "[   10] loss: 0.120\n",
            "[   11] loss: 0.174\n",
            "[   12] loss: 0.119\n",
            "[   13] loss: 0.121\n",
            "[   14] loss: 0.120\n",
            "[   15] loss: 0.088\n",
            "[   16] loss: 0.161\n",
            "[   17] loss: 0.135\n",
            "[   18] loss: 0.108\n",
            "[   19] loss: 0.541\n",
            "epoch:  104\n",
            "[    1] loss: 0.114\n",
            "[    2] loss: 0.100\n",
            "[    3] loss: 0.151\n",
            "[    4] loss: 0.082\n",
            "[    5] loss: 0.139\n",
            "[    6] loss: 0.109\n",
            "[    7] loss: 0.131\n",
            "[    8] loss: 0.129\n",
            "[    9] loss: 0.114\n",
            "[   10] loss: 0.154\n",
            "[   11] loss: 0.123\n",
            "[   12] loss: 0.099\n",
            "[   13] loss: 0.124\n",
            "[   14] loss: 0.120\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.124\n",
            "[   17] loss: 0.118\n",
            "[   18] loss: 0.163\n",
            "[   19] loss: 0.466\n",
            "epoch:  105\n",
            "[    1] loss: 0.127\n",
            "[    2] loss: 0.107\n",
            "[    3] loss: 0.108\n",
            "[    4] loss: 0.171\n",
            "[    5] loss: 0.090\n",
            "[    6] loss: 0.128\n",
            "[    7] loss: 0.142\n",
            "[    8] loss: 0.139\n",
            "[    9] loss: 0.107\n",
            "[   10] loss: 0.095\n",
            "[   11] loss: 0.117\n",
            "[   12] loss: 0.115\n",
            "[   13] loss: 0.121\n",
            "[   14] loss: 0.119\n",
            "[   15] loss: 0.142\n",
            "[   16] loss: 0.106\n",
            "[   17] loss: 0.132\n",
            "[   18] loss: 0.117\n",
            "[   19] loss: 0.297\n",
            "epoch:  106\n",
            "[    1] loss: 0.114\n",
            "[    2] loss: 0.119\n",
            "[    3] loss: 0.131\n",
            "[    4] loss: 0.121\n",
            "[    5] loss: 0.125\n",
            "[    6] loss: 0.143\n",
            "[    7] loss: 0.094\n",
            "[    8] loss: 0.171\n",
            "[    9] loss: 0.170\n",
            "[   10] loss: 0.107\n",
            "[   11] loss: 0.132\n",
            "[   12] loss: 0.082\n",
            "[   13] loss: 0.124\n",
            "[   14] loss: 0.132\n",
            "[   15] loss: 0.082\n",
            "[   16] loss: 0.141\n",
            "[   17] loss: 0.114\n",
            "[   18] loss: 0.100\n",
            "[   19] loss: 0.478\n",
            "train loss:  0.09445542828453814\n",
            "val loss:  0.18139107152819633\n",
            "epoch:  107\n",
            "[    1] loss: 0.094\n",
            "[    2] loss: 0.111\n",
            "[    3] loss: 0.081\n",
            "[    4] loss: 0.134\n",
            "[    5] loss: 0.071\n",
            "[    6] loss: 0.116\n",
            "[    7] loss: 0.117\n",
            "[    8] loss: 0.131\n",
            "[    9] loss: 0.111\n",
            "[   10] loss: 0.092\n",
            "[   11] loss: 0.114\n",
            "[   12] loss: 0.157\n",
            "[   13] loss: 0.117\n",
            "[   14] loss: 0.143\n",
            "[   15] loss: 0.201\n",
            "[   16] loss: 0.083\n",
            "[   17] loss: 0.138\n",
            "[   18] loss: 0.115\n",
            "[   19] loss: 0.223\n",
            "epoch:  108\n",
            "[    1] loss: 0.107\n",
            "[    2] loss: 0.153\n",
            "[    3] loss: 0.128\n",
            "[    4] loss: 0.207\n",
            "[    5] loss: 0.110\n",
            "[    6] loss: 0.091\n",
            "[    7] loss: 0.181\n",
            "[    8] loss: 0.108\n",
            "[    9] loss: 0.123\n",
            "[   10] loss: 0.182\n",
            "[   11] loss: 0.102\n",
            "[   12] loss: 0.091\n",
            "[   13] loss: 0.099\n",
            "[   14] loss: 0.121\n",
            "[   15] loss: 0.127\n",
            "[   16] loss: 0.103\n",
            "[   17] loss: 0.119\n",
            "[   18] loss: 0.106\n",
            "[   19] loss: 0.160\n",
            "epoch:  109\n",
            "[    1] loss: 0.117\n",
            "[    2] loss: 0.107\n",
            "[    3] loss: 0.115\n",
            "[    4] loss: 0.150\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.131\n",
            "[    7] loss: 0.098\n",
            "[    8] loss: 0.096\n",
            "[    9] loss: 0.107\n",
            "[   10] loss: 0.109\n",
            "[   11] loss: 0.101\n",
            "[   12] loss: 0.104\n",
            "[   13] loss: 0.124\n",
            "[   14] loss: 0.170\n",
            "[   15] loss: 0.094\n",
            "[   16] loss: 0.132\n",
            "[   17] loss: 0.098\n",
            "[   18] loss: 0.156\n",
            "[   19] loss: 0.155\n",
            "epoch:  110\n",
            "[    1] loss: 0.099\n",
            "[    2] loss: 0.137\n",
            "[    3] loss: 0.142\n",
            "[    4] loss: 0.107\n",
            "[    5] loss: 0.112\n",
            "[    6] loss: 0.140\n",
            "[    7] loss: 0.186\n",
            "[    8] loss: 0.159\n",
            "[    9] loss: 0.099\n",
            "[   10] loss: 0.101\n",
            "[   11] loss: 0.115\n",
            "[   12] loss: 0.155\n",
            "[   13] loss: 0.095\n",
            "[   14] loss: 0.109\n",
            "[   15] loss: 0.105\n",
            "[   16] loss: 0.100\n",
            "[   17] loss: 0.151\n",
            "[   18] loss: 0.111\n",
            "[   19] loss: 0.505\n",
            "epoch:  111\n",
            "[    1] loss: 0.154\n",
            "[    2] loss: 0.121\n",
            "[    3] loss: 0.123\n",
            "[    4] loss: 0.190\n",
            "[    5] loss: 0.110\n",
            "[    6] loss: 0.149\n",
            "[    7] loss: 0.129\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.176\n",
            "[   10] loss: 0.107\n",
            "[   11] loss: 0.108\n",
            "[   12] loss: 0.129\n",
            "[   13] loss: 0.087\n",
            "[   14] loss: 0.147\n",
            "[   15] loss: 0.090\n",
            "[   16] loss: 0.112\n",
            "[   17] loss: 0.099\n",
            "[   18] loss: 0.122\n",
            "[   19] loss: 0.174\n",
            "train loss:  0.0956989765742465\n",
            "val loss:  0.1807398982346058\n",
            "epoch:  112\n",
            "[    1] loss: 0.139\n",
            "[    2] loss: 0.126\n",
            "[    3] loss: 0.157\n",
            "[    4] loss: 0.127\n",
            "[    5] loss: 0.092\n",
            "[    6] loss: 0.117\n",
            "[    7] loss: 0.137\n",
            "[    8] loss: 0.121\n",
            "[    9] loss: 0.084\n",
            "[   10] loss: 0.178\n",
            "[   11] loss: 0.106\n",
            "[   12] loss: 0.115\n",
            "[   13] loss: 0.097\n",
            "[   14] loss: 0.177\n",
            "[   15] loss: 0.131\n",
            "[   16] loss: 0.099\n",
            "[   17] loss: 0.126\n",
            "[   18] loss: 0.103\n",
            "[   19] loss: 0.109\n",
            "epoch:  113\n",
            "[    1] loss: 0.090\n",
            "[    2] loss: 0.176\n",
            "[    3] loss: 0.119\n",
            "[    4] loss: 0.092\n",
            "[    5] loss: 0.105\n",
            "[    6] loss: 0.099\n",
            "[    7] loss: 0.161\n",
            "[    8] loss: 0.167\n",
            "[    9] loss: 0.131\n",
            "[   10] loss: 0.103\n",
            "[   11] loss: 0.141\n",
            "[   12] loss: 0.078\n",
            "[   13] loss: 0.116\n",
            "[   14] loss: 0.125\n",
            "[   15] loss: 0.079\n",
            "[   16] loss: 0.093\n",
            "[   17] loss: 0.124\n",
            "[   18] loss: 0.130\n",
            "[   19] loss: 0.429\n",
            "epoch:  114\n",
            "[    1] loss: 0.115\n",
            "[    2] loss: 0.174\n",
            "[    3] loss: 0.106\n",
            "[    4] loss: 0.092\n",
            "[    5] loss: 0.091\n",
            "[    6] loss: 0.101\n",
            "[    7] loss: 0.117\n",
            "[    8] loss: 0.149\n",
            "[    9] loss: 0.080\n",
            "[   10] loss: 0.151\n",
            "[   11] loss: 0.103\n",
            "[   12] loss: 0.102\n",
            "[   13] loss: 0.129\n",
            "[   14] loss: 0.153\n",
            "[   15] loss: 0.162\n",
            "[   16] loss: 0.118\n",
            "[   17] loss: 0.128\n",
            "[   18] loss: 0.156\n",
            "[   19] loss: 0.357\n",
            "epoch:  115\n",
            "[    1] loss: 0.099\n",
            "[    2] loss: 0.111\n",
            "[    3] loss: 0.127\n",
            "[    4] loss: 0.132\n",
            "[    5] loss: 0.192\n",
            "[    6] loss: 0.113\n",
            "[    7] loss: 0.102\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.106\n",
            "[   10] loss: 0.182\n",
            "[   11] loss: 0.092\n",
            "[   12] loss: 0.176\n",
            "[   13] loss: 0.061\n",
            "[   14] loss: 0.133\n",
            "[   15] loss: 0.131\n",
            "[   16] loss: 0.138\n",
            "[   17] loss: 0.119\n",
            "[   18] loss: 0.094\n",
            "[   19] loss: 0.071\n",
            "epoch:  116\n",
            "[    1] loss: 0.139\n",
            "[    2] loss: 0.096\n",
            "[    3] loss: 0.115\n",
            "[    4] loss: 0.128\n",
            "[    5] loss: 0.135\n",
            "[    6] loss: 0.140\n",
            "[    7] loss: 0.098\n",
            "[    8] loss: 0.146\n",
            "[    9] loss: 0.123\n",
            "[   10] loss: 0.118\n",
            "[   11] loss: 0.142\n",
            "[   12] loss: 0.106\n",
            "[   13] loss: 0.146\n",
            "[   14] loss: 0.094\n",
            "[   15] loss: 0.145\n",
            "[   16] loss: 0.100\n",
            "[   17] loss: 0.118\n",
            "[   18] loss: 0.105\n",
            "[   19] loss: 0.690\n",
            "train loss:  0.09811206164715044\n",
            "val loss:  0.16862296126782894\n",
            "epoch:  117\n",
            "[    1] loss: 0.099\n",
            "[    2] loss: 0.106\n",
            "[    3] loss: 0.105\n",
            "[    4] loss: 0.096\n",
            "[    5] loss: 0.111\n",
            "[    6] loss: 0.101\n",
            "[    7] loss: 0.115\n",
            "[    8] loss: 0.129\n",
            "[    9] loss: 0.122\n",
            "[   10] loss: 0.139\n",
            "[   11] loss: 0.119\n",
            "[   12] loss: 0.108\n",
            "[   13] loss: 0.097\n",
            "[   14] loss: 0.124\n",
            "[   15] loss: 0.135\n",
            "[   16] loss: 0.111\n",
            "[   17] loss: 0.115\n",
            "[   18] loss: 0.092\n",
            "[   19] loss: 0.195\n",
            "epoch:  118\n",
            "[    1] loss: 0.098\n",
            "[    2] loss: 0.104\n",
            "[    3] loss: 0.114\n",
            "[    4] loss: 0.114\n",
            "[    5] loss: 0.101\n",
            "[    6] loss: 0.102\n",
            "[    7] loss: 0.118\n",
            "[    8] loss: 0.109\n",
            "[    9] loss: 0.106\n",
            "[   10] loss: 0.079\n",
            "[   11] loss: 0.122\n",
            "[   12] loss: 0.124\n",
            "[   13] loss: 0.182\n",
            "[   14] loss: 0.196\n",
            "[   15] loss: 0.105\n",
            "[   16] loss: 0.109\n",
            "[   17] loss: 0.132\n",
            "[   18] loss: 0.134\n",
            "[   19] loss: 0.103\n",
            "epoch:  119\n",
            "[    1] loss: 0.119\n",
            "[    2] loss: 0.076\n",
            "[    3] loss: 0.108\n",
            "[    4] loss: 0.196\n",
            "[    5] loss: 0.157\n",
            "[    6] loss: 0.119\n",
            "[    7] loss: 0.099\n",
            "[    8] loss: 0.182\n",
            "[    9] loss: 0.182\n",
            "[   10] loss: 0.092\n",
            "[   11] loss: 0.116\n",
            "[   12] loss: 0.139\n",
            "[   13] loss: 0.110\n",
            "[   14] loss: 0.170\n",
            "[   15] loss: 0.104\n",
            "[   16] loss: 0.164\n",
            "[   17] loss: 0.119\n",
            "[   18] loss: 0.106\n",
            "[   19] loss: 0.800\n",
            "epoch:  120\n",
            "[    1] loss: 0.139\n",
            "[    2] loss: 0.118\n",
            "[    3] loss: 0.089\n",
            "[    4] loss: 0.107\n",
            "[    5] loss: 0.112\n",
            "[    6] loss: 0.095\n",
            "[    7] loss: 0.141\n",
            "[    8] loss: 0.095\n",
            "[    9] loss: 0.123\n",
            "[   10] loss: 0.124\n",
            "[   11] loss: 0.094\n",
            "[   12] loss: 0.123\n",
            "[   13] loss: 0.118\n",
            "[   14] loss: 0.144\n",
            "[   15] loss: 0.120\n",
            "[   16] loss: 0.117\n",
            "[   17] loss: 0.122\n",
            "[   18] loss: 0.120\n",
            "[   19] loss: 0.121\n",
            "epoch:  121\n",
            "[    1] loss: 0.126\n",
            "[    2] loss: 0.128\n",
            "[    3] loss: 0.111\n",
            "[    4] loss: 0.134\n",
            "[    5] loss: 0.085\n",
            "[    6] loss: 0.097\n",
            "[    7] loss: 0.131\n",
            "[    8] loss: 0.090\n",
            "[    9] loss: 0.119\n",
            "[   10] loss: 0.094\n",
            "[   11] loss: 0.216\n",
            "[   12] loss: 0.094\n",
            "[   13] loss: 0.108\n",
            "[   14] loss: 0.113\n",
            "[   15] loss: 0.085\n",
            "[   16] loss: 0.130\n",
            "[   17] loss: 0.122\n",
            "[   18] loss: 0.086\n",
            "[   19] loss: 0.274\n",
            "train loss:  0.1045244262444184\n",
            "val loss:  0.1736663542687893\n",
            "epoch:  122\n",
            "[    1] loss: 0.099\n",
            "[    2] loss: 0.108\n",
            "[    3] loss: 0.120\n",
            "[    4] loss: 0.095\n",
            "[    5] loss: 0.107\n",
            "[    6] loss: 0.121\n",
            "[    7] loss: 0.134\n",
            "[    8] loss: 0.108\n",
            "[    9] loss: 0.101\n",
            "[   10] loss: 0.109\n",
            "[   11] loss: 0.101\n",
            "[   12] loss: 0.098\n",
            "[   13] loss: 0.134\n",
            "[   14] loss: 0.091\n",
            "[   15] loss: 0.164\n",
            "[   16] loss: 0.104\n",
            "[   17] loss: 0.151\n",
            "[   18] loss: 0.146\n",
            "[   19] loss: 0.372\n",
            "epoch:  123\n",
            "[    1] loss: 0.086\n",
            "[    2] loss: 0.095\n",
            "[    3] loss: 0.147\n",
            "[    4] loss: 0.128\n",
            "[    5] loss: 0.137\n",
            "[    6] loss: 0.110\n",
            "[    7] loss: 0.104\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.120\n",
            "[   10] loss: 0.099\n",
            "[   11] loss: 0.146\n",
            "[   12] loss: 0.218\n",
            "[   13] loss: 0.114\n",
            "[   14] loss: 0.125\n",
            "[   15] loss: 0.178\n",
            "[   16] loss: 0.080\n",
            "[   17] loss: 0.083\n",
            "[   18] loss: 0.106\n",
            "[   19] loss: 0.278\n",
            "epoch:  124\n",
            "[    1] loss: 0.096\n",
            "[    2] loss: 0.114\n",
            "[    3] loss: 0.118\n",
            "[    4] loss: 0.167\n",
            "[    5] loss: 0.082\n",
            "[    6] loss: 0.135\n",
            "[    7] loss: 0.128\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.100\n",
            "[   10] loss: 0.170\n",
            "[   11] loss: 0.158\n",
            "[   12] loss: 0.107\n",
            "[   13] loss: 0.095\n",
            "[   14] loss: 0.119\n",
            "[   15] loss: 0.114\n",
            "[   16] loss: 0.212\n",
            "[   17] loss: 0.103\n",
            "[   18] loss: 0.124\n",
            "[   19] loss: 0.312\n",
            "epoch:  125\n",
            "[    1] loss: 0.170\n",
            "[    2] loss: 0.137\n",
            "[    3] loss: 0.085\n",
            "[    4] loss: 0.183\n",
            "[    5] loss: 0.127\n",
            "[    6] loss: 0.097\n",
            "[    7] loss: 0.130\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.141\n",
            "[   10] loss: 0.233\n",
            "[   11] loss: 0.188\n",
            "[   12] loss: 0.191\n",
            "[   13] loss: 0.084\n",
            "[   14] loss: 0.085\n",
            "[   15] loss: 0.164\n",
            "[   16] loss: 0.126\n",
            "[   17] loss: 0.116\n",
            "[   18] loss: 0.180\n",
            "[   19] loss: 0.508\n",
            "epoch:  126\n",
            "[    1] loss: 0.173\n",
            "[    2] loss: 0.114\n",
            "[    3] loss: 0.110\n",
            "[    4] loss: 0.150\n",
            "[    5] loss: 0.095\n",
            "[    6] loss: 0.086\n",
            "[    7] loss: 0.106\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.096\n",
            "[   10] loss: 0.150\n",
            "[   11] loss: 0.101\n",
            "[   12] loss: 0.156\n",
            "[   13] loss: 0.114\n",
            "[   14] loss: 0.089\n",
            "[   15] loss: 0.127\n",
            "[   16] loss: 0.095\n",
            "[   17] loss: 0.139\n",
            "[   18] loss: 0.122\n",
            "[   19] loss: 0.160\n",
            "train loss:  0.09395743465489324\n",
            "val loss:  0.16886196099221706\n",
            "epoch:  127\n",
            "[    1] loss: 0.086\n",
            "[    2] loss: 0.240\n",
            "[    3] loss: 0.087\n",
            "[    4] loss: 0.105\n",
            "[    5] loss: 0.135\n",
            "[    6] loss: 0.147\n",
            "[    7] loss: 0.095\n",
            "[    8] loss: 0.100\n",
            "[    9] loss: 0.109\n",
            "[   10] loss: 0.071\n",
            "[   11] loss: 0.105\n",
            "[   12] loss: 0.131\n",
            "[   13] loss: 0.125\n",
            "[   14] loss: 0.103\n",
            "[   15] loss: 0.124\n",
            "[   16] loss: 0.140\n",
            "[   17] loss: 0.124\n",
            "[   18] loss: 0.095\n",
            "[   19] loss: 0.217\n",
            "epoch:  128\n",
            "[    1] loss: 0.120\n",
            "[    2] loss: 0.116\n",
            "[    3] loss: 0.117\n",
            "[    4] loss: 0.092\n",
            "[    5] loss: 0.094\n",
            "[    6] loss: 0.108\n",
            "[    7] loss: 0.224\n",
            "[    8] loss: 0.180\n",
            "[    9] loss: 0.118\n",
            "[   10] loss: 0.113\n",
            "[   11] loss: 0.092\n",
            "[   12] loss: 0.117\n",
            "[   13] loss: 0.129\n",
            "[   14] loss: 0.099\n",
            "[   15] loss: 0.119\n",
            "[   16] loss: 0.114\n",
            "[   17] loss: 0.113\n",
            "[   18] loss: 0.120\n",
            "[   19] loss: 0.250\n",
            "epoch:  129\n",
            "[    1] loss: 0.166\n",
            "[    2] loss: 0.120\n",
            "[    3] loss: 0.104\n",
            "[    4] loss: 0.128\n",
            "[    5] loss: 0.126\n",
            "[    6] loss: 0.137\n",
            "[    7] loss: 0.135\n",
            "[    8] loss: 0.161\n",
            "[    9] loss: 0.182\n",
            "[   10] loss: 0.109\n",
            "[   11] loss: 0.188\n",
            "[   12] loss: 0.113\n",
            "[   13] loss: 0.170\n",
            "[   14] loss: 0.158\n",
            "[   15] loss: 0.126\n",
            "[   16] loss: 0.123\n",
            "[   17] loss: 0.134\n",
            "[   18] loss: 0.125\n",
            "[   19] loss: 0.296\n",
            "epoch:  130\n",
            "[    1] loss: 0.126\n",
            "[    2] loss: 0.093\n",
            "[    3] loss: 0.126\n",
            "[    4] loss: 0.139\n",
            "[    5] loss: 0.139\n",
            "[    6] loss: 0.123\n",
            "[    7] loss: 0.106\n",
            "[    8] loss: 0.156\n",
            "[    9] loss: 0.099\n",
            "[   10] loss: 0.092\n",
            "[   11] loss: 0.163\n",
            "[   12] loss: 0.099\n",
            "[   13] loss: 0.102\n",
            "[   14] loss: 0.128\n",
            "[   15] loss: 0.116\n",
            "[   16] loss: 0.116\n",
            "[   17] loss: 0.086\n",
            "[   18] loss: 0.188\n",
            "[   19] loss: 0.245\n",
            "epoch:  131\n",
            "[    1] loss: 0.085\n",
            "[    2] loss: 0.090\n",
            "[    3] loss: 0.096\n",
            "[    4] loss: 0.094\n",
            "[    5] loss: 0.130\n",
            "[    6] loss: 0.063\n",
            "[    7] loss: 0.080\n",
            "[    8] loss: 0.084\n",
            "[    9] loss: 0.159\n",
            "[   10] loss: 0.113\n",
            "[   11] loss: 0.127\n",
            "[   12] loss: 0.101\n",
            "[   13] loss: 0.079\n",
            "[   14] loss: 0.134\n",
            "[   15] loss: 0.134\n",
            "[   16] loss: 0.158\n",
            "[   17] loss: 0.129\n",
            "[   18] loss: 0.131\n",
            "[   19] loss: 0.388\n",
            "train loss:  0.09211429230430547\n",
            "val loss:  0.1669755894690752\n",
            "epoch:  132\n",
            "[    1] loss: 0.107\n",
            "[    2] loss: 0.139\n",
            "[    3] loss: 0.147\n",
            "[    4] loss: 0.090\n",
            "[    5] loss: 0.090\n",
            "[    6] loss: 0.159\n",
            "[    7] loss: 0.079\n",
            "[    8] loss: 0.129\n",
            "[    9] loss: 0.091\n",
            "[   10] loss: 0.139\n",
            "[   11] loss: 0.104\n",
            "[   12] loss: 0.100\n",
            "[   13] loss: 0.114\n",
            "[   14] loss: 0.124\n",
            "[   15] loss: 0.085\n",
            "[   16] loss: 0.212\n",
            "[   17] loss: 0.119\n",
            "[   18] loss: 0.150\n",
            "[   19] loss: 0.220\n",
            "epoch:  133\n",
            "[    1] loss: 0.126\n",
            "[    2] loss: 0.102\n",
            "[    3] loss: 0.084\n",
            "[    4] loss: 0.094\n",
            "[    5] loss: 0.119\n",
            "[    6] loss: 0.101\n",
            "[    7] loss: 0.229\n",
            "[    8] loss: 0.101\n",
            "[    9] loss: 0.107\n",
            "[   10] loss: 0.111\n",
            "[   11] loss: 0.087\n",
            "[   12] loss: 0.128\n",
            "[   13] loss: 0.123\n",
            "[   14] loss: 0.133\n",
            "[   15] loss: 0.072\n",
            "[   16] loss: 0.085\n",
            "[   17] loss: 0.109\n",
            "[   18] loss: 0.115\n",
            "[   19] loss: 0.293\n",
            "epoch:  134\n",
            "[    1] loss: 0.105\n",
            "[    2] loss: 0.122\n",
            "[    3] loss: 0.131\n",
            "[    4] loss: 0.136\n",
            "[    5] loss: 0.126\n",
            "[    6] loss: 0.119\n",
            "[    7] loss: 0.158\n",
            "[    8] loss: 0.142\n",
            "[    9] loss: 0.134\n",
            "[   10] loss: 0.094\n",
            "[   11] loss: 0.117\n",
            "[   12] loss: 0.114\n",
            "[   13] loss: 0.097\n",
            "[   14] loss: 0.113\n",
            "[   15] loss: 0.099\n",
            "[   16] loss: 0.090\n",
            "[   17] loss: 0.119\n",
            "[   18] loss: 0.164\n",
            "[   19] loss: 0.211\n",
            "epoch:  135\n",
            "[    1] loss: 0.093\n",
            "[    2] loss: 0.110\n",
            "[    3] loss: 0.099\n",
            "[    4] loss: 0.080\n",
            "[    5] loss: 0.135\n",
            "[    6] loss: 0.106\n",
            "[    7] loss: 0.127\n",
            "[    8] loss: 0.123\n",
            "[    9] loss: 0.104\n",
            "[   10] loss: 0.123\n",
            "[   11] loss: 0.105\n",
            "[   12] loss: 0.114\n",
            "[   13] loss: 0.124\n",
            "[   14] loss: 0.091\n",
            "[   15] loss: 0.116\n",
            "[   16] loss: 0.112\n",
            "[   17] loss: 0.152\n",
            "[   18] loss: 0.106\n",
            "[   19] loss: 0.132\n",
            "epoch:  136\n",
            "[    1] loss: 0.127\n",
            "[    2] loss: 0.090\n",
            "[    3] loss: 0.131\n",
            "[    4] loss: 0.130\n",
            "[    5] loss: 0.101\n",
            "[    6] loss: 0.145\n",
            "[    7] loss: 0.102\n",
            "[    8] loss: 0.186\n",
            "[    9] loss: 0.095\n",
            "[   10] loss: 0.129\n",
            "[   11] loss: 0.154\n",
            "[   12] loss: 0.080\n",
            "[   13] loss: 0.107\n",
            "[   14] loss: 0.152\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.109\n",
            "[   17] loss: 0.112\n",
            "[   18] loss: 0.087\n",
            "[   19] loss: 0.155\n",
            "train loss:  0.09786916055771358\n",
            "val loss:  0.17076408676803112\n",
            "epoch:  137\n",
            "[    1] loss: 0.148\n",
            "[    2] loss: 0.089\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.117\n",
            "[    5] loss: 0.133\n",
            "[    6] loss: 0.115\n",
            "[    7] loss: 0.147\n",
            "[    8] loss: 0.110\n",
            "[    9] loss: 0.090\n",
            "[   10] loss: 0.084\n",
            "[   11] loss: 0.100\n",
            "[   12] loss: 0.106\n",
            "[   13] loss: 0.082\n",
            "[   14] loss: 0.124\n",
            "[   15] loss: 0.083\n",
            "[   16] loss: 0.103\n",
            "[   17] loss: 0.077\n",
            "[   18] loss: 0.138\n",
            "[   19] loss: 0.445\n",
            "epoch:  138\n",
            "[    1] loss: 0.106\n",
            "[    2] loss: 0.120\n",
            "[    3] loss: 0.112\n",
            "[    4] loss: 0.117\n",
            "[    5] loss: 0.117\n",
            "[    6] loss: 0.109\n",
            "[    7] loss: 0.100\n",
            "[    8] loss: 0.111\n",
            "[    9] loss: 0.109\n",
            "[   10] loss: 0.097\n",
            "[   11] loss: 0.090\n",
            "[   12] loss: 0.133\n",
            "[   13] loss: 0.114\n",
            "[   14] loss: 0.182\n",
            "[   15] loss: 0.098\n",
            "[   16] loss: 0.182\n",
            "[   17] loss: 0.104\n",
            "[   18] loss: 0.089\n",
            "[   19] loss: 0.106\n",
            "epoch:  139\n",
            "[    1] loss: 0.249\n",
            "[    2] loss: 0.110\n",
            "[    3] loss: 0.069\n",
            "[    4] loss: 0.086\n",
            "[    5] loss: 0.158\n",
            "[    6] loss: 0.128\n",
            "[    7] loss: 0.097\n",
            "[    8] loss: 0.127\n",
            "[    9] loss: 0.128\n",
            "[   10] loss: 0.120\n",
            "[   11] loss: 0.108\n",
            "[   12] loss: 0.152\n",
            "[   13] loss: 0.090\n",
            "[   14] loss: 0.164\n",
            "[   15] loss: 0.102\n",
            "[   16] loss: 0.142\n",
            "[   17] loss: 0.097\n",
            "[   18] loss: 0.150\n",
            "[   19] loss: 1.322\n",
            "epoch:  140\n",
            "[    1] loss: 0.101\n",
            "[    2] loss: 0.136\n",
            "[    3] loss: 0.134\n",
            "[    4] loss: 0.140\n",
            "[    5] loss: 0.148\n",
            "[    6] loss: 0.137\n",
            "[    7] loss: 0.143\n",
            "[    8] loss: 0.140\n",
            "[    9] loss: 0.082\n",
            "[   10] loss: 0.142\n",
            "[   11] loss: 0.080\n",
            "[   12] loss: 0.107\n",
            "[   13] loss: 0.153\n",
            "[   14] loss: 0.163\n",
            "[   15] loss: 0.085\n",
            "[   16] loss: 0.118\n",
            "[   17] loss: 0.165\n",
            "[   18] loss: 0.108\n",
            "[   19] loss: 0.129\n",
            "epoch:  141\n",
            "[    1] loss: 0.074\n",
            "[    2] loss: 0.144\n",
            "[    3] loss: 0.155\n",
            "[    4] loss: 0.127\n",
            "[    5] loss: 0.089\n",
            "[    6] loss: 0.109\n",
            "[    7] loss: 0.114\n",
            "[    8] loss: 0.088\n",
            "[    9] loss: 0.112\n",
            "[   10] loss: 0.100\n",
            "[   11] loss: 0.117\n",
            "[   12] loss: 0.122\n",
            "[   13] loss: 0.219\n",
            "[   14] loss: 0.103\n",
            "[   15] loss: 0.131\n",
            "[   16] loss: 0.096\n",
            "[   17] loss: 0.168\n",
            "[   18] loss: 0.095\n",
            "[   19] loss: 0.327\n",
            "train loss:  0.09720214963068857\n",
            "val loss:  0.15501562133431435\n",
            "epoch:  142\n",
            "[    1] loss: 0.210\n",
            "[    2] loss: 0.110\n",
            "[    3] loss: 0.101\n",
            "[    4] loss: 0.133\n",
            "[    5] loss: 0.109\n",
            "[    6] loss: 0.119\n",
            "[    7] loss: 0.096\n",
            "[    8] loss: 0.098\n",
            "[    9] loss: 0.124\n",
            "[   10] loss: 0.126\n",
            "[   11] loss: 0.094\n",
            "[   12] loss: 0.130\n",
            "[   13] loss: 0.202\n",
            "[   14] loss: 0.070\n",
            "[   15] loss: 0.079\n",
            "[   16] loss: 0.091\n",
            "[   17] loss: 0.143\n",
            "[   18] loss: 0.148\n",
            "[   19] loss: 0.195\n",
            "epoch:  143\n",
            "[    1] loss: 0.122\n",
            "[    2] loss: 0.150\n",
            "[    3] loss: 0.099\n",
            "[    4] loss: 0.130\n",
            "[    5] loss: 0.096\n",
            "[    6] loss: 0.150\n",
            "[    7] loss: 0.112\n",
            "[    8] loss: 0.126\n",
            "[    9] loss: 0.092\n",
            "[   10] loss: 0.125\n",
            "[   11] loss: 0.161\n",
            "[   12] loss: 0.102\n",
            "[   13] loss: 0.104\n",
            "[   14] loss: 0.083\n",
            "[   15] loss: 0.131\n",
            "[   16] loss: 0.099\n",
            "[   17] loss: 0.101\n",
            "[   18] loss: 0.155\n",
            "[   19] loss: 0.133\n",
            "epoch:  144\n",
            "[    1] loss: 0.145\n",
            "[    2] loss: 0.113\n",
            "[    3] loss: 0.121\n",
            "[    4] loss: 0.064\n",
            "[    5] loss: 0.093\n",
            "[    6] loss: 0.111\n",
            "[    7] loss: 0.125\n",
            "[    8] loss: 0.115\n",
            "[    9] loss: 0.116\n",
            "[   10] loss: 0.100\n",
            "[   11] loss: 0.118\n",
            "[   12] loss: 0.090\n",
            "[   13] loss: 0.136\n",
            "[   14] loss: 0.103\n",
            "[   15] loss: 0.096\n",
            "[   16] loss: 0.162\n",
            "[   17] loss: 0.160\n",
            "[   18] loss: 0.122\n",
            "[   19] loss: 0.189\n",
            "epoch:  145\n",
            "[    1] loss: 0.098\n",
            "[    2] loss: 0.102\n",
            "[    3] loss: 0.124\n",
            "[    4] loss: 0.113\n",
            "[    5] loss: 0.123\n",
            "[    6] loss: 0.119\n",
            "[    7] loss: 0.143\n",
            "[    8] loss: 0.130\n",
            "[    9] loss: 0.120\n",
            "[   10] loss: 0.097\n",
            "[   11] loss: 0.131\n",
            "[   12] loss: 0.098\n",
            "[   13] loss: 0.078\n",
            "[   14] loss: 0.104\n",
            "[   15] loss: 0.098\n",
            "[   16] loss: 0.104\n",
            "[   17] loss: 0.095\n",
            "[   18] loss: 0.111\n",
            "[   19] loss: 0.486\n",
            "epoch:  146\n",
            "[    1] loss: 0.172\n",
            "[    2] loss: 0.104\n",
            "[    3] loss: 0.120\n",
            "[    4] loss: 0.118\n",
            "[    5] loss: 0.118\n",
            "[    6] loss: 0.130\n",
            "[    7] loss: 0.101\n",
            "[    8] loss: 0.134\n",
            "[    9] loss: 0.117\n",
            "[   10] loss: 0.149\n",
            "[   11] loss: 0.086\n",
            "[   12] loss: 0.139\n",
            "[   13] loss: 0.102\n",
            "[   14] loss: 0.181\n",
            "[   15] loss: 0.094\n",
            "[   16] loss: 0.094\n",
            "[   17] loss: 0.139\n",
            "[   18] loss: 0.111\n",
            "[   19] loss: 0.590\n",
            "train loss:  0.09097610041499138\n",
            "val loss:  0.17845514975488186\n",
            "epoch:  147\n",
            "[    1] loss: 0.083\n",
            "[    2] loss: 0.128\n",
            "[    3] loss: 0.083\n",
            "[    4] loss: 0.151\n",
            "[    5] loss: 0.141\n",
            "[    6] loss: 0.142\n",
            "[    7] loss: 0.120\n",
            "[    8] loss: 0.147\n",
            "[    9] loss: 0.126\n",
            "[   10] loss: 0.142\n",
            "[   11] loss: 0.084\n",
            "[   12] loss: 0.127\n",
            "[   13] loss: 0.115\n",
            "[   14] loss: 0.145\n",
            "[   15] loss: 0.095\n",
            "[   16] loss: 0.122\n",
            "[   17] loss: 0.098\n",
            "[   18] loss: 0.132\n",
            "[   19] loss: 0.275\n",
            "epoch:  148\n",
            "[    1] loss: 0.120\n",
            "[    2] loss: 0.142\n",
            "[    3] loss: 0.109\n",
            "[    4] loss: 0.112\n",
            "[    5] loss: 0.136\n",
            "[    6] loss: 0.122\n",
            "[    7] loss: 0.128\n",
            "[    8] loss: 0.150\n",
            "[    9] loss: 0.103\n",
            "[   10] loss: 0.097\n",
            "[   11] loss: 0.082\n",
            "[   12] loss: 0.132\n",
            "[   13] loss: 0.099\n",
            "[   14] loss: 0.196\n",
            "[   15] loss: 0.125\n",
            "[   16] loss: 0.088\n",
            "[   17] loss: 0.093\n",
            "[   18] loss: 0.116\n",
            "[   19] loss: 0.143\n",
            "epoch:  149\n",
            "[    1] loss: 0.134\n",
            "[    2] loss: 0.096\n",
            "[    3] loss: 0.096\n",
            "[    4] loss: 0.131\n",
            "[    5] loss: 0.177\n",
            "[    6] loss: 0.084\n",
            "[    7] loss: 0.104\n",
            "[    8] loss: 0.107\n",
            "[    9] loss: 0.077\n",
            "[   10] loss: 0.085\n",
            "[   11] loss: 0.122\n",
            "[   12] loss: 0.083\n",
            "[   13] loss: 0.132\n",
            "[   14] loss: 0.093\n",
            "[   15] loss: 0.133\n",
            "[   16] loss: 0.118\n",
            "[   17] loss: 0.147\n",
            "[   18] loss: 0.123\n",
            "[   19] loss: 0.457\n",
            "epoch:  150\n",
            "[    1] loss: 0.114\n",
            "[    2] loss: 0.093\n",
            "[    3] loss: 0.125\n",
            "[    4] loss: 0.093\n",
            "[    5] loss: 0.173\n",
            "[    6] loss: 0.127\n",
            "[    7] loss: 0.100\n",
            "[    8] loss: 0.110\n",
            "[    9] loss: 0.139\n",
            "[   10] loss: 0.098\n",
            "[   11] loss: 0.101\n",
            "[   12] loss: 0.093\n",
            "[   13] loss: 0.107\n",
            "[   14] loss: 0.110\n",
            "[   15] loss: 0.148\n",
            "[   16] loss: 0.109\n",
            "[   17] loss: 0.113\n",
            "[   18] loss: 0.149\n",
            "[   19] loss: 0.447\n",
            "epoch:  151\n",
            "[    1] loss: 0.096\n",
            "[    2] loss: 0.082\n",
            "[    3] loss: 0.129\n",
            "[    4] loss: 0.146\n",
            "[    5] loss: 0.148\n",
            "[    6] loss: 0.121\n",
            "[    7] loss: 0.091\n",
            "[    8] loss: 0.103\n",
            "[    9] loss: 0.117\n",
            "[   10] loss: 0.104\n",
            "[   11] loss: 0.085\n",
            "[   12] loss: 0.124\n",
            "[   13] loss: 0.127\n",
            "[   14] loss: 0.168\n",
            "[   15] loss: 0.118\n",
            "[   16] loss: 0.102\n",
            "[   17] loss: 0.122\n",
            "[   18] loss: 0.119\n",
            "[   19] loss: 0.186\n",
            "train loss:  0.09586868955589392\n",
            "val loss:  0.16285103559494019\n",
            "epoch:  152\n",
            "[    1] loss: 0.083\n",
            "[    2] loss: 0.175\n",
            "[    3] loss: 0.114\n",
            "[    4] loss: 0.119\n",
            "[    5] loss: 0.167\n",
            "[    6] loss: 0.093\n",
            "[    7] loss: 0.121\n",
            "[    8] loss: 0.117\n",
            "[    9] loss: 0.098\n",
            "[   10] loss: 0.138\n",
            "[   11] loss: 0.093\n",
            "[   12] loss: 0.146\n",
            "[   13] loss: 0.090\n",
            "[   14] loss: 0.137\n",
            "[   15] loss: 0.130\n",
            "[   16] loss: 0.080\n",
            "[   17] loss: 0.115\n",
            "[   18] loss: 0.127\n",
            "[   19] loss: 0.102\n",
            "epoch:  153\n",
            "[    1] loss: 0.104\n",
            "[    2] loss: 0.094\n",
            "[    3] loss: 0.089\n",
            "[    4] loss: 0.135\n",
            "[    5] loss: 0.089\n",
            "[    6] loss: 0.141\n",
            "[    7] loss: 0.100\n",
            "[    8] loss: 0.133\n",
            "[    9] loss: 0.128\n",
            "[   10] loss: 0.121\n",
            "[   11] loss: 0.112\n",
            "[   12] loss: 0.098\n",
            "[   13] loss: 0.105\n",
            "[   14] loss: 0.094\n",
            "[   15] loss: 0.122\n",
            "[   16] loss: 0.101\n",
            "[   17] loss: 0.098\n",
            "[   18] loss: 0.093\n",
            "[   19] loss: 0.400\n",
            "epoch:  154\n",
            "[    1] loss: 0.110\n",
            "[    2] loss: 0.109\n",
            "[    3] loss: 0.145\n",
            "[    4] loss: 0.083\n",
            "[    5] loss: 0.113\n",
            "[    6] loss: 0.144\n",
            "[    7] loss: 0.203\n",
            "[    8] loss: 0.096\n",
            "[    9] loss: 0.119\n",
            "[   10] loss: 0.097\n",
            "[   11] loss: 0.118\n",
            "[   12] loss: 0.095\n",
            "[   13] loss: 0.129\n",
            "[   14] loss: 0.109\n",
            "[   15] loss: 0.114\n",
            "[   16] loss: 0.075\n",
            "[   17] loss: 0.093\n",
            "[   18] loss: 0.139\n",
            "[   19] loss: 0.292\n",
            "epoch:  155\n",
            "[    1] loss: 0.088\n",
            "[    2] loss: 0.111\n",
            "[    3] loss: 0.124\n",
            "[    4] loss: 0.115\n",
            "[    5] loss: 0.118\n",
            "[    6] loss: 0.151\n",
            "[    7] loss: 0.152\n",
            "[    8] loss: 0.174\n",
            "[    9] loss: 0.093\n",
            "[   10] loss: 0.115\n",
            "[   11] loss: 0.103\n",
            "[   12] loss: 0.147\n",
            "[   13] loss: 0.111\n",
            "[   14] loss: 0.128\n",
            "[   15] loss: 0.093\n",
            "[   16] loss: 0.105\n",
            "[   17] loss: 0.119\n",
            "[   18] loss: 0.125\n",
            "[   19] loss: 0.313\n",
            "epoch:  156\n",
            "[    1] loss: 0.089\n",
            "[    2] loss: 0.130\n",
            "[    3] loss: 0.096\n",
            "[    4] loss: 0.106\n",
            "[    5] loss: 0.111\n",
            "[    6] loss: 0.102\n",
            "[    7] loss: 0.083\n",
            "[    8] loss: 0.089\n",
            "[    9] loss: 0.117\n",
            "[   10] loss: 0.108\n",
            "[   11] loss: 0.145\n",
            "[   12] loss: 0.126\n",
            "[   13] loss: 0.143\n",
            "[   14] loss: 0.181\n",
            "[   15] loss: 0.109\n",
            "[   16] loss: 0.087\n",
            "[   17] loss: 0.093\n",
            "[   18] loss: 0.082\n",
            "[   19] loss: 0.234\n",
            "train loss:  0.09422661090160117\n",
            "val loss:  0.16302225645631552\n",
            "epoch:  157\n",
            "[    1] loss: 0.093\n",
            "[    2] loss: 0.117\n",
            "[    3] loss: 0.138\n",
            "[    4] loss: 0.092\n",
            "[    5] loss: 0.071\n",
            "[    6] loss: 0.093\n",
            "[    7] loss: 0.147\n",
            "[    8] loss: 0.091\n",
            "[    9] loss: 0.131\n",
            "[   10] loss: 0.136\n",
            "[   11] loss: 0.101\n",
            "[   12] loss: 0.081\n",
            "[   13] loss: 0.132\n",
            "[   14] loss: 0.125\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.111\n",
            "[   17] loss: 0.126\n",
            "[   18] loss: 0.202\n",
            "[   19] loss: 0.454\n",
            "epoch:  158\n",
            "[    1] loss: 0.100\n",
            "[    2] loss: 0.110\n",
            "[    3] loss: 0.111\n",
            "[    4] loss: 0.120\n",
            "[    5] loss: 0.147\n",
            "[    6] loss: 0.119\n",
            "[    7] loss: 0.082\n",
            "[    8] loss: 0.116\n",
            "[    9] loss: 0.168\n",
            "[   10] loss: 0.097\n",
            "[   11] loss: 0.114\n",
            "[   12] loss: 0.132\n",
            "[   13] loss: 0.101\n",
            "[   14] loss: 0.097\n",
            "[   15] loss: 0.119\n",
            "[   16] loss: 0.096\n",
            "[   17] loss: 0.118\n",
            "[   18] loss: 0.131\n",
            "[   19] loss: 0.666\n",
            "epoch:  159\n",
            "[    1] loss: 0.097\n",
            "[    2] loss: 0.131\n",
            "[    3] loss: 0.132\n",
            "[    4] loss: 0.160\n",
            "[    5] loss: 0.113\n",
            "[    6] loss: 0.125\n",
            "[    7] loss: 0.120\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.115\n",
            "[   10] loss: 0.152\n",
            "[   11] loss: 0.140\n",
            "[   12] loss: 0.097\n",
            "[   13] loss: 0.142\n",
            "[   14] loss: 0.096\n",
            "[   15] loss: 0.078\n",
            "[   16] loss: 0.094\n",
            "[   17] loss: 0.106\n",
            "[   18] loss: 0.110\n",
            "[   19] loss: 0.112\n",
            "epoch:  160\n",
            "[    1] loss: 0.116\n",
            "[    2] loss: 0.153\n",
            "[    3] loss: 0.144\n",
            "[    4] loss: 0.137\n",
            "[    5] loss: 0.084\n",
            "[    6] loss: 0.099\n",
            "[    7] loss: 0.162\n",
            "[    8] loss: 0.108\n",
            "[    9] loss: 0.132\n",
            "[   10] loss: 0.192\n",
            "[   11] loss: 0.122\n",
            "[   12] loss: 0.151\n",
            "[   13] loss: 0.096\n",
            "[   14] loss: 0.121\n",
            "[   15] loss: 0.089\n",
            "[   16] loss: 0.091\n",
            "[   17] loss: 0.098\n",
            "[   18] loss: 0.215\n",
            "[   19] loss: 0.150\n",
            "epoch:  161\n",
            "[    1] loss: 0.091\n",
            "[    2] loss: 0.099\n",
            "[    3] loss: 0.086\n",
            "[    4] loss: 0.088\n",
            "[    5] loss: 0.188\n",
            "[    6] loss: 0.110\n",
            "[    7] loss: 0.137\n",
            "[    8] loss: 0.103\n",
            "[    9] loss: 0.113\n",
            "[   10] loss: 0.111\n",
            "[   11] loss: 0.109\n",
            "[   12] loss: 0.076\n",
            "[   13] loss: 0.105\n",
            "[   14] loss: 0.138\n",
            "[   15] loss: 0.106\n",
            "[   16] loss: 0.090\n",
            "[   17] loss: 0.109\n",
            "[   18] loss: 0.112\n",
            "[   19] loss: 0.396\n",
            "train loss:  0.0943797598592937\n",
            "val loss:  0.16044706106185913\n",
            "epoch:  162\n",
            "[    1] loss: 0.106\n",
            "[    2] loss: 0.099\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.101\n",
            "[    5] loss: 0.101\n",
            "[    6] loss: 0.097\n",
            "[    7] loss: 0.154\n",
            "[    8] loss: 0.129\n",
            "[    9] loss: 0.180\n",
            "[   10] loss: 0.106\n",
            "[   11] loss: 0.127\n",
            "[   12] loss: 0.095\n",
            "[   13] loss: 0.138\n",
            "[   14] loss: 0.101\n",
            "[   15] loss: 0.084\n",
            "[   16] loss: 0.100\n",
            "[   17] loss: 0.108\n",
            "[   18] loss: 0.094\n",
            "[   19] loss: 0.412\n",
            "epoch:  163\n",
            "[    1] loss: 0.079\n",
            "[    2] loss: 0.110\n",
            "[    3] loss: 0.156\n",
            "[    4] loss: 0.082\n",
            "[    5] loss: 0.104\n",
            "[    6] loss: 0.082\n",
            "[    7] loss: 0.238\n",
            "[    8] loss: 0.197\n",
            "[    9] loss: 0.094\n",
            "[   10] loss: 0.144\n",
            "[   11] loss: 0.154\n",
            "[   12] loss: 0.143\n",
            "[   13] loss: 0.121\n",
            "[   14] loss: 0.177\n",
            "[   15] loss: 0.094\n",
            "[   16] loss: 0.151\n",
            "[   17] loss: 0.132\n",
            "[   18] loss: 0.098\n",
            "[   19] loss: 0.113\n",
            "epoch:  164\n",
            "[    1] loss: 0.147\n",
            "[    2] loss: 0.108\n",
            "[    3] loss: 0.113\n",
            "[    4] loss: 0.091\n",
            "[    5] loss: 0.088\n",
            "[    6] loss: 0.125\n",
            "[    7] loss: 0.125\n",
            "[    8] loss: 0.116\n",
            "[    9] loss: 0.169\n",
            "[   10] loss: 0.113\n",
            "[   11] loss: 0.133\n",
            "[   12] loss: 0.124\n",
            "[   13] loss: 0.093\n",
            "[   14] loss: 0.078\n",
            "[   15] loss: 0.115\n",
            "[   16] loss: 0.114\n",
            "[   17] loss: 0.113\n",
            "[   18] loss: 0.124\n",
            "[   19] loss: 0.313\n",
            "epoch:  165\n",
            "[    1] loss: 0.110\n",
            "[    2] loss: 0.118\n",
            "[    3] loss: 0.128\n",
            "[    4] loss: 0.095\n",
            "[    5] loss: 0.126\n",
            "[    6] loss: 0.101\n",
            "[    7] loss: 0.127\n",
            "[    8] loss: 0.094\n",
            "[    9] loss: 0.092\n",
            "[   10] loss: 0.129\n",
            "[   11] loss: 0.148\n",
            "[   12] loss: 0.132\n",
            "[   13] loss: 0.092\n",
            "[   14] loss: 0.089\n",
            "[   15] loss: 0.151\n",
            "[   16] loss: 0.121\n",
            "[   17] loss: 0.110\n",
            "[   18] loss: 0.111\n",
            "[   19] loss: 0.512\n",
            "epoch:  166\n",
            "[    1] loss: 0.101\n",
            "[    2] loss: 0.127\n",
            "[    3] loss: 0.143\n",
            "[    4] loss: 0.147\n",
            "[    5] loss: 0.114\n",
            "[    6] loss: 0.127\n",
            "[    7] loss: 0.146\n",
            "[    8] loss: 0.126\n",
            "[    9] loss: 0.099\n",
            "[   10] loss: 0.098\n",
            "[   11] loss: 0.127\n",
            "[   12] loss: 0.127\n",
            "[   13] loss: 0.090\n",
            "[   14] loss: 0.106\n",
            "[   15] loss: 0.112\n",
            "[   16] loss: 0.100\n",
            "[   17] loss: 0.131\n",
            "[   18] loss: 0.115\n",
            "[   19] loss: 0.149\n",
            "train loss:  0.09558788289809052\n",
            "val loss:  0.17520489543676376\n",
            "epoch:  167\n",
            "[    1] loss: 0.152\n",
            "[    2] loss: 0.111\n",
            "[    3] loss: 0.118\n",
            "[    4] loss: 0.129\n",
            "[    5] loss: 0.101\n",
            "[    6] loss: 0.124\n",
            "[    7] loss: 0.110\n",
            "[    8] loss: 0.103\n",
            "[    9] loss: 0.104\n",
            "[   10] loss: 0.096\n",
            "[   11] loss: 0.083\n",
            "[   12] loss: 0.168\n",
            "[   13] loss: 0.133\n",
            "[   14] loss: 0.148\n",
            "[   15] loss: 0.104\n",
            "[   16] loss: 0.113\n",
            "[   17] loss: 0.125\n",
            "[   18] loss: 0.208\n",
            "[   19] loss: 0.242\n",
            "epoch:  168\n",
            "[    1] loss: 0.124\n",
            "[    2] loss: 0.109\n",
            "[    3] loss: 0.151\n",
            "[    4] loss: 0.088\n",
            "[    5] loss: 0.136\n",
            "[    6] loss: 0.115\n",
            "[    7] loss: 0.120\n",
            "[    8] loss: 0.172\n",
            "[    9] loss: 0.148\n",
            "[   10] loss: 0.116\n",
            "[   11] loss: 0.088\n",
            "[   12] loss: 0.097\n",
            "[   13] loss: 0.079\n",
            "[   14] loss: 0.127\n",
            "[   15] loss: 0.117\n",
            "[   16] loss: 0.127\n",
            "[   17] loss: 0.123\n",
            "[   18] loss: 0.089\n",
            "[   19] loss: 0.449\n",
            "epoch:  169\n",
            "[    1] loss: 0.169\n",
            "[    2] loss: 0.167\n",
            "[    3] loss: 0.108\n",
            "[    4] loss: 0.095\n",
            "[    5] loss: 0.079\n",
            "[    6] loss: 0.137\n",
            "[    7] loss: 0.164\n",
            "[    8] loss: 0.141\n",
            "[    9] loss: 0.087\n",
            "[   10] loss: 0.139\n",
            "[   11] loss: 0.125\n",
            "[   12] loss: 0.124\n",
            "[   13] loss: 0.108\n",
            "[   14] loss: 0.118\n",
            "[   15] loss: 0.120\n",
            "[   16] loss: 0.146\n",
            "[   17] loss: 0.119\n",
            "[   18] loss: 0.097\n",
            "[   19] loss: 0.257\n",
            "epoch:  170\n",
            "[    1] loss: 0.109\n",
            "[    2] loss: 0.086\n",
            "[    3] loss: 0.081\n",
            "[    4] loss: 0.097\n",
            "[    5] loss: 0.147\n",
            "[    6] loss: 0.149\n",
            "[    7] loss: 0.131\n",
            "[    8] loss: 0.115\n",
            "[    9] loss: 0.095\n",
            "[   10] loss: 0.111\n",
            "[   11] loss: 0.130\n",
            "[   12] loss: 0.119\n",
            "[   13] loss: 0.087\n",
            "[   14] loss: 0.125\n",
            "[   15] loss: 0.155\n",
            "[   16] loss: 0.084\n",
            "[   17] loss: 0.114\n",
            "[   18] loss: 0.074\n",
            "[   19] loss: 0.398\n",
            "epoch:  171\n",
            "[    1] loss: 0.110\n",
            "[    2] loss: 0.195\n",
            "[    3] loss: 0.132\n",
            "[    4] loss: 0.105\n",
            "[    5] loss: 0.194\n",
            "[    6] loss: 0.122\n",
            "[    7] loss: 0.094\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.138\n",
            "[   10] loss: 0.092\n",
            "[   11] loss: 0.137\n",
            "[   12] loss: 0.088\n",
            "[   13] loss: 0.123\n",
            "[   14] loss: 0.134\n",
            "[   15] loss: 0.114\n",
            "[   16] loss: 0.100\n",
            "[   17] loss: 0.108\n",
            "[   18] loss: 0.125\n",
            "[   19] loss: 0.144\n",
            "train loss:  0.098226686446544\n",
            "val loss:  0.1822866965085268\n",
            "epoch:  172\n",
            "[    1] loss: 0.127\n",
            "[    2] loss: 0.136\n",
            "[    3] loss: 0.179\n",
            "[    4] loss: 0.105\n",
            "[    5] loss: 0.109\n",
            "[    6] loss: 0.117\n",
            "[    7] loss: 0.075\n",
            "[    8] loss: 0.118\n",
            "[    9] loss: 0.102\n",
            "[   10] loss: 0.151\n",
            "[   11] loss: 0.107\n",
            "[   12] loss: 0.098\n",
            "[   13] loss: 0.110\n",
            "[   14] loss: 0.116\n",
            "[   15] loss: 0.131\n",
            "[   16] loss: 0.092\n",
            "[   17] loss: 0.086\n",
            "[   18] loss: 0.114\n",
            "[   19] loss: 0.191\n",
            "epoch:  173\n",
            "[    1] loss: 0.078\n",
            "[    2] loss: 0.112\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.169\n",
            "[    5] loss: 0.107\n",
            "[    6] loss: 0.145\n",
            "[    7] loss: 0.105\n",
            "[    8] loss: 0.147\n",
            "[    9] loss: 0.111\n",
            "[   10] loss: 0.097\n",
            "[   11] loss: 0.092\n",
            "[   12] loss: 0.120\n",
            "[   13] loss: 0.103\n",
            "[   14] loss: 0.101\n",
            "[   15] loss: 0.098\n",
            "[   16] loss: 0.120\n",
            "[   17] loss: 0.098\n",
            "[   18] loss: 0.119\n",
            "[   19] loss: 0.144\n",
            "epoch:  174\n",
            "[    1] loss: 0.112\n",
            "[    2] loss: 0.117\n",
            "[    3] loss: 0.105\n",
            "[    4] loss: 0.113\n",
            "[    5] loss: 0.109\n",
            "[    6] loss: 0.115\n",
            "[    7] loss: 0.090\n",
            "[    8] loss: 0.129\n",
            "[    9] loss: 0.087\n",
            "[   10] loss: 0.115\n",
            "[   11] loss: 0.264\n",
            "[   12] loss: 0.089\n",
            "[   13] loss: 0.095\n",
            "[   14] loss: 0.082\n",
            "[   15] loss: 0.103\n",
            "[   16] loss: 0.095\n",
            "[   17] loss: 0.089\n",
            "[   18] loss: 0.084\n",
            "[   19] loss: 0.152\n",
            "epoch:  175\n",
            "[    1] loss: 0.080\n",
            "[    2] loss: 0.089\n",
            "[    3] loss: 0.102\n",
            "[    4] loss: 0.114\n",
            "[    5] loss: 0.092\n",
            "[    6] loss: 0.139\n",
            "[    7] loss: 0.081\n",
            "[    8] loss: 0.104\n",
            "[    9] loss: 0.102\n",
            "[   10] loss: 0.103\n",
            "[   11] loss: 0.113\n",
            "[   12] loss: 0.089\n",
            "[   13] loss: 0.090\n",
            "[   14] loss: 0.148\n",
            "[   15] loss: 0.151\n",
            "[   16] loss: 0.106\n",
            "[   17] loss: 0.085\n",
            "[   18] loss: 0.117\n",
            "[   19] loss: 0.133\n",
            "epoch:  176\n",
            "[    1] loss: 0.098\n",
            "[    2] loss: 0.102\n",
            "[    3] loss: 0.094\n",
            "[    4] loss: 0.100\n",
            "[    5] loss: 0.088\n",
            "[    6] loss: 0.102\n",
            "[    7] loss: 0.112\n",
            "[    8] loss: 0.101\n",
            "[    9] loss: 0.085\n",
            "[   10] loss: 0.082\n",
            "[   11] loss: 0.085\n",
            "[   12] loss: 0.171\n",
            "[   13] loss: 0.109\n",
            "[   14] loss: 0.139\n",
            "[   15] loss: 0.112\n",
            "[   16] loss: 0.095\n",
            "[   17] loss: 0.107\n",
            "[   18] loss: 0.101\n",
            "[   19] loss: 0.229\n",
            "train loss:  0.09925438050071106\n",
            "val loss:  0.1693723425269127\n",
            "epoch:  177\n",
            "[    1] loss: 0.091\n",
            "[    2] loss: 0.127\n",
            "[    3] loss: 0.119\n",
            "[    4] loss: 0.174\n",
            "[    5] loss: 0.137\n",
            "[    6] loss: 0.124\n",
            "[    7] loss: 0.103\n",
            "[    8] loss: 0.103\n",
            "[    9] loss: 0.099\n",
            "[   10] loss: 0.116\n",
            "[   11] loss: 0.117\n",
            "[   12] loss: 0.096\n",
            "[   13] loss: 0.096\n",
            "[   14] loss: 0.103\n",
            "[   15] loss: 0.093\n",
            "[   16] loss: 0.135\n",
            "[   17] loss: 0.162\n",
            "[   18] loss: 0.118\n",
            "[   19] loss: 0.279\n",
            "epoch:  178\n",
            "[    1] loss: 0.108\n",
            "[    2] loss: 0.118\n",
            "[    3] loss: 0.129\n",
            "[    4] loss: 0.123\n",
            "[    5] loss: 0.141\n",
            "[    6] loss: 0.099\n",
            "[    7] loss: 0.097\n",
            "[    8] loss: 0.119\n",
            "[    9] loss: 0.105\n",
            "[   10] loss: 0.105\n",
            "[   11] loss: 0.123\n",
            "[   12] loss: 0.157\n",
            "[   13] loss: 0.108\n",
            "[   14] loss: 0.090\n",
            "[   15] loss: 0.116\n",
            "[   16] loss: 0.112\n",
            "[   17] loss: 0.091\n",
            "[   18] loss: 0.125\n",
            "[   19] loss: 0.292\n",
            "epoch:  179\n",
            "[    1] loss: 0.103\n",
            "[    2] loss: 0.183\n",
            "[    3] loss: 0.122\n",
            "[    4] loss: 0.090\n",
            "[    5] loss: 0.134\n",
            "[    6] loss: 0.092\n",
            "[    7] loss: 0.074\n",
            "[    8] loss: 0.111\n",
            "[    9] loss: 0.089\n",
            "[   10] loss: 0.107\n",
            "[   11] loss: 0.122\n",
            "[   12] loss: 0.118\n",
            "[   13] loss: 0.118\n",
            "[   14] loss: 0.127\n",
            "[   15] loss: 0.150\n",
            "[   16] loss: 0.138\n",
            "[   17] loss: 0.144\n",
            "[   18] loss: 0.112\n",
            "[   19] loss: 0.244\n",
            "epoch:  180\n",
            "[    1] loss: 0.090\n",
            "[    2] loss: 0.164\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.151\n",
            "[    5] loss: 0.097\n",
            "[    6] loss: 0.157\n",
            "[    7] loss: 0.100\n",
            "[    8] loss: 0.090\n",
            "[    9] loss: 0.122\n",
            "[   10] loss: 0.206\n",
            "[   11] loss: 0.083\n",
            "[   12] loss: 0.117\n",
            "[   13] loss: 0.150\n",
            "[   14] loss: 0.126\n",
            "[   15] loss: 0.097\n",
            "[   16] loss: 0.130\n",
            "[   17] loss: 0.087\n",
            "[   18] loss: 0.119\n",
            "[   19] loss: 0.138\n",
            "epoch:  181\n",
            "[    1] loss: 0.087\n",
            "[    2] loss: 0.101\n",
            "[    3] loss: 0.155\n",
            "[    4] loss: 0.123\n",
            "[    5] loss: 0.136\n",
            "[    6] loss: 0.082\n",
            "[    7] loss: 0.105\n",
            "[    8] loss: 0.104\n",
            "[    9] loss: 0.127\n",
            "[   10] loss: 0.106\n",
            "[   11] loss: 0.114\n",
            "[   12] loss: 0.153\n",
            "[   13] loss: 0.143\n",
            "[   14] loss: 0.134\n",
            "[   15] loss: 0.114\n",
            "[   16] loss: 0.120\n",
            "[   17] loss: 0.109\n",
            "[   18] loss: 0.120\n",
            "[   19] loss: 0.206\n",
            "train loss:  0.09208038284936372\n",
            "val loss:  0.18553175404667854\n",
            "epoch:  182\n",
            "[    1] loss: 0.082\n",
            "[    2] loss: 0.124\n",
            "[    3] loss: 0.169\n",
            "[    4] loss: 0.094\n",
            "[    5] loss: 0.114\n",
            "[    6] loss: 0.137\n",
            "[    7] loss: 0.138\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.155\n",
            "[   10] loss: 0.104\n",
            "[   11] loss: 0.098\n",
            "[   12] loss: 0.084\n",
            "[   13] loss: 0.080\n",
            "[   14] loss: 0.120\n",
            "[   15] loss: 0.129\n",
            "[   16] loss: 0.115\n",
            "[   17] loss: 0.099\n",
            "[   18] loss: 0.108\n",
            "[   19] loss: 0.245\n",
            "epoch:  183\n",
            "[    1] loss: 0.118\n",
            "[    2] loss: 0.113\n",
            "[    3] loss: 0.121\n",
            "[    4] loss: 0.107\n",
            "[    5] loss: 0.090\n",
            "[    6] loss: 0.094\n",
            "[    7] loss: 0.155\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.082\n",
            "[   10] loss: 0.121\n",
            "[   11] loss: 0.149\n",
            "[   12] loss: 0.133\n",
            "[   13] loss: 0.103\n",
            "[   14] loss: 0.114\n",
            "[   15] loss: 0.151\n",
            "[   16] loss: 0.110\n",
            "[   17] loss: 0.110\n",
            "[   18] loss: 0.141\n",
            "[   19] loss: 0.218\n",
            "epoch:  184\n",
            "[    1] loss: 0.117\n",
            "[    2] loss: 0.145\n",
            "[    3] loss: 0.094\n",
            "[    4] loss: 0.078\n",
            "[    5] loss: 0.148\n",
            "[    6] loss: 0.098\n",
            "[    7] loss: 0.150\n",
            "[    8] loss: 0.093\n",
            "[    9] loss: 0.142\n",
            "[   10] loss: 0.110\n",
            "[   11] loss: 0.099\n",
            "[   12] loss: 0.119\n",
            "[   13] loss: 0.111\n",
            "[   14] loss: 0.095\n",
            "[   15] loss: 0.112\n",
            "[   16] loss: 0.118\n",
            "[   17] loss: 0.108\n",
            "[   18] loss: 0.145\n",
            "[   19] loss: 0.103\n",
            "epoch:  185\n",
            "[    1] loss: 0.158\n",
            "[    2] loss: 0.097\n",
            "[    3] loss: 0.094\n",
            "[    4] loss: 0.106\n",
            "[    5] loss: 0.171\n",
            "[    6] loss: 0.104\n",
            "[    7] loss: 0.120\n",
            "[    8] loss: 0.095\n",
            "[    9] loss: 0.118\n",
            "[   10] loss: 0.208\n",
            "[   11] loss: 0.085\n",
            "[   12] loss: 0.167\n",
            "[   13] loss: 0.119\n",
            "[   14] loss: 0.125\n",
            "[   15] loss: 0.115\n",
            "[   16] loss: 0.075\n",
            "[   17] loss: 0.168\n",
            "[   18] loss: 0.114\n",
            "[   19] loss: 0.426\n",
            "epoch:  186\n",
            "[    1] loss: 0.113\n",
            "[    2] loss: 0.086\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.114\n",
            "[    5] loss: 0.080\n",
            "[    6] loss: 0.091\n",
            "[    7] loss: 0.105\n",
            "[    8] loss: 0.127\n",
            "[    9] loss: 0.172\n",
            "[   10] loss: 0.107\n",
            "[   11] loss: 0.148\n",
            "[   12] loss: 0.106\n",
            "[   13] loss: 0.118\n",
            "[   14] loss: 0.102\n",
            "[   15] loss: 0.126\n",
            "[   16] loss: 0.123\n",
            "[   17] loss: 0.106\n",
            "[   18] loss: 0.135\n",
            "[   19] loss: 0.258\n",
            "train loss:  0.09199968222802614\n",
            "val loss:  0.1856786385178566\n",
            "epoch:  187\n",
            "[    1] loss: 0.114\n",
            "[    2] loss: 0.137\n",
            "[    3] loss: 0.174\n",
            "[    4] loss: 0.122\n",
            "[    5] loss: 0.120\n",
            "[    6] loss: 0.121\n",
            "[    7] loss: 0.086\n",
            "[    8] loss: 0.107\n",
            "[    9] loss: 0.120\n",
            "[   10] loss: 0.092\n",
            "[   11] loss: 0.136\n",
            "[   12] loss: 0.143\n",
            "[   13] loss: 0.091\n",
            "[   14] loss: 0.107\n",
            "[   15] loss: 0.111\n",
            "[   16] loss: 0.110\n",
            "[   17] loss: 0.146\n",
            "[   18] loss: 0.195\n",
            "[   19] loss: 0.657\n",
            "epoch:  188\n",
            "[    1] loss: 0.109\n",
            "[    2] loss: 0.080\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.090\n",
            "[    5] loss: 0.106\n",
            "[    6] loss: 0.089\n",
            "[    7] loss: 0.102\n",
            "[    8] loss: 0.107\n",
            "[    9] loss: 0.136\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.097\n",
            "[   12] loss: 0.106\n",
            "[   13] loss: 0.116\n",
            "[   14] loss: 0.089\n",
            "[   15] loss: 0.103\n",
            "[   16] loss: 0.143\n",
            "[   17] loss: 0.171\n",
            "[   18] loss: 0.124\n",
            "[   19] loss: 0.327\n",
            "epoch:  189\n",
            "[    1] loss: 0.107\n",
            "[    2] loss: 0.118\n",
            "[    3] loss: 0.152\n",
            "[    4] loss: 0.115\n",
            "[    5] loss: 0.100\n",
            "[    6] loss: 0.084\n",
            "[    7] loss: 0.094\n",
            "[    8] loss: 0.145\n",
            "[    9] loss: 0.109\n",
            "[   10] loss: 0.123\n",
            "[   11] loss: 0.113\n",
            "[   12] loss: 0.116\n",
            "[   13] loss: 0.122\n",
            "[   14] loss: 0.105\n",
            "[   15] loss: 0.087\n",
            "[   16] loss: 0.121\n",
            "[   17] loss: 0.100\n",
            "[   18] loss: 0.158\n",
            "[   19] loss: 0.229\n",
            "epoch:  190\n",
            "[    1] loss: 0.084\n",
            "[    2] loss: 0.115\n",
            "[    3] loss: 0.129\n",
            "[    4] loss: 0.088\n",
            "[    5] loss: 0.107\n",
            "[    6] loss: 0.109\n",
            "[    7] loss: 0.089\n",
            "[    8] loss: 0.124\n",
            "[    9] loss: 0.100\n",
            "[   10] loss: 0.104\n",
            "[   11] loss: 0.089\n",
            "[   12] loss: 0.156\n",
            "[   13] loss: 0.120\n",
            "[   14] loss: 0.139\n",
            "[   15] loss: 0.092\n",
            "[   16] loss: 0.119\n",
            "[   17] loss: 0.113\n",
            "[   18] loss: 0.095\n",
            "[   19] loss: 0.120\n",
            "epoch:  191\n",
            "[    1] loss: 0.105\n",
            "[    2] loss: 0.089\n",
            "[    3] loss: 0.159\n",
            "[    4] loss: 0.117\n",
            "[    5] loss: 0.112\n",
            "[    6] loss: 0.096\n",
            "[    7] loss: 0.176\n",
            "[    8] loss: 0.093\n",
            "[    9] loss: 0.116\n",
            "[   10] loss: 0.129\n",
            "[   11] loss: 0.092\n",
            "[   12] loss: 0.123\n",
            "[   13] loss: 0.114\n",
            "[   14] loss: 0.109\n",
            "[   15] loss: 0.095\n",
            "[   16] loss: 0.078\n",
            "[   17] loss: 0.136\n",
            "[   18] loss: 0.091\n",
            "[   19] loss: 0.245\n",
            "train loss:  0.09854100304929649\n",
            "val loss:  0.17185571417212486\n",
            "epoch:  192\n",
            "[    1] loss: 0.098\n",
            "[    2] loss: 0.107\n",
            "[    3] loss: 0.113\n",
            "[    4] loss: 0.165\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.162\n",
            "[    7] loss: 0.119\n",
            "[    8] loss: 0.085\n",
            "[    9] loss: 0.162\n",
            "[   10] loss: 0.123\n",
            "[   11] loss: 0.114\n",
            "[   12] loss: 0.100\n",
            "[   13] loss: 0.102\n",
            "[   14] loss: 0.104\n",
            "[   15] loss: 0.188\n",
            "[   16] loss: 0.124\n",
            "[   17] loss: 0.087\n",
            "[   18] loss: 0.093\n",
            "[   19] loss: 0.137\n",
            "epoch:  193\n",
            "[    1] loss: 0.079\n",
            "[    2] loss: 0.128\n",
            "[    3] loss: 0.125\n",
            "[    4] loss: 0.128\n",
            "[    5] loss: 0.101\n",
            "[    6] loss: 0.114\n",
            "[    7] loss: 0.093\n",
            "[    8] loss: 0.175\n",
            "[    9] loss: 0.115\n",
            "[   10] loss: 0.125\n",
            "[   11] loss: 0.095\n",
            "[   12] loss: 0.130\n",
            "[   13] loss: 0.120\n",
            "[   14] loss: 0.087\n",
            "[   15] loss: 0.115\n",
            "[   16] loss: 0.149\n",
            "[   17] loss: 0.132\n",
            "[   18] loss: 0.090\n",
            "[   19] loss: 0.287\n",
            "epoch:  194\n",
            "[    1] loss: 0.130\n",
            "[    2] loss: 0.113\n",
            "[    3] loss: 0.092\n",
            "[    4] loss: 0.112\n",
            "[    5] loss: 0.150\n",
            "[    6] loss: 0.157\n",
            "[    7] loss: 0.097\n",
            "[    8] loss: 0.125\n",
            "[    9] loss: 0.089\n",
            "[   10] loss: 0.086\n",
            "[   11] loss: 0.093\n",
            "[   12] loss: 0.133\n",
            "[   13] loss: 0.173\n",
            "[   14] loss: 0.082\n",
            "[   15] loss: 0.128\n",
            "[   16] loss: 0.125\n",
            "[   17] loss: 0.116\n",
            "[   18] loss: 0.091\n",
            "[   19] loss: 0.380\n",
            "epoch:  195\n",
            "[    1] loss: 0.131\n",
            "[    2] loss: 0.095\n",
            "[    3] loss: 0.155\n",
            "[    4] loss: 0.127\n",
            "[    5] loss: 0.134\n",
            "[    6] loss: 0.128\n",
            "[    7] loss: 0.093\n",
            "[    8] loss: 0.107\n",
            "[    9] loss: 0.153\n",
            "[   10] loss: 0.181\n",
            "[   11] loss: 0.108\n",
            "[   12] loss: 0.112\n",
            "[   13] loss: 0.095\n",
            "[   14] loss: 0.087\n",
            "[   15] loss: 0.071\n",
            "[   16] loss: 0.101\n",
            "[   17] loss: 0.111\n",
            "[   18] loss: 0.151\n",
            "[   19] loss: 0.283\n",
            "epoch:  196\n",
            "[    1] loss: 0.088\n",
            "[    2] loss: 0.087\n",
            "[    3] loss: 0.090\n",
            "[    4] loss: 0.108\n",
            "[    5] loss: 0.121\n",
            "[    6] loss: 0.097\n",
            "[    7] loss: 0.136\n",
            "[    8] loss: 0.091\n",
            "[    9] loss: 0.160\n",
            "[   10] loss: 0.148\n",
            "[   11] loss: 0.116\n",
            "[   12] loss: 0.101\n",
            "[   13] loss: 0.107\n",
            "[   14] loss: 0.119\n",
            "[   15] loss: 0.094\n",
            "[   16] loss: 0.127\n",
            "[   17] loss: 0.095\n",
            "[   18] loss: 0.135\n",
            "[   19] loss: 0.481\n",
            "train loss:  0.09484041709562435\n",
            "val loss:  0.16839340887963772\n",
            "epoch:  197\n",
            "[    1] loss: 0.117\n",
            "[    2] loss: 0.156\n",
            "[    3] loss: 0.097\n",
            "[    4] loss: 0.154\n",
            "[    5] loss: 0.089\n",
            "[    6] loss: 0.173\n",
            "[    7] loss: 0.124\n",
            "[    8] loss: 0.131\n",
            "[    9] loss: 0.074\n",
            "[   10] loss: 0.092\n",
            "[   11] loss: 0.126\n",
            "[   12] loss: 0.119\n",
            "[   13] loss: 0.105\n",
            "[   14] loss: 0.100\n",
            "[   15] loss: 0.110\n",
            "[   16] loss: 0.164\n",
            "[   17] loss: 0.087\n",
            "[   18] loss: 0.126\n",
            "[   19] loss: 0.115\n",
            "epoch:  198\n",
            "[    1] loss: 0.133\n",
            "[    2] loss: 0.142\n",
            "[    3] loss: 0.106\n",
            "[    4] loss: 0.167\n",
            "[    5] loss: 0.110\n",
            "[    6] loss: 0.103\n",
            "[    7] loss: 0.112\n",
            "[    8] loss: 0.124\n",
            "[    9] loss: 0.126\n",
            "[   10] loss: 0.143\n",
            "[   11] loss: 0.107\n",
            "[   12] loss: 0.085\n",
            "[   13] loss: 0.116\n",
            "[   14] loss: 0.211\n",
            "[   15] loss: 0.114\n",
            "[   16] loss: 0.145\n",
            "[   17] loss: 0.114\n",
            "[   18] loss: 0.104\n",
            "[   19] loss: 0.126\n",
            "epoch:  199\n",
            "[    1] loss: 0.085\n",
            "[    2] loss: 0.126\n",
            "[    3] loss: 0.150\n",
            "[    4] loss: 0.120\n",
            "[    5] loss: 0.113\n",
            "[    6] loss: 0.081\n",
            "[    7] loss: 0.106\n",
            "[    8] loss: 0.099\n",
            "[    9] loss: 0.118\n",
            "[   10] loss: 0.093\n",
            "[   11] loss: 0.105\n",
            "[   12] loss: 0.104\n",
            "[   13] loss: 0.140\n",
            "[   14] loss: 0.108\n",
            "[   15] loss: 0.102\n",
            "[   16] loss: 0.181\n",
            "[   17] loss: 0.121\n",
            "[   18] loss: 0.099\n",
            "[   19] loss: 0.153\n",
            "epoch:  200\n",
            "[    1] loss: 0.093\n",
            "[    2] loss: 0.095\n",
            "[    3] loss: 0.095\n",
            "[    4] loss: 0.099\n",
            "[    5] loss: 0.121\n",
            "[    6] loss: 0.107\n",
            "[    7] loss: 0.170\n",
            "[    8] loss: 0.101\n",
            "[    9] loss: 0.123\n",
            "[   10] loss: 0.085\n",
            "[   11] loss: 0.171\n",
            "[   12] loss: 0.102\n",
            "[   13] loss: 0.101\n",
            "[   14] loss: 0.117\n",
            "[   15] loss: 0.132\n",
            "[   16] loss: 0.095\n",
            "[   17] loss: 0.105\n",
            "[   18] loss: 0.101\n",
            "[   19] loss: 0.197\n",
            "epoch:  201\n",
            "[    1] loss: 0.125\n",
            "[    2] loss: 0.121\n",
            "[    3] loss: 0.113\n",
            "[    4] loss: 0.123\n",
            "[    5] loss: 0.140\n",
            "[    6] loss: 0.201\n",
            "[    7] loss: 0.093\n",
            "[    8] loss: 0.129\n",
            "[    9] loss: 0.083\n",
            "[   10] loss: 0.093\n",
            "[   11] loss: 0.101\n",
            "[   12] loss: 0.090\n",
            "[   13] loss: 0.122\n",
            "[   14] loss: 0.122\n",
            "[   15] loss: 0.084\n",
            "[   16] loss: 0.134\n",
            "[   17] loss: 0.102\n",
            "[   18] loss: 0.085\n",
            "[   19] loss: 0.333\n",
            "train loss:  0.09464560951763655\n",
            "val loss:  0.19424712657928467\n",
            "epoch:  202\n",
            "[    1] loss: 0.105\n",
            "[    2] loss: 0.084\n",
            "[    3] loss: 0.120\n",
            "[    4] loss: 0.131\n",
            "[    5] loss: 0.110\n",
            "[    6] loss: 0.100\n",
            "[    7] loss: 0.137\n",
            "[    8] loss: 0.168\n",
            "[    9] loss: 0.086\n",
            "[   10] loss: 0.133\n",
            "[   11] loss: 0.079\n",
            "[   12] loss: 0.099\n",
            "[   13] loss: 0.146\n",
            "[   14] loss: 0.113\n",
            "[   15] loss: 0.097\n",
            "[   16] loss: 0.122\n",
            "[   17] loss: 0.099\n",
            "[   18] loss: 0.138\n",
            "[   19] loss: 0.439\n",
            "epoch:  203\n",
            "[    1] loss: 0.126\n",
            "[    2] loss: 0.102\n",
            "[    3] loss: 0.202\n",
            "[    4] loss: 0.118\n",
            "[    5] loss: 0.164\n",
            "[    6] loss: 0.103\n",
            "[    7] loss: 0.102\n",
            "[    8] loss: 0.093\n",
            "[    9] loss: 0.082\n",
            "[   10] loss: 0.113\n",
            "[   11] loss: 0.082\n",
            "[   12] loss: 0.095\n",
            "[   13] loss: 0.129\n",
            "[   14] loss: 0.083\n",
            "[   15] loss: 0.100\n",
            "[   16] loss: 0.109\n",
            "[   17] loss: 0.120\n",
            "[   18] loss: 0.088\n",
            "[   19] loss: 0.238\n",
            "epoch:  204\n",
            "[    1] loss: 0.112\n",
            "[    2] loss: 0.093\n",
            "[    3] loss: 0.091\n",
            "[    4] loss: 0.116\n",
            "[    5] loss: 0.140\n",
            "[    6] loss: 0.105\n",
            "[    7] loss: 0.109\n",
            "[    8] loss: 0.169\n",
            "[    9] loss: 0.173\n",
            "[   10] loss: 0.129\n",
            "[   11] loss: 0.121\n",
            "[   12] loss: 0.106\n",
            "[   13] loss: 0.078\n",
            "[   14] loss: 0.095\n",
            "[   15] loss: 0.092\n",
            "[   16] loss: 0.112\n",
            "[   17] loss: 0.158\n",
            "[   18] loss: 0.205\n",
            "[   19] loss: 0.336\n",
            "epoch:  205\n",
            "[    1] loss: 0.104\n",
            "[    2] loss: 0.082\n",
            "[    3] loss: 0.111\n",
            "[    4] loss: 0.131\n",
            "[    5] loss: 0.138\n",
            "[    6] loss: 0.113\n",
            "[    7] loss: 0.137\n",
            "[    8] loss: 0.124\n",
            "[    9] loss: 0.168\n",
            "[   10] loss: 0.111\n",
            "[   11] loss: 0.111\n",
            "[   12] loss: 0.124\n",
            "[   13] loss: 0.129\n",
            "[   14] loss: 0.147\n",
            "[   15] loss: 0.129\n",
            "[   16] loss: 0.101\n",
            "[   17] loss: 0.133\n",
            "[   18] loss: 0.112\n",
            "[   19] loss: 0.102\n",
            "epoch:  206\n",
            "[    1] loss: 0.080\n",
            "[    2] loss: 0.149\n",
            "[    3] loss: 0.135\n",
            "[    4] loss: 0.120\n",
            "[    5] loss: 0.118\n",
            "[    6] loss: 0.229\n",
            "[    7] loss: 0.123\n",
            "[    8] loss: 0.124\n",
            "[    9] loss: 0.079\n",
            "[   10] loss: 0.096\n",
            "[   11] loss: 0.134\n",
            "[   12] loss: 0.123\n",
            "[   13] loss: 0.097\n",
            "[   14] loss: 0.180\n",
            "[   15] loss: 0.095\n",
            "[   16] loss: 0.183\n",
            "[   17] loss: 0.095\n",
            "[   18] loss: 0.134\n",
            "[   19] loss: 0.122\n",
            "train loss:  0.09211693224771057\n",
            "val loss:  0.17701462097465992\n",
            "epoch:  207\n",
            "[    1] loss: 0.103\n",
            "[    2] loss: 0.143\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.100\n",
            "[    5] loss: 0.099\n",
            "[    6] loss: 0.159\n",
            "[    7] loss: 0.100\n",
            "[    8] loss: 0.100\n",
            "[    9] loss: 0.098\n",
            "[   10] loss: 0.155\n",
            "[   11] loss: 0.112\n",
            "[   12] loss: 0.157\n",
            "[   13] loss: 0.099\n",
            "[   14] loss: 0.102\n",
            "[   15] loss: 0.137\n",
            "[   16] loss: 0.090\n",
            "[   17] loss: 0.108\n",
            "[   18] loss: 0.118\n",
            "[   19] loss: 0.284\n",
            "epoch:  208\n",
            "[    1] loss: 0.108\n",
            "[    2] loss: 0.136\n",
            "[    3] loss: 0.107\n",
            "[    4] loss: 0.158\n",
            "[    5] loss: 0.093\n",
            "[    6] loss: 0.075\n",
            "[    7] loss: 0.147\n",
            "[    8] loss: 0.098\n",
            "[    9] loss: 0.141\n",
            "[   10] loss: 0.111\n",
            "[   11] loss: 0.130\n",
            "[   12] loss: 0.094\n",
            "[   13] loss: 0.115\n",
            "[   14] loss: 0.132\n",
            "[   15] loss: 0.109\n",
            "[   16] loss: 0.145\n",
            "[   17] loss: 0.129\n",
            "[   18] loss: 0.087\n",
            "[   19] loss: 0.200\n",
            "epoch:  209\n",
            "[    1] loss: 0.093\n",
            "[    2] loss: 0.123\n",
            "[    3] loss: 0.092\n",
            "[    4] loss: 0.136\n",
            "[    5] loss: 0.089\n",
            "[    6] loss: 0.124\n",
            "[    7] loss: 0.128\n",
            "[    8] loss: 0.127\n",
            "[    9] loss: 0.093\n",
            "[   10] loss: 0.096\n",
            "[   11] loss: 0.123\n",
            "[   12] loss: 0.096\n",
            "[   13] loss: 0.143\n",
            "[   14] loss: 0.172\n",
            "[   15] loss: 0.117\n",
            "[   16] loss: 0.103\n",
            "[   17] loss: 0.105\n",
            "[   18] loss: 0.105\n",
            "[   19] loss: 0.194\n",
            "epoch:  210\n",
            "[    1] loss: 0.155\n",
            "[    2] loss: 0.091\n",
            "[    3] loss: 0.146\n",
            "[    4] loss: 0.142\n",
            "[    5] loss: 0.096\n",
            "[    6] loss: 0.119\n",
            "[    7] loss: 0.141\n",
            "[    8] loss: 0.098\n",
            "[    9] loss: 0.128\n",
            "[   10] loss: 0.111\n",
            "[   11] loss: 0.115\n",
            "[   12] loss: 0.104\n",
            "[   13] loss: 0.104\n",
            "[   14] loss: 0.131\n",
            "[   15] loss: 0.102\n",
            "[   16] loss: 0.113\n",
            "[   17] loss: 0.113\n",
            "[   18] loss: 0.132\n",
            "[   19] loss: 0.099\n",
            "epoch:  211\n",
            "[    1] loss: 0.118\n",
            "[    2] loss: 0.103\n",
            "[    3] loss: 0.105\n",
            "[    4] loss: 0.111\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.092\n",
            "[    7] loss: 0.135\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.146\n",
            "[   10] loss: 0.143\n",
            "[   11] loss: 0.106\n",
            "[   12] loss: 0.108\n",
            "[   13] loss: 0.173\n",
            "[   14] loss: 0.087\n",
            "[   15] loss: 0.112\n",
            "[   16] loss: 0.105\n",
            "[   17] loss: 0.127\n",
            "[   18] loss: 0.108\n",
            "[   19] loss: 0.467\n",
            "train loss:  0.09629613248740926\n",
            "val loss:  0.17582105845212936\n",
            "epoch:  212\n",
            "[    1] loss: 0.134\n",
            "[    2] loss: 0.099\n",
            "[    3] loss: 0.105\n",
            "[    4] loss: 0.121\n",
            "[    5] loss: 0.110\n",
            "[    6] loss: 0.129\n",
            "[    7] loss: 0.086\n",
            "[    8] loss: 0.126\n",
            "[    9] loss: 0.102\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.126\n",
            "[   12] loss: 0.098\n",
            "[   13] loss: 0.126\n",
            "[   14] loss: 0.111\n",
            "[   15] loss: 0.113\n",
            "[   16] loss: 0.089\n",
            "[   17] loss: 0.147\n",
            "[   18] loss: 0.114\n",
            "[   19] loss: 0.241\n",
            "epoch:  213\n",
            "[    1] loss: 0.102\n",
            "[    2] loss: 0.132\n",
            "[    3] loss: 0.099\n",
            "[    4] loss: 0.131\n",
            "[    5] loss: 0.110\n",
            "[    6] loss: 0.105\n",
            "[    7] loss: 0.122\n",
            "[    8] loss: 0.102\n",
            "[    9] loss: 0.089\n",
            "[   10] loss: 0.100\n",
            "[   11] loss: 0.085\n",
            "[   12] loss: 0.112\n",
            "[   13] loss: 0.101\n",
            "[   14] loss: 0.177\n",
            "[   15] loss: 0.128\n",
            "[   16] loss: 0.123\n",
            "[   17] loss: 0.137\n",
            "[   18] loss: 0.086\n",
            "[   19] loss: 0.200\n",
            "epoch:  214\n",
            "[    1] loss: 0.132\n",
            "[    2] loss: 0.153\n",
            "[    3] loss: 0.109\n",
            "[    4] loss: 0.146\n",
            "[    5] loss: 0.096\n",
            "[    6] loss: 0.119\n",
            "[    7] loss: 0.103\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.091\n",
            "[   10] loss: 0.146\n",
            "[   11] loss: 0.097\n",
            "[   12] loss: 0.112\n",
            "[   13] loss: 0.110\n",
            "[   14] loss: 0.132\n",
            "[   15] loss: 0.106\n",
            "[   16] loss: 0.131\n",
            "[   17] loss: 0.117\n",
            "[   18] loss: 0.108\n",
            "[   19] loss: 0.259\n",
            "epoch:  215\n",
            "[    1] loss: 0.093\n",
            "[    2] loss: 0.107\n",
            "[    3] loss: 0.104\n",
            "[    4] loss: 0.258\n",
            "[    5] loss: 0.128\n",
            "[    6] loss: 0.104\n",
            "[    7] loss: 0.177\n",
            "[    8] loss: 0.147\n",
            "[    9] loss: 0.134\n",
            "[   10] loss: 0.098\n",
            "[   11] loss: 0.078\n",
            "[   12] loss: 0.111\n",
            "[   13] loss: 0.120\n",
            "[   14] loss: 0.125\n",
            "[   15] loss: 0.095\n",
            "[   16] loss: 0.100\n",
            "[   17] loss: 0.122\n",
            "[   18] loss: 0.118\n",
            "[   19] loss: 0.308\n",
            "epoch:  216\n",
            "[    1] loss: 0.162\n",
            "[    2] loss: 0.078\n",
            "[    3] loss: 0.151\n",
            "[    4] loss: 0.118\n",
            "[    5] loss: 0.150\n",
            "[    6] loss: 0.151\n",
            "[    7] loss: 0.126\n",
            "[    8] loss: 0.087\n",
            "[    9] loss: 0.126\n",
            "[   10] loss: 0.100\n",
            "[   11] loss: 0.086\n",
            "[   12] loss: 0.125\n",
            "[   13] loss: 0.110\n",
            "[   14] loss: 0.231\n",
            "[   15] loss: 0.085\n",
            "[   16] loss: 0.110\n",
            "[   17] loss: 0.068\n",
            "[   18] loss: 0.115\n",
            "[   19] loss: 0.561\n",
            "train loss:  0.09278363093514652\n",
            "val loss:  0.19615545123815536\n",
            "epoch:  217\n",
            "[    1] loss: 0.116\n",
            "[    2] loss: 0.096\n",
            "[    3] loss: 0.089\n",
            "[    4] loss: 0.093\n",
            "[    5] loss: 0.105\n",
            "[    6] loss: 0.088\n",
            "[    7] loss: 0.135\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.127\n",
            "[   10] loss: 0.107\n",
            "[   11] loss: 0.189\n",
            "[   12] loss: 0.138\n",
            "[   13] loss: 0.092\n",
            "[   14] loss: 0.100\n",
            "[   15] loss: 0.093\n",
            "[   16] loss: 0.141\n",
            "[   17] loss: 0.096\n",
            "[   18] loss: 0.083\n",
            "[   19] loss: 0.399\n",
            "epoch:  218\n",
            "[    1] loss: 0.139\n",
            "[    2] loss: 0.110\n",
            "[    3] loss: 0.122\n",
            "[    4] loss: 0.091\n",
            "[    5] loss: 0.128\n",
            "[    6] loss: 0.164\n",
            "[    7] loss: 0.079\n",
            "[    8] loss: 0.119\n",
            "[    9] loss: 0.099\n",
            "[   10] loss: 0.112\n",
            "[   11] loss: 0.116\n",
            "[   12] loss: 0.115\n",
            "[   13] loss: 0.139\n",
            "[   14] loss: 0.104\n",
            "[   15] loss: 0.135\n",
            "[   16] loss: 0.116\n",
            "[   17] loss: 0.089\n",
            "[   18] loss: 0.105\n",
            "[   19] loss: 0.180\n",
            "epoch:  219\n",
            "[    1] loss: 0.097\n",
            "[    2] loss: 0.134\n",
            "[    3] loss: 0.117\n",
            "[    4] loss: 0.092\n",
            "[    5] loss: 0.130\n",
            "[    6] loss: 0.103\n",
            "[    7] loss: 0.102\n",
            "[    8] loss: 0.093\n",
            "[    9] loss: 0.100\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.105\n",
            "[   12] loss: 0.091\n",
            "[   13] loss: 0.126\n",
            "[   14] loss: 0.096\n",
            "[   15] loss: 0.106\n",
            "[   16] loss: 0.070\n",
            "[   17] loss: 0.099\n",
            "[   18] loss: 0.102\n",
            "[   19] loss: 0.230\n",
            "epoch:  220\n",
            "[    1] loss: 0.110\n",
            "[    2] loss: 0.121\n",
            "[    3] loss: 0.131\n",
            "[    4] loss: 0.147\n",
            "[    5] loss: 0.102\n",
            "[    6] loss: 0.107\n",
            "[    7] loss: 0.081\n",
            "[    8] loss: 0.081\n",
            "[    9] loss: 0.154\n",
            "[   10] loss: 0.078\n",
            "[   11] loss: 0.099\n",
            "[   12] loss: 0.110\n",
            "[   13] loss: 0.104\n",
            "[   14] loss: 0.101\n",
            "[   15] loss: 0.128\n",
            "[   16] loss: 0.125\n",
            "[   17] loss: 0.087\n",
            "[   18] loss: 0.088\n",
            "[   19] loss: 0.128\n",
            "epoch:  221\n",
            "[    1] loss: 0.096\n",
            "[    2] loss: 0.118\n",
            "[    3] loss: 0.120\n",
            "[    4] loss: 0.128\n",
            "[    5] loss: 0.086\n",
            "[    6] loss: 0.104\n",
            "[    7] loss: 0.084\n",
            "[    8] loss: 0.122\n",
            "[    9] loss: 0.118\n",
            "[   10] loss: 0.130\n",
            "[   11] loss: 0.154\n",
            "[   12] loss: 0.153\n",
            "[   13] loss: 0.089\n",
            "[   14] loss: 0.109\n",
            "[   15] loss: 0.178\n",
            "[   16] loss: 0.105\n",
            "[   17] loss: 0.107\n",
            "[   18] loss: 0.104\n",
            "[   19] loss: 0.109\n",
            "train loss:  0.09035825422581505\n",
            "val loss:  0.1608948279172182\n",
            "epoch:  222\n",
            "[    1] loss: 0.107\n",
            "[    2] loss: 0.155\n",
            "[    3] loss: 0.128\n",
            "[    4] loss: 0.129\n",
            "[    5] loss: 0.100\n",
            "[    6] loss: 0.084\n",
            "[    7] loss: 0.119\n",
            "[    8] loss: 0.075\n",
            "[    9] loss: 0.120\n",
            "[   10] loss: 0.115\n",
            "[   11] loss: 0.116\n",
            "[   12] loss: 0.104\n",
            "[   13] loss: 0.085\n",
            "[   14] loss: 0.105\n",
            "[   15] loss: 0.117\n",
            "[   16] loss: 0.114\n",
            "[   17] loss: 0.116\n",
            "[   18] loss: 0.158\n",
            "[   19] loss: 0.663\n",
            "epoch:  223\n",
            "[    1] loss: 0.126\n",
            "[    2] loss: 0.110\n",
            "[    3] loss: 0.126\n",
            "[    4] loss: 0.088\n",
            "[    5] loss: 0.132\n",
            "[    6] loss: 0.161\n",
            "[    7] loss: 0.107\n",
            "[    8] loss: 0.104\n",
            "[    9] loss: 0.123\n",
            "[   10] loss: 0.125\n",
            "[   11] loss: 0.123\n",
            "[   12] loss: 0.096\n",
            "[   13] loss: 0.106\n",
            "[   14] loss: 0.126\n",
            "[   15] loss: 0.122\n",
            "[   16] loss: 0.108\n",
            "[   17] loss: 0.093\n",
            "[   18] loss: 0.089\n",
            "[   19] loss: 0.182\n",
            "epoch:  224\n",
            "[    1] loss: 0.105\n",
            "[    2] loss: 0.112\n",
            "[    3] loss: 0.160\n",
            "[    4] loss: 0.101\n",
            "[    5] loss: 0.125\n",
            "[    6] loss: 0.095\n",
            "[    7] loss: 0.115\n",
            "[    8] loss: 0.124\n",
            "[    9] loss: 0.089\n",
            "[   10] loss: 0.083\n",
            "[   11] loss: 0.113\n",
            "[   12] loss: 0.102\n",
            "[   13] loss: 0.094\n",
            "[   14] loss: 0.093\n",
            "[   15] loss: 0.110\n",
            "[   16] loss: 0.113\n",
            "[   17] loss: 0.128\n",
            "[   18] loss: 0.095\n",
            "[   19] loss: 0.174\n",
            "epoch:  225\n",
            "[    1] loss: 0.114\n",
            "[    2] loss: 0.116\n",
            "[    3] loss: 0.076\n",
            "[    4] loss: 0.112\n",
            "[    5] loss: 0.109\n",
            "[    6] loss: 0.098\n",
            "[    7] loss: 0.106\n",
            "[    8] loss: 0.129\n",
            "[    9] loss: 0.152\n",
            "[   10] loss: 0.153\n",
            "[   11] loss: 0.157\n",
            "[   12] loss: 0.150\n",
            "[   13] loss: 0.102\n",
            "[   14] loss: 0.180\n",
            "[   15] loss: 0.097\n",
            "[   16] loss: 0.134\n",
            "[   17] loss: 0.155\n",
            "[   18] loss: 0.119\n",
            "[   19] loss: 0.236\n",
            "epoch:  226\n",
            "[    1] loss: 0.111\n",
            "[    2] loss: 0.104\n",
            "[    3] loss: 0.170\n",
            "[    4] loss: 0.102\n",
            "[    5] loss: 0.100\n",
            "[    6] loss: 0.103\n",
            "[    7] loss: 0.089\n",
            "[    8] loss: 0.092\n",
            "[    9] loss: 0.088\n",
            "[   10] loss: 0.109\n",
            "[   11] loss: 0.125\n",
            "[   12] loss: 0.105\n",
            "[   13] loss: 0.132\n",
            "[   14] loss: 0.104\n",
            "[   15] loss: 0.124\n",
            "[   16] loss: 0.116\n",
            "[   17] loss: 0.155\n",
            "[   18] loss: 0.157\n",
            "[   19] loss: 0.146\n",
            "train loss:  0.09520168635336791\n",
            "val loss:  0.17548837326467037\n",
            "epoch:  227\n",
            "[    1] loss: 0.094\n",
            "[    2] loss: 0.134\n",
            "[    3] loss: 0.099\n",
            "[    4] loss: 0.097\n",
            "[    5] loss: 0.128\n",
            "[    6] loss: 0.116\n",
            "[    7] loss: 0.099\n",
            "[    8] loss: 0.157\n",
            "[    9] loss: 0.106\n",
            "[   10] loss: 0.099\n",
            "[   11] loss: 0.149\n",
            "[   12] loss: 0.119\n",
            "[   13] loss: 0.086\n",
            "[   14] loss: 0.085\n",
            "[   15] loss: 0.137\n",
            "[   16] loss: 0.161\n",
            "[   17] loss: 0.132\n",
            "[   18] loss: 0.100\n",
            "[   19] loss: 0.117\n",
            "epoch:  228\n",
            "[    1] loss: 0.110\n",
            "[    2] loss: 0.126\n",
            "[    3] loss: 0.117\n",
            "[    4] loss: 0.101\n",
            "[    5] loss: 0.109\n",
            "[    6] loss: 0.118\n",
            "[    7] loss: 0.112\n",
            "[    8] loss: 0.090\n",
            "[    9] loss: 0.121\n",
            "[   10] loss: 0.101\n",
            "[   11] loss: 0.137\n",
            "[   12] loss: 0.068\n",
            "[   13] loss: 0.093\n",
            "[   14] loss: 0.115\n",
            "[   15] loss: 0.110\n",
            "[   16] loss: 0.104\n",
            "[   17] loss: 0.117\n",
            "[   18] loss: 0.127\n",
            "[   19] loss: 0.178\n",
            "epoch:  229\n",
            "[    1] loss: 0.177\n",
            "[    2] loss: 0.104\n",
            "[    3] loss: 0.112\n",
            "[    4] loss: 0.130\n",
            "[    5] loss: 0.105\n",
            "[    6] loss: 0.082\n",
            "[    7] loss: 0.089\n",
            "[    8] loss: 0.188\n",
            "[    9] loss: 0.103\n",
            "[   10] loss: 0.113\n",
            "[   11] loss: 0.087\n",
            "[   12] loss: 0.116\n",
            "[   13] loss: 0.088\n",
            "[   14] loss: 0.108\n",
            "[   15] loss: 0.102\n",
            "[   16] loss: 0.098\n",
            "[   17] loss: 0.153\n",
            "[   18] loss: 0.109\n",
            "[   19] loss: 0.238\n",
            "epoch:  230\n",
            "[    1] loss: 0.120\n",
            "[    2] loss: 0.130\n",
            "[    3] loss: 0.100\n",
            "[    4] loss: 0.134\n",
            "[    5] loss: 0.129\n",
            "[    6] loss: 0.122\n",
            "[    7] loss: 0.115\n",
            "[    8] loss: 0.108\n",
            "[    9] loss: 0.125\n",
            "[   10] loss: 0.119\n",
            "[   11] loss: 0.117\n",
            "[   12] loss: 0.099\n",
            "[   13] loss: 0.102\n",
            "[   14] loss: 0.149\n",
            "[   15] loss: 0.090\n",
            "[   16] loss: 0.091\n",
            "[   17] loss: 0.112\n",
            "[   18] loss: 0.100\n",
            "[   19] loss: 0.566\n",
            "epoch:  231\n",
            "[    1] loss: 0.108\n",
            "[    2] loss: 0.135\n",
            "[    3] loss: 0.081\n",
            "[    4] loss: 0.106\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.085\n",
            "[    7] loss: 0.103\n",
            "[    8] loss: 0.171\n",
            "[    9] loss: 0.078\n",
            "[   10] loss: 0.144\n",
            "[   11] loss: 0.109\n",
            "[   12] loss: 0.161\n",
            "[   13] loss: 0.106\n",
            "[   14] loss: 0.138\n",
            "[   15] loss: 0.111\n",
            "[   16] loss: 0.131\n",
            "[   17] loss: 0.115\n",
            "[   18] loss: 0.074\n",
            "[   19] loss: 0.139\n",
            "train loss:  0.09363037575145855\n",
            "val loss:  0.17233167216181755\n",
            "epoch:  232\n",
            "[    1] loss: 0.129\n",
            "[    2] loss: 0.093\n",
            "[    3] loss: 0.102\n",
            "[    4] loss: 0.083\n",
            "[    5] loss: 0.150\n",
            "[    6] loss: 0.120\n",
            "[    7] loss: 0.122\n",
            "[    8] loss: 0.108\n",
            "[    9] loss: 0.145\n",
            "[   10] loss: 0.098\n",
            "[   11] loss: 0.166\n",
            "[   12] loss: 0.103\n",
            "[   13] loss: 0.130\n",
            "[   14] loss: 0.092\n",
            "[   15] loss: 0.091\n",
            "[   16] loss: 0.133\n",
            "[   17] loss: 0.139\n",
            "[   18] loss: 0.116\n",
            "[   19] loss: 0.572\n",
            "epoch:  233\n",
            "[    1] loss: 0.090\n",
            "[    2] loss: 0.095\n",
            "[    3] loss: 0.093\n",
            "[    4] loss: 0.075\n",
            "[    5] loss: 0.115\n",
            "[    6] loss: 0.155\n",
            "[    7] loss: 0.132\n",
            "[    8] loss: 0.085\n",
            "[    9] loss: 0.084\n",
            "[   10] loss: 0.106\n",
            "[   11] loss: 0.112\n",
            "[   12] loss: 0.089\n",
            "[   13] loss: 0.167\n",
            "[   14] loss: 0.155\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.076\n",
            "[   17] loss: 0.101\n",
            "[   18] loss: 0.144\n",
            "[   19] loss: 0.147\n",
            "epoch:  234\n",
            "[    1] loss: 0.153\n",
            "[    2] loss: 0.076\n",
            "[    3] loss: 0.184\n",
            "[    4] loss: 0.098\n",
            "[    5] loss: 0.134\n",
            "[    6] loss: 0.140\n",
            "[    7] loss: 0.101\n",
            "[    8] loss: 0.102\n",
            "[    9] loss: 0.124\n",
            "[   10] loss: 0.121\n",
            "[   11] loss: 0.099\n",
            "[   12] loss: 0.121\n",
            "[   13] loss: 0.119\n",
            "[   14] loss: 0.107\n",
            "[   15] loss: 0.142\n",
            "[   16] loss: 0.151\n",
            "[   17] loss: 0.093\n",
            "[   18] loss: 0.109\n",
            "[   19] loss: 0.335\n",
            "epoch:  235\n",
            "[    1] loss: 0.118\n",
            "[    2] loss: 0.097\n",
            "[    3] loss: 0.106\n",
            "[    4] loss: 0.110\n",
            "[    5] loss: 0.094\n",
            "[    6] loss: 0.129\n",
            "[    7] loss: 0.155\n",
            "[    8] loss: 0.108\n",
            "[    9] loss: 0.143\n",
            "[   10] loss: 0.096\n",
            "[   11] loss: 0.137\n",
            "[   12] loss: 0.127\n",
            "[   13] loss: 0.117\n",
            "[   14] loss: 0.121\n",
            "[   15] loss: 0.122\n",
            "[   16] loss: 0.106\n",
            "[   17] loss: 0.108\n",
            "[   18] loss: 0.114\n",
            "[   19] loss: 0.447\n",
            "epoch:  236\n",
            "[    1] loss: 0.104\n",
            "[    2] loss: 0.087\n",
            "[    3] loss: 0.115\n",
            "[    4] loss: 0.106\n",
            "[    5] loss: 0.115\n",
            "[    6] loss: 0.119\n",
            "[    7] loss: 0.100\n",
            "[    8] loss: 0.095\n",
            "[    9] loss: 0.127\n",
            "[   10] loss: 0.112\n",
            "[   11] loss: 0.107\n",
            "[   12] loss: 0.112\n",
            "[   13] loss: 0.083\n",
            "[   14] loss: 0.120\n",
            "[   15] loss: 0.087\n",
            "[   16] loss: 0.095\n",
            "[   17] loss: 0.123\n",
            "[   18] loss: 0.131\n",
            "[   19] loss: 0.099\n",
            "train loss:  0.09921432215282146\n",
            "val loss:  0.16112101823091507\n",
            "epoch:  237\n",
            "[    1] loss: 0.118\n",
            "[    2] loss: 0.125\n",
            "[    3] loss: 0.101\n",
            "[    4] loss: 0.130\n",
            "[    5] loss: 0.202\n",
            "[    6] loss: 0.121\n",
            "[    7] loss: 0.099\n",
            "[    8] loss: 0.103\n",
            "[    9] loss: 0.113\n",
            "[   10] loss: 0.103\n",
            "[   11] loss: 0.138\n",
            "[   12] loss: 0.123\n",
            "[   13] loss: 0.179\n",
            "[   14] loss: 0.144\n",
            "[   15] loss: 0.102\n",
            "[   16] loss: 0.125\n",
            "[   17] loss: 0.119\n",
            "[   18] loss: 0.136\n",
            "[   19] loss: 0.134\n",
            "epoch:  238\n",
            "[    1] loss: 0.091\n",
            "[    2] loss: 0.101\n",
            "[    3] loss: 0.109\n",
            "[    4] loss: 0.087\n",
            "[    5] loss: 0.143\n",
            "[    6] loss: 0.107\n",
            "[    7] loss: 0.082\n",
            "[    8] loss: 0.130\n",
            "[    9] loss: 0.116\n",
            "[   10] loss: 0.147\n",
            "[   11] loss: 0.107\n",
            "[   12] loss: 0.099\n",
            "[   13] loss: 0.140\n",
            "[   14] loss: 0.107\n",
            "[   15] loss: 0.086\n",
            "[   16] loss: 0.094\n",
            "[   17] loss: 0.108\n",
            "[   18] loss: 0.135\n",
            "[   19] loss: 0.209\n",
            "epoch:  239\n",
            "[    1] loss: 0.100\n",
            "[    2] loss: 0.101\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.117\n",
            "[    5] loss: 0.104\n",
            "[    6] loss: 0.110\n",
            "[    7] loss: 0.118\n",
            "[    8] loss: 0.103\n",
            "[    9] loss: 0.092\n",
            "[   10] loss: 0.089\n",
            "[   11] loss: 0.103\n",
            "[   12] loss: 0.146\n",
            "[   13] loss: 0.110\n",
            "[   14] loss: 0.075\n",
            "[   15] loss: 0.145\n",
            "[   16] loss: 0.077\n",
            "[   17] loss: 0.121\n",
            "[   18] loss: 0.117\n",
            "[   19] loss: 0.315\n",
            "epoch:  240\n",
            "[    1] loss: 0.102\n",
            "[    2] loss: 0.134\n",
            "[    3] loss: 0.152\n",
            "[    4] loss: 0.133\n",
            "[    5] loss: 0.148\n",
            "[    6] loss: 0.089\n",
            "[    7] loss: 0.102\n",
            "[    8] loss: 0.100\n",
            "[    9] loss: 0.082\n",
            "[   10] loss: 0.150\n",
            "[   11] loss: 0.093\n",
            "[   12] loss: 0.110\n",
            "[   13] loss: 0.110\n",
            "[   14] loss: 0.116\n",
            "[   15] loss: 0.096\n",
            "[   16] loss: 0.145\n",
            "[   17] loss: 0.150\n",
            "[   18] loss: 0.097\n",
            "[   19] loss: 0.265\n",
            "epoch:  241\n",
            "[    1] loss: 0.108\n",
            "[    2] loss: 0.080\n",
            "[    3] loss: 0.155\n",
            "[    4] loss: 0.092\n",
            "[    5] loss: 0.122\n",
            "[    6] loss: 0.108\n",
            "[    7] loss: 0.107\n",
            "[    8] loss: 0.127\n",
            "[    9] loss: 0.110\n",
            "[   10] loss: 0.119\n",
            "[   11] loss: 0.204\n",
            "[   12] loss: 0.123\n",
            "[   13] loss: 0.153\n",
            "[   14] loss: 0.100\n",
            "[   15] loss: 0.125\n",
            "[   16] loss: 0.128\n",
            "[   17] loss: 0.138\n",
            "[   18] loss: 0.177\n",
            "[   19] loss: 0.933\n",
            "train loss:  0.09367685321280185\n",
            "val loss:  0.15779638104140759\n",
            "epoch:  242\n",
            "[    1] loss: 0.103\n",
            "[    2] loss: 0.092\n",
            "[    3] loss: 0.096\n",
            "[    4] loss: 0.156\n",
            "[    5] loss: 0.106\n",
            "[    6] loss: 0.140\n",
            "[    7] loss: 0.088\n",
            "[    8] loss: 0.092\n",
            "[    9] loss: 0.099\n",
            "[   10] loss: 0.131\n",
            "[   11] loss: 0.105\n",
            "[   12] loss: 0.117\n",
            "[   13] loss: 0.089\n",
            "[   14] loss: 0.120\n",
            "[   15] loss: 0.117\n",
            "[   16] loss: 0.121\n",
            "[   17] loss: 0.108\n",
            "[   18] loss: 0.103\n",
            "[   19] loss: 0.123\n",
            "epoch:  243\n",
            "[    1] loss: 0.083\n",
            "[    2] loss: 0.170\n",
            "[    3] loss: 0.111\n",
            "[    4] loss: 0.095\n",
            "[    5] loss: 0.109\n",
            "[    6] loss: 0.125\n",
            "[    7] loss: 0.139\n",
            "[    8] loss: 0.114\n",
            "[    9] loss: 0.089\n",
            "[   10] loss: 0.105\n",
            "[   11] loss: 0.103\n",
            "[   12] loss: 0.147\n",
            "[   13] loss: 0.106\n",
            "[   14] loss: 0.128\n",
            "[   15] loss: 0.090\n",
            "[   16] loss: 0.124\n",
            "[   17] loss: 0.123\n",
            "[   18] loss: 0.076\n",
            "[   19] loss: 0.412\n",
            "epoch:  244\n",
            "[    1] loss: 0.074\n",
            "[    2] loss: 0.152\n",
            "[    3] loss: 0.091\n",
            "[    4] loss: 0.237\n",
            "[    5] loss: 0.125\n",
            "[    6] loss: 0.081\n",
            "[    7] loss: 0.116\n",
            "[    8] loss: 0.153\n",
            "[    9] loss: 0.152\n",
            "[   10] loss: 0.099\n",
            "[   11] loss: 0.082\n",
            "[   12] loss: 0.092\n",
            "[   13] loss: 0.097\n",
            "[   14] loss: 0.080\n",
            "[   15] loss: 0.126\n",
            "[   16] loss: 0.152\n",
            "[   17] loss: 0.116\n",
            "[   18] loss: 0.102\n",
            "[   19] loss: 0.293\n",
            "epoch:  245\n",
            "[    1] loss: 0.084\n",
            "[    2] loss: 0.115\n",
            "[    3] loss: 0.085\n",
            "[    4] loss: 0.115\n",
            "[    5] loss: 0.101\n",
            "[    6] loss: 0.092\n",
            "[    7] loss: 0.074\n",
            "[    8] loss: 0.121\n",
            "[    9] loss: 0.083\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.111\n",
            "[   12] loss: 0.125\n",
            "[   13] loss: 0.091\n",
            "[   14] loss: 0.127\n",
            "[   15] loss: 0.120\n",
            "[   16] loss: 0.115\n",
            "[   17] loss: 0.109\n",
            "[   18] loss: 0.105\n",
            "[   19] loss: 0.189\n",
            "epoch:  246\n",
            "[    1] loss: 0.123\n",
            "[    2] loss: 0.080\n",
            "[    3] loss: 0.102\n",
            "[    4] loss: 0.112\n",
            "[    5] loss: 0.088\n",
            "[    6] loss: 0.107\n",
            "[    7] loss: 0.129\n",
            "[    8] loss: 0.206\n",
            "[    9] loss: 0.155\n",
            "[   10] loss: 0.114\n",
            "[   11] loss: 0.132\n",
            "[   12] loss: 0.098\n",
            "[   13] loss: 0.146\n",
            "[   14] loss: 0.138\n",
            "[   15] loss: 0.113\n",
            "[   16] loss: 0.132\n",
            "[   17] loss: 0.149\n",
            "[   18] loss: 0.130\n",
            "[   19] loss: 0.169\n",
            "train loss:  0.09822842782801565\n",
            "val loss:  0.18614681623876095\n",
            "epoch:  247\n",
            "[    1] loss: 0.108\n",
            "[    2] loss: 0.097\n",
            "[    3] loss: 0.093\n",
            "[    4] loss: 0.106\n",
            "[    5] loss: 0.095\n",
            "[    6] loss: 0.112\n",
            "[    7] loss: 0.164\n",
            "[    8] loss: 0.108\n",
            "[    9] loss: 0.155\n",
            "[   10] loss: 0.104\n",
            "[   11] loss: 0.087\n",
            "[   12] loss: 0.136\n",
            "[   13] loss: 0.092\n",
            "[   14] loss: 0.105\n",
            "[   15] loss: 0.168\n",
            "[   16] loss: 0.135\n",
            "[   17] loss: 0.102\n",
            "[   18] loss: 0.127\n",
            "[   19] loss: 0.206\n",
            "epoch:  248\n",
            "[    1] loss: 0.114\n",
            "[    2] loss: 0.138\n",
            "[    3] loss: 0.135\n",
            "[    4] loss: 0.116\n",
            "[    5] loss: 0.090\n",
            "[    6] loss: 0.078\n",
            "[    7] loss: 0.140\n",
            "[    8] loss: 0.100\n",
            "[    9] loss: 0.094\n",
            "[   10] loss: 0.121\n",
            "[   11] loss: 0.102\n",
            "[   12] loss: 0.100\n",
            "[   13] loss: 0.115\n",
            "[   14] loss: 0.119\n",
            "[   15] loss: 0.129\n",
            "[   16] loss: 0.151\n",
            "[   17] loss: 0.127\n",
            "[   18] loss: 0.072\n",
            "[   19] loss: 0.277\n",
            "epoch:  249\n",
            "[    1] loss: 0.082\n",
            "[    2] loss: 0.116\n",
            "[    3] loss: 0.103\n",
            "[    4] loss: 0.142\n",
            "[    5] loss: 0.100\n",
            "[    6] loss: 0.109\n",
            "[    7] loss: 0.116\n",
            "[    8] loss: 0.093\n",
            "[    9] loss: 0.098\n",
            "[   10] loss: 0.111\n",
            "[   11] loss: 0.076\n",
            "[   12] loss: 0.149\n",
            "[   13] loss: 0.092\n",
            "[   14] loss: 0.115\n",
            "[   15] loss: 0.088\n",
            "[   16] loss: 0.111\n",
            "[   17] loss: 0.103\n",
            "[   18] loss: 0.109\n",
            "[   19] loss: 0.126\n",
            "epoch:  250\n",
            "[    1] loss: 0.096\n",
            "[    2] loss: 0.112\n",
            "[    3] loss: 0.083\n",
            "[    4] loss: 0.109\n",
            "[    5] loss: 0.101\n",
            "[    6] loss: 0.108\n",
            "[    7] loss: 0.108\n",
            "[    8] loss: 0.125\n",
            "[    9] loss: 0.094\n",
            "[   10] loss: 0.089\n",
            "[   11] loss: 0.112\n",
            "[   12] loss: 0.117\n",
            "[   13] loss: 0.122\n",
            "[   14] loss: 0.124\n",
            "[   15] loss: 0.119\n",
            "[   16] loss: 0.098\n",
            "[   17] loss: 0.100\n",
            "[   18] loss: 0.088\n",
            "[   19] loss: 0.333\n",
            "epoch:  251\n",
            "[    1] loss: 0.095\n",
            "[    2] loss: 0.114\n",
            "[    3] loss: 0.155\n",
            "[    4] loss: 0.122\n",
            "[    5] loss: 0.110\n",
            "[    6] loss: 0.106\n",
            "[    7] loss: 0.097\n",
            "[    8] loss: 0.146\n",
            "[    9] loss: 0.138\n",
            "[   10] loss: 0.091\n",
            "[   11] loss: 0.127\n",
            "[   12] loss: 0.097\n",
            "[   13] loss: 0.155\n",
            "[   14] loss: 0.088\n",
            "[   15] loss: 0.187\n",
            "[   16] loss: 0.092\n",
            "[   17] loss: 0.105\n",
            "[   18] loss: 0.099\n",
            "[   19] loss: 0.181\n",
            "train loss:  0.09711315449984635\n",
            "val loss:  0.1853342391550541\n",
            "epoch:  252\n",
            "[    1] loss: 0.121\n",
            "[    2] loss: 0.124\n",
            "[    3] loss: 0.096\n",
            "[    4] loss: 0.180\n",
            "[    5] loss: 0.121\n",
            "[    6] loss: 0.120\n",
            "[    7] loss: 0.125\n",
            "[    8] loss: 0.185\n",
            "[    9] loss: 0.128\n",
            "[   10] loss: 0.118\n",
            "[   11] loss: 0.087\n",
            "[   12] loss: 0.109\n",
            "[   13] loss: 0.082\n",
            "[   14] loss: 0.103\n",
            "[   15] loss: 0.107\n",
            "[   16] loss: 0.109\n",
            "[   17] loss: 0.127\n",
            "[   18] loss: 0.088\n",
            "[   19] loss: 0.433\n",
            "epoch:  253\n",
            "[    1] loss: 0.105\n",
            "[    2] loss: 0.181\n",
            "[    3] loss: 0.113\n",
            "[    4] loss: 0.104\n",
            "[    5] loss: 0.139\n",
            "[    6] loss: 0.141\n",
            "[    7] loss: 0.083\n",
            "[    8] loss: 0.087\n",
            "[    9] loss: 0.126\n",
            "[   10] loss: 0.131\n",
            "[   11] loss: 0.143\n",
            "[   12] loss: 0.163\n",
            "[   13] loss: 0.159\n",
            "[   14] loss: 0.092\n",
            "[   15] loss: 0.111\n",
            "[   16] loss: 0.140\n",
            "[   17] loss: 0.087\n",
            "[   18] loss: 0.156\n",
            "[   19] loss: 0.405\n",
            "epoch:  254\n",
            "[    1] loss: 0.128\n",
            "[    2] loss: 0.123\n",
            "[    3] loss: 0.117\n",
            "[    4] loss: 0.105\n",
            "[    5] loss: 0.095\n",
            "[    6] loss: 0.111\n",
            "[    7] loss: 0.121\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.126\n",
            "[   10] loss: 0.103\n",
            "[   11] loss: 0.092\n",
            "[   12] loss: 0.099\n",
            "[   13] loss: 0.106\n",
            "[   14] loss: 0.106\n",
            "[   15] loss: 0.110\n",
            "[   16] loss: 0.157\n",
            "[   17] loss: 0.087\n",
            "[   18] loss: 0.171\n",
            "[   19] loss: 0.219\n",
            "epoch:  255\n",
            "[    1] loss: 0.086\n",
            "[    2] loss: 0.139\n",
            "[    3] loss: 0.129\n",
            "[    4] loss: 0.099\n",
            "[    5] loss: 0.107\n",
            "[    6] loss: 0.084\n",
            "[    7] loss: 0.119\n",
            "[    8] loss: 0.133\n",
            "[    9] loss: 0.091\n",
            "[   10] loss: 0.116\n",
            "[   11] loss: 0.148\n",
            "[   12] loss: 0.091\n",
            "[   13] loss: 0.121\n",
            "[   14] loss: 0.098\n",
            "[   15] loss: 0.090\n",
            "[   16] loss: 0.107\n",
            "[   17] loss: 0.114\n",
            "[   18] loss: 0.133\n",
            "[   19] loss: 0.160\n",
            "epoch:  256\n",
            "[    1] loss: 0.107\n",
            "[    2] loss: 0.130\n",
            "[    3] loss: 0.081\n",
            "[    4] loss: 0.108\n",
            "[    5] loss: 0.100\n",
            "[    6] loss: 0.111\n",
            "[    7] loss: 0.127\n",
            "[    8] loss: 0.141\n",
            "[    9] loss: 0.102\n",
            "[   10] loss: 0.100\n",
            "[   11] loss: 0.123\n",
            "[   12] loss: 0.079\n",
            "[   13] loss: 0.106\n",
            "[   14] loss: 0.087\n",
            "[   15] loss: 0.132\n",
            "[   16] loss: 0.146\n",
            "[   17] loss: 0.075\n",
            "[   18] loss: 0.093\n",
            "[   19] loss: 0.185\n",
            "train loss:  0.09771510284832295\n",
            "val loss:  0.1976135466247797\n",
            "epoch:  257\n",
            "[    1] loss: 0.089\n",
            "[    2] loss: 0.087\n",
            "[    3] loss: 0.218\n",
            "[    4] loss: 0.073\n",
            "[    5] loss: 0.101\n",
            "[    6] loss: 0.126\n",
            "[    7] loss: 0.142\n",
            "[    8] loss: 0.088\n",
            "[    9] loss: 0.087\n",
            "[   10] loss: 0.127\n",
            "[   11] loss: 0.119\n",
            "[   12] loss: 0.155\n",
            "[   13] loss: 0.112\n",
            "[   14] loss: 0.119\n",
            "[   15] loss: 0.115\n",
            "[   16] loss: 0.089\n",
            "[   17] loss: 0.088\n",
            "[   18] loss: 0.115\n",
            "[   19] loss: 0.272\n",
            "epoch:  258\n",
            "[    1] loss: 0.108\n",
            "[    2] loss: 0.121\n",
            "[    3] loss: 0.191\n",
            "[    4] loss: 0.111\n",
            "[    5] loss: 0.105\n",
            "[    6] loss: 0.134\n",
            "[    7] loss: 0.092\n",
            "[    8] loss: 0.108\n",
            "[    9] loss: 0.162\n",
            "[   10] loss: 0.107\n",
            "[   11] loss: 0.092\n",
            "[   12] loss: 0.085\n",
            "[   13] loss: 0.129\n",
            "[   14] loss: 0.077\n",
            "[   15] loss: 0.086\n",
            "[   16] loss: 0.087\n",
            "[   17] loss: 0.105\n",
            "[   18] loss: 0.120\n",
            "[   19] loss: 0.187\n",
            "epoch:  259\n",
            "[    1] loss: 0.129\n",
            "[    2] loss: 0.113\n",
            "[    3] loss: 0.113\n",
            "[    4] loss: 0.121\n",
            "[    5] loss: 0.095\n",
            "[    6] loss: 0.120\n",
            "[    7] loss: 0.210\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.128\n",
            "[   10] loss: 0.101\n",
            "[   11] loss: 0.194\n",
            "[   12] loss: 0.087\n",
            "[   13] loss: 0.097\n",
            "[   14] loss: 0.124\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.138\n",
            "[   17] loss: 0.105\n",
            "[   18] loss: 0.106\n",
            "[   19] loss: 0.545\n",
            "epoch:  260\n",
            "[    1] loss: 0.106\n",
            "[    2] loss: 0.114\n",
            "[    3] loss: 0.093\n",
            "[    4] loss: 0.096\n",
            "[    5] loss: 0.098\n",
            "[    6] loss: 0.090\n",
            "[    7] loss: 0.092\n",
            "[    8] loss: 0.146\n",
            "[    9] loss: 0.132\n",
            "[   10] loss: 0.174\n",
            "[   11] loss: 0.153\n",
            "[   12] loss: 0.105\n",
            "[   13] loss: 0.102\n",
            "[   14] loss: 0.121\n",
            "[   15] loss: 0.098\n",
            "[   16] loss: 0.141\n",
            "[   17] loss: 0.084\n",
            "[   18] loss: 0.123\n",
            "[   19] loss: 0.633\n",
            "epoch:  261\n",
            "[    1] loss: 0.073\n",
            "[    2] loss: 0.092\n",
            "[    3] loss: 0.165\n",
            "[    4] loss: 0.083\n",
            "[    5] loss: 0.170\n",
            "[    6] loss: 0.108\n",
            "[    7] loss: 0.098\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.105\n",
            "[   10] loss: 0.071\n",
            "[   11] loss: 0.111\n",
            "[   12] loss: 0.083\n",
            "[   13] loss: 0.160\n",
            "[   14] loss: 0.175\n",
            "[   15] loss: 0.154\n",
            "[   16] loss: 0.061\n",
            "[   17] loss: 0.109\n",
            "[   18] loss: 0.132\n",
            "[   19] loss: 0.263\n",
            "train loss:  0.09306084865923314\n",
            "val loss:  0.18595555797219276\n",
            "epoch:  262\n",
            "[    1] loss: 0.099\n",
            "[    2] loss: 0.158\n",
            "[    3] loss: 0.140\n",
            "[    4] loss: 0.087\n",
            "[    5] loss: 0.077\n",
            "[    6] loss: 0.094\n",
            "[    7] loss: 0.108\n",
            "[    8] loss: 0.089\n",
            "[    9] loss: 0.117\n",
            "[   10] loss: 0.104\n",
            "[   11] loss: 0.160\n",
            "[   12] loss: 0.098\n",
            "[   13] loss: 0.111\n",
            "[   14] loss: 0.124\n",
            "[   15] loss: 0.109\n",
            "[   16] loss: 0.101\n",
            "[   17] loss: 0.149\n",
            "[   18] loss: 0.122\n",
            "[   19] loss: 0.268\n",
            "epoch:  263\n",
            "[    1] loss: 0.121\n",
            "[    2] loss: 0.154\n",
            "[    3] loss: 0.100\n",
            "[    4] loss: 0.185\n",
            "[    5] loss: 0.081\n",
            "[    6] loss: 0.188\n",
            "[    7] loss: 0.179\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.108\n",
            "[   10] loss: 0.133\n",
            "[   11] loss: 0.202\n",
            "[   12] loss: 0.118\n",
            "[   13] loss: 0.096\n",
            "[   14] loss: 0.122\n",
            "[   15] loss: 0.148\n",
            "[   16] loss: 0.089\n",
            "[   17] loss: 0.088\n",
            "[   18] loss: 0.073\n",
            "[   19] loss: 0.203\n",
            "epoch:  264\n",
            "[    1] loss: 0.090\n",
            "[    2] loss: 0.127\n",
            "[    3] loss: 0.077\n",
            "[    4] loss: 0.106\n",
            "[    5] loss: 0.082\n",
            "[    6] loss: 0.145\n",
            "[    7] loss: 0.100\n",
            "[    8] loss: 0.128\n",
            "[    9] loss: 0.136\n",
            "[   10] loss: 0.132\n",
            "[   11] loss: 0.121\n",
            "[   12] loss: 0.096\n",
            "[   13] loss: 0.092\n",
            "[   14] loss: 0.089\n",
            "[   15] loss: 0.096\n",
            "[   16] loss: 0.097\n",
            "[   17] loss: 0.134\n",
            "[   18] loss: 0.120\n",
            "[   19] loss: 0.223\n",
            "epoch:  265\n",
            "[    1] loss: 0.160\n",
            "[    2] loss: 0.086\n",
            "[    3] loss: 0.115\n",
            "[    4] loss: 0.103\n",
            "[    5] loss: 0.131\n",
            "[    6] loss: 0.104\n",
            "[    7] loss: 0.105\n",
            "[    8] loss: 0.108\n",
            "[    9] loss: 0.128\n",
            "[   10] loss: 0.079\n",
            "[   11] loss: 0.103\n",
            "[   12] loss: 0.156\n",
            "[   13] loss: 0.115\n",
            "[   14] loss: 0.100\n",
            "[   15] loss: 0.123\n",
            "[   16] loss: 0.076\n",
            "[   17] loss: 0.131\n",
            "[   18] loss: 0.097\n",
            "[   19] loss: 0.167\n",
            "epoch:  266\n",
            "[    1] loss: 0.135\n",
            "[    2] loss: 0.107\n",
            "[    3] loss: 0.172\n",
            "[    4] loss: 0.115\n",
            "[    5] loss: 0.176\n",
            "[    6] loss: 0.113\n",
            "[    7] loss: 0.094\n",
            "[    8] loss: 0.118\n",
            "[    9] loss: 0.132\n",
            "[   10] loss: 0.128\n",
            "[   11] loss: 0.082\n",
            "[   12] loss: 0.091\n",
            "[   13] loss: 0.122\n",
            "[   14] loss: 0.100\n",
            "[   15] loss: 0.123\n",
            "[   16] loss: 0.136\n",
            "[   17] loss: 0.126\n",
            "[   18] loss: 0.144\n",
            "[   19] loss: 0.264\n",
            "train loss:  0.09352262383874725\n",
            "val loss:  0.1814270969480276\n",
            "epoch:  267\n",
            "[    1] loss: 0.115\n",
            "[    2] loss: 0.140\n",
            "[    3] loss: 0.088\n",
            "[    4] loss: 0.148\n",
            "[    5] loss: 0.099\n",
            "[    6] loss: 0.105\n",
            "[    7] loss: 0.151\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.100\n",
            "[   10] loss: 0.093\n",
            "[   11] loss: 0.127\n",
            "[   12] loss: 0.094\n",
            "[   13] loss: 0.085\n",
            "[   14] loss: 0.110\n",
            "[   15] loss: 0.225\n",
            "[   16] loss: 0.143\n",
            "[   17] loss: 0.099\n",
            "[   18] loss: 0.100\n",
            "[   19] loss: 0.147\n",
            "epoch:  268\n",
            "[    1] loss: 0.149\n",
            "[    2] loss: 0.104\n",
            "[    3] loss: 0.122\n",
            "[    4] loss: 0.080\n",
            "[    5] loss: 0.101\n",
            "[    6] loss: 0.231\n",
            "[    7] loss: 0.144\n",
            "[    8] loss: 0.125\n",
            "[    9] loss: 0.097\n",
            "[   10] loss: 0.136\n",
            "[   11] loss: 0.101\n",
            "[   12] loss: 0.093\n",
            "[   13] loss: 0.122\n",
            "[   14] loss: 0.091\n",
            "[   15] loss: 0.085\n",
            "[   16] loss: 0.142\n",
            "[   17] loss: 0.134\n",
            "[   18] loss: 0.089\n",
            "[   19] loss: 0.258\n",
            "epoch:  269\n",
            "[    1] loss: 0.109\n",
            "[    2] loss: 0.122\n",
            "[    3] loss: 0.095\n",
            "[    4] loss: 0.114\n",
            "[    5] loss: 0.111\n",
            "[    6] loss: 0.118\n",
            "[    7] loss: 0.144\n",
            "[    8] loss: 0.121\n",
            "[    9] loss: 0.132\n",
            "[   10] loss: 0.134\n",
            "[   11] loss: 0.095\n",
            "[   12] loss: 0.098\n",
            "[   13] loss: 0.101\n",
            "[   14] loss: 0.112\n",
            "[   15] loss: 0.153\n",
            "[   16] loss: 0.135\n",
            "[   17] loss: 0.126\n",
            "[   18] loss: 0.108\n",
            "[   19] loss: 0.367\n",
            "epoch:  270\n",
            "[    1] loss: 0.135\n",
            "[    2] loss: 0.087\n",
            "[    3] loss: 0.111\n",
            "[    4] loss: 0.120\n",
            "[    5] loss: 0.157\n",
            "[    6] loss: 0.134\n",
            "[    7] loss: 0.085\n",
            "[    8] loss: 0.191\n",
            "[    9] loss: 0.091\n",
            "[   10] loss: 0.091\n",
            "[   11] loss: 0.164\n",
            "[   12] loss: 0.170\n",
            "[   13] loss: 0.101\n",
            "[   14] loss: 0.099\n",
            "[   15] loss: 0.137\n",
            "[   16] loss: 0.126\n",
            "[   17] loss: 0.167\n",
            "[   18] loss: 0.094\n",
            "[   19] loss: 0.207\n",
            "epoch:  271\n",
            "[    1] loss: 0.106\n",
            "[    2] loss: 0.125\n",
            "[    3] loss: 0.074\n",
            "[    4] loss: 0.077\n",
            "[    5] loss: 0.095\n",
            "[    6] loss: 0.134\n",
            "[    7] loss: 0.197\n",
            "[    8] loss: 0.145\n",
            "[    9] loss: 0.110\n",
            "[   10] loss: 0.096\n",
            "[   11] loss: 0.101\n",
            "[   12] loss: 0.081\n",
            "[   13] loss: 0.146\n",
            "[   14] loss: 0.124\n",
            "[   15] loss: 0.091\n",
            "[   16] loss: 0.098\n",
            "[   17] loss: 0.179\n",
            "[   18] loss: 0.125\n",
            "[   19] loss: 0.193\n",
            "train loss:  0.09812172587193987\n",
            "val loss:  0.19182895682752132\n",
            "epoch:  272\n",
            "[    1] loss: 0.164\n",
            "[    2] loss: 0.094\n",
            "[    3] loss: 0.071\n",
            "[    4] loss: 0.118\n",
            "[    5] loss: 0.093\n",
            "[    6] loss: 0.116\n",
            "[    7] loss: 0.165\n",
            "[    8] loss: 0.082\n",
            "[    9] loss: 0.187\n",
            "[   10] loss: 0.111\n",
            "[   11] loss: 0.100\n",
            "[   12] loss: 0.107\n",
            "[   13] loss: 0.159\n",
            "[   14] loss: 0.103\n",
            "[   15] loss: 0.116\n",
            "[   16] loss: 0.102\n",
            "[   17] loss: 0.088\n",
            "[   18] loss: 0.080\n",
            "[   19] loss: 0.429\n",
            "epoch:  273\n",
            "[    1] loss: 0.083\n",
            "[    2] loss: 0.149\n",
            "[    3] loss: 0.087\n",
            "[    4] loss: 0.106\n",
            "[    5] loss: 0.142\n",
            "[    6] loss: 0.123\n",
            "[    7] loss: 0.120\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.101\n",
            "[   10] loss: 0.210\n",
            "[   11] loss: 0.139\n",
            "[   12] loss: 0.104\n",
            "[   13] loss: 0.120\n",
            "[   14] loss: 0.104\n",
            "[   15] loss: 0.133\n",
            "[   16] loss: 0.130\n",
            "[   17] loss: 0.157\n",
            "[   18] loss: 0.099\n",
            "[   19] loss: 0.332\n",
            "epoch:  274\n",
            "[    1] loss: 0.115\n",
            "[    2] loss: 0.092\n",
            "[    3] loss: 0.109\n",
            "[    4] loss: 0.128\n",
            "[    5] loss: 0.091\n",
            "[    6] loss: 0.104\n",
            "[    7] loss: 0.161\n",
            "[    8] loss: 0.131\n",
            "[    9] loss: 0.143\n",
            "[   10] loss: 0.081\n",
            "[   11] loss: 0.127\n",
            "[   12] loss: 0.100\n",
            "[   13] loss: 0.092\n",
            "[   14] loss: 0.139\n",
            "[   15] loss: 0.167\n",
            "[   16] loss: 0.095\n",
            "[   17] loss: 0.104\n",
            "[   18] loss: 0.121\n",
            "[   19] loss: 0.231\n",
            "epoch:  275\n",
            "[    1] loss: 0.125\n",
            "[    2] loss: 0.114\n",
            "[    3] loss: 0.095\n",
            "[    4] loss: 0.089\n",
            "[    5] loss: 0.098\n",
            "[    6] loss: 0.096\n",
            "[    7] loss: 0.108\n",
            "[    8] loss: 0.087\n",
            "[    9] loss: 0.165\n",
            "[   10] loss: 0.177\n",
            "[   11] loss: 0.077\n",
            "[   12] loss: 0.099\n",
            "[   13] loss: 0.200\n",
            "[   14] loss: 0.106\n",
            "[   15] loss: 0.096\n",
            "[   16] loss: 0.107\n",
            "[   17] loss: 0.136\n",
            "[   18] loss: 0.116\n",
            "[   19] loss: 0.390\n",
            "epoch:  276\n",
            "[    1] loss: 0.099\n",
            "[    2] loss: 0.140\n",
            "[    3] loss: 0.143\n",
            "[    4] loss: 0.089\n",
            "[    5] loss: 0.087\n",
            "[    6] loss: 0.107\n",
            "[    7] loss: 0.088\n",
            "[    8] loss: 0.131\n",
            "[    9] loss: 0.120\n",
            "[   10] loss: 0.116\n",
            "[   11] loss: 0.135\n",
            "[   12] loss: 0.101\n",
            "[   13] loss: 0.139\n",
            "[   14] loss: 0.086\n",
            "[   15] loss: 0.109\n",
            "[   16] loss: 0.103\n",
            "[   17] loss: 0.085\n",
            "[   18] loss: 0.100\n",
            "[   19] loss: 0.185\n",
            "train loss:  0.09080911101773381\n",
            "val loss:  0.1748119778931141\n",
            "epoch:  277\n",
            "[    1] loss: 0.092\n",
            "[    2] loss: 0.136\n",
            "[    3] loss: 0.108\n",
            "[    4] loss: 0.154\n",
            "[    5] loss: 0.126\n",
            "[    6] loss: 0.123\n",
            "[    7] loss: 0.110\n",
            "[    8] loss: 0.088\n",
            "[    9] loss: 0.120\n",
            "[   10] loss: 0.084\n",
            "[   11] loss: 0.110\n",
            "[   12] loss: 0.130\n",
            "[   13] loss: 0.133\n",
            "[   14] loss: 0.120\n",
            "[   15] loss: 0.087\n",
            "[   16] loss: 0.150\n",
            "[   17] loss: 0.102\n",
            "[   18] loss: 0.128\n",
            "[   19] loss: 0.132\n",
            "epoch:  278\n",
            "[    1] loss: 0.093\n",
            "[    2] loss: 0.089\n",
            "[    3] loss: 0.105\n",
            "[    4] loss: 0.103\n",
            "[    5] loss: 0.171\n",
            "[    6] loss: 0.111\n",
            "[    7] loss: 0.148\n",
            "[    8] loss: 0.129\n",
            "[    9] loss: 0.087\n",
            "[   10] loss: 0.081\n",
            "[   11] loss: 0.090\n",
            "[   12] loss: 0.107\n",
            "[   13] loss: 0.131\n",
            "[   14] loss: 0.085\n",
            "[   15] loss: 0.093\n",
            "[   16] loss: 0.106\n",
            "[   17] loss: 0.137\n",
            "[   18] loss: 0.078\n",
            "[   19] loss: 0.605\n",
            "epoch:  279\n",
            "[    1] loss: 0.124\n",
            "[    2] loss: 0.131\n",
            "[    3] loss: 0.103\n",
            "[    4] loss: 0.134\n",
            "[    5] loss: 0.093\n",
            "[    6] loss: 0.116\n",
            "[    7] loss: 0.094\n",
            "[    8] loss: 0.091\n",
            "[    9] loss: 0.102\n",
            "[   10] loss: 0.086\n",
            "[   11] loss: 0.121\n",
            "[   12] loss: 0.112\n",
            "[   13] loss: 0.115\n",
            "[   14] loss: 0.147\n",
            "[   15] loss: 0.117\n",
            "[   16] loss: 0.152\n",
            "[   17] loss: 0.091\n",
            "[   18] loss: 0.112\n",
            "[   19] loss: 0.175\n",
            "epoch:  280\n",
            "[    1] loss: 0.157\n",
            "[    2] loss: 0.094\n",
            "[    3] loss: 0.112\n",
            "[    4] loss: 0.090\n",
            "[    5] loss: 0.153\n",
            "[    6] loss: 0.123\n",
            "[    7] loss: 0.095\n",
            "[    8] loss: 0.123\n",
            "[    9] loss: 0.107\n",
            "[   10] loss: 0.099\n",
            "[   11] loss: 0.109\n",
            "[   12] loss: 0.108\n",
            "[   13] loss: 0.095\n",
            "[   14] loss: 0.174\n",
            "[   15] loss: 0.104\n",
            "[   16] loss: 0.090\n",
            "[   17] loss: 0.097\n",
            "[   18] loss: 0.123\n",
            "[   19] loss: 0.172\n",
            "epoch:  281\n",
            "[    1] loss: 0.130\n",
            "[    2] loss: 0.129\n",
            "[    3] loss: 0.109\n",
            "[    4] loss: 0.151\n",
            "[    5] loss: 0.101\n",
            "[    6] loss: 0.144\n",
            "[    7] loss: 0.103\n",
            "[    8] loss: 0.123\n",
            "[    9] loss: 0.096\n",
            "[   10] loss: 0.159\n",
            "[   11] loss: 0.124\n",
            "[   12] loss: 0.095\n",
            "[   13] loss: 0.119\n",
            "[   14] loss: 0.101\n",
            "[   15] loss: 0.069\n",
            "[   16] loss: 0.088\n",
            "[   17] loss: 0.112\n",
            "[   18] loss: 0.100\n",
            "[   19] loss: 0.271\n",
            "train loss:  0.09359321222804926\n",
            "val loss:  0.17296084389090538\n",
            "epoch:  282\n",
            "[    1] loss: 0.141\n",
            "[    2] loss: 0.100\n",
            "[    3] loss: 0.155\n",
            "[    4] loss: 0.132\n",
            "[    5] loss: 0.099\n",
            "[    6] loss: 0.094\n",
            "[    7] loss: 0.126\n",
            "[    8] loss: 0.148\n",
            "[    9] loss: 0.098\n",
            "[   10] loss: 0.092\n",
            "[   11] loss: 0.157\n",
            "[   12] loss: 0.150\n",
            "[   13] loss: 0.129\n",
            "[   14] loss: 0.134\n",
            "[   15] loss: 0.110\n",
            "[   16] loss: 0.132\n",
            "[   17] loss: 0.092\n",
            "[   18] loss: 0.195\n",
            "[   19] loss: 0.197\n",
            "epoch:  283\n",
            "[    1] loss: 0.107\n",
            "[    2] loss: 0.105\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.099\n",
            "[    5] loss: 0.130\n",
            "[    6] loss: 0.113\n",
            "[    7] loss: 0.127\n",
            "[    8] loss: 0.118\n",
            "[    9] loss: 0.099\n",
            "[   10] loss: 0.120\n",
            "[   11] loss: 0.115\n",
            "[   12] loss: 0.099\n",
            "[   13] loss: 0.095\n",
            "[   14] loss: 0.109\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.125\n",
            "[   17] loss: 0.099\n",
            "[   18] loss: 0.100\n",
            "[   19] loss: 0.169\n",
            "epoch:  284\n",
            "[    1] loss: 0.150\n",
            "[    2] loss: 0.092\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.081\n",
            "[    5] loss: 0.103\n",
            "[    6] loss: 0.108\n",
            "[    7] loss: 0.120\n",
            "[    8] loss: 0.093\n",
            "[    9] loss: 0.112\n",
            "[   10] loss: 0.103\n",
            "[   11] loss: 0.131\n",
            "[   12] loss: 0.111\n",
            "[   13] loss: 0.137\n",
            "[   14] loss: 0.090\n",
            "[   15] loss: 0.168\n",
            "[   16] loss: 0.074\n",
            "[   17] loss: 0.101\n",
            "[   18] loss: 0.116\n",
            "[   19] loss: 0.138\n",
            "epoch:  285\n",
            "[    1] loss: 0.105\n",
            "[    2] loss: 0.130\n",
            "[    3] loss: 0.123\n",
            "[    4] loss: 0.136\n",
            "[    5] loss: 0.078\n",
            "[    6] loss: 0.099\n",
            "[    7] loss: 0.127\n",
            "[    8] loss: 0.127\n",
            "[    9] loss: 0.087\n",
            "[   10] loss: 0.079\n",
            "[   11] loss: 0.106\n",
            "[   12] loss: 0.130\n",
            "[   13] loss: 0.091\n",
            "[   14] loss: 0.138\n",
            "[   15] loss: 0.112\n",
            "[   16] loss: 0.139\n",
            "[   17] loss: 0.113\n",
            "[   18] loss: 0.100\n",
            "[   19] loss: 0.162\n",
            "epoch:  286\n",
            "[    1] loss: 0.105\n",
            "[    2] loss: 0.158\n",
            "[    3] loss: 0.094\n",
            "[    4] loss: 0.088\n",
            "[    5] loss: 0.139\n",
            "[    6] loss: 0.102\n",
            "[    7] loss: 0.130\n",
            "[    8] loss: 0.209\n",
            "[    9] loss: 0.092\n",
            "[   10] loss: 0.094\n",
            "[   11] loss: 0.088\n",
            "[   12] loss: 0.088\n",
            "[   13] loss: 0.163\n",
            "[   14] loss: 0.096\n",
            "[   15] loss: 0.078\n",
            "[   16] loss: 0.119\n",
            "[   17] loss: 0.097\n",
            "[   18] loss: 0.098\n",
            "[   19] loss: 0.568\n",
            "train loss:  0.0941581220146926\n",
            "val loss:  0.18325206451117992\n",
            "epoch:  287\n",
            "[    1] loss: 0.116\n",
            "[    2] loss: 0.101\n",
            "[    3] loss: 0.181\n",
            "[    4] loss: 0.146\n",
            "[    5] loss: 0.145\n",
            "[    6] loss: 0.151\n",
            "[    7] loss: 0.117\n",
            "[    8] loss: 0.136\n",
            "[    9] loss: 0.139\n",
            "[   10] loss: 0.103\n",
            "[   11] loss: 0.110\n",
            "[   12] loss: 0.141\n",
            "[   13] loss: 0.106\n",
            "[   14] loss: 0.112\n",
            "[   15] loss: 0.113\n",
            "[   16] loss: 0.125\n",
            "[   17] loss: 0.154\n",
            "[   18] loss: 0.124\n",
            "[   19] loss: 0.385\n",
            "epoch:  288\n",
            "[    1] loss: 0.108\n",
            "[    2] loss: 0.094\n",
            "[    3] loss: 0.096\n",
            "[    4] loss: 0.137\n",
            "[    5] loss: 0.132\n",
            "[    6] loss: 0.135\n",
            "[    7] loss: 0.168\n",
            "[    8] loss: 0.153\n",
            "[    9] loss: 0.111\n",
            "[   10] loss: 0.117\n",
            "[   11] loss: 0.089\n",
            "[   12] loss: 0.103\n",
            "[   13] loss: 0.116\n",
            "[   14] loss: 0.113\n",
            "[   15] loss: 0.126\n",
            "[   16] loss: 0.097\n",
            "[   17] loss: 0.091\n",
            "[   18] loss: 0.078\n",
            "[   19] loss: 0.906\n",
            "epoch:  289\n",
            "[    1] loss: 0.119\n",
            "[    2] loss: 0.105\n",
            "[    3] loss: 0.093\n",
            "[    4] loss: 0.110\n",
            "[    5] loss: 0.117\n",
            "[    6] loss: 0.112\n",
            "[    7] loss: 0.116\n",
            "[    8] loss: 0.210\n",
            "[    9] loss: 0.114\n",
            "[   10] loss: 0.126\n",
            "[   11] loss: 0.105\n",
            "[   12] loss: 0.128\n",
            "[   13] loss: 0.091\n",
            "[   14] loss: 0.126\n",
            "[   15] loss: 0.103\n",
            "[   16] loss: 0.100\n",
            "[   17] loss: 0.108\n",
            "[   18] loss: 0.122\n",
            "[   19] loss: 0.188\n",
            "epoch:  290\n",
            "[    1] loss: 0.073\n",
            "[    2] loss: 0.148\n",
            "[    3] loss: 0.079\n",
            "[    4] loss: 0.116\n",
            "[    5] loss: 0.097\n",
            "[    6] loss: 0.134\n",
            "[    7] loss: 0.118\n",
            "[    8] loss: 0.125\n",
            "[    9] loss: 0.110\n",
            "[   10] loss: 0.122\n",
            "[   11] loss: 0.096\n",
            "[   12] loss: 0.117\n",
            "[   13] loss: 0.126\n",
            "[   14] loss: 0.115\n",
            "[   15] loss: 0.093\n",
            "[   16] loss: 0.135\n",
            "[   17] loss: 0.150\n",
            "[   18] loss: 0.091\n",
            "[   19] loss: 0.156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2G2aIj04AYR"
      },
      "source": [
        "np.save(os.path.join(save_path, \"logs.npy\"), logs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8kvMbkg5kZl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}