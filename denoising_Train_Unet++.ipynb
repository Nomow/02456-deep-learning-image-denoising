{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "denoising_Train_Unet++.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "3506496467e24fa9a37c82bb6681376b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c1cc030b99a94cb09d470ac6a84626b9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c0c87a1430984ab2956d707ddb488cc4",
              "IPY_MODEL_ab1b608d66bd4beeb563ad149071373a",
              "IPY_MODEL_0dceeab030e1400bbc3b506a37244e4f"
            ]
          }
        },
        "c1cc030b99a94cb09d470ac6a84626b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c0c87a1430984ab2956d707ddb488cc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_0d14a37fa3ea4e6e898c3b2379889cb2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_4d52514e22cc438abf26cf5ab7d49591"
          }
        },
        "ab1b608d66bd4beeb563ad149071373a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_eaf85ea01c6042fc977f1a7bbbab6a50",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 36988158,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 36988158,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3022f80801f243f59803d31bf88ddd06"
          }
        },
        "0dceeab030e1400bbc3b506a37244e4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b60d4fe8139a4318abe35b8a54ad6a03",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 35.3M/35.3M [00:00&lt;00:00, 139MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9796bb373cbd45b480c643ee72f2f0b8"
          }
        },
        "0d14a37fa3ea4e6e898c3b2379889cb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "4d52514e22cc438abf26cf5ab7d49591": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eaf85ea01c6042fc977f1a7bbbab6a50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3022f80801f243f59803d31bf88ddd06": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b60d4fe8139a4318abe35b8a54ad6a03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9796bb373cbd45b480c643ee72f2f0b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "negxzpN_iEth",
        "outputId": "67d856a2-4558-424a-b13d-4f63dc84af67"
      },
      "source": [
        "!git clone https://github.com/Nomow/02456-deep-learning-image-denoising.git\n",
        "!pip install -U albumentations\n",
        "!pip install git+https://github.com/qubvel/segmentation_models.pytorch\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '02456-deep-learning-image-denoising'...\n",
            "remote: Enumerating objects: 62, done.\u001b[K\n",
            "remote: Counting objects: 100% (62/62), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 62 (delta 28), reused 44 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (62/62), done.\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.7/dist-packages (0.1.12)\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-1.1.0-py3-none-any.whl (102 kB)\n",
            "\u001b[K     |████████████████████████████████| 102 kB 8.7 MB/s \n",
            "\u001b[?25hCollecting qudida>=0.0.4\n",
            "  Downloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied: scikit-image>=0.16.1 in /usr/local/lib/python3.7/dist-packages (from albumentations) (0.18.3)\n",
            "Collecting opencv-python-headless>=4.1.1\n",
            "  Downloading opencv_python_headless-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (47.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 47.6 MB 1.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (1.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from qudida>=0.0.4->albumentations) (3.10.0.2)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
            "Requirement already satisfied: imageio>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (1.2.0)\n",
            "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,>=4.3.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.16.1->albumentations) (7.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (3.0.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.15.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.0.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
            "Installing collected packages: opencv-python-headless, qudida, albumentations\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-1.1.0 opencv-python-headless-4.5.4.60 qudida-0.0.4\n",
            "Collecting git+https://github.com/qubvel/segmentation_models.pytorch\n",
            "  Cloning https://github.com/qubvel/segmentation_models.pytorch to /tmp/pip-req-build-22h_lztr\n",
            "  Running command git clone -q https://github.com/qubvel/segmentation_models.pytorch /tmp/pip-req-build-22h_lztr\n",
            "Requirement already satisfied: torchvision>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from segmentation-models-pytorch==0.2.1) (0.11.1+cu111)\n",
            "Collecting pretrainedmodels==0.7.4\n",
            "  Downloading pretrainedmodels-0.7.4.tar.gz (58 kB)\n",
            "\u001b[K     |████████████████████████████████| 58 kB 4.1 MB/s \n",
            "\u001b[?25hCollecting efficientnet-pytorch==0.6.3\n",
            "  Downloading efficientnet_pytorch-0.6.3.tar.gz (16 kB)\n",
            "Collecting timm==0.4.12\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 16.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.1) (1.10.0+cu111)\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.1) (4.62.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->efficientnet-pytorch==0.6.3->segmentation-models-pytorch==0.2.1) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.1) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5.0->segmentation-models-pytorch==0.2.1) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch->pretrainedmodels==0.7.4->segmentation-models-pytorch==0.2.1) (1.15.0)\n",
            "Building wheels for collected packages: segmentation-models-pytorch, efficientnet-pytorch, pretrainedmodels\n",
            "  Building wheel for segmentation-models-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for segmentation-models-pytorch: filename=segmentation_models_pytorch-0.2.1-py3-none-any.whl size=88599 sha256=ae0b1c847057bc2523980dc24c46d37f869d401089de45d98ab3b8ffb5e1b596\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-51p0mqdm/wheels/fa/c5/a8/1e8af6cb04a0974db8a4a156ebd2fdd1d99ad2558d3fce49d4\n",
            "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for efficientnet-pytorch: filename=efficientnet_pytorch-0.6.3-py3-none-any.whl size=12421 sha256=6187f3359586cfec9bc4d41775f841f71d8a43b82b50b8715c7a27b226e1d453\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/6b/0c/f0ad36d00310e65390b0d4c9218ae6250ac579c92540c9097a\n",
            "  Building wheel for pretrainedmodels (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretrainedmodels: filename=pretrainedmodels-0.7.4-py3-none-any.whl size=60965 sha256=5122a87e42b105ff6aacb8130dcbc7546bdcdd0aa29a518434d03bfc96869032\n",
            "  Stored in directory: /root/.cache/pip/wheels/ed/27/e8/9543d42de2740d3544db96aefef63bda3f2c1761b3334f4873\n",
            "Successfully built segmentation-models-pytorch efficientnet-pytorch pretrainedmodels\n",
            "Installing collected packages: munch, timm, pretrainedmodels, efficientnet-pytorch, segmentation-models-pytorch\n",
            "Successfully installed efficientnet-pytorch-0.6.3 munch-2.5.0 pretrainedmodels-0.7.4 segmentation-models-pytorch-0.2.1 timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mnl3m6v2iQyP",
        "outputId": "df2f093f-3b69-4ef1-92bb-0887ed987205"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7FYrrn0ikFf",
        "outputId": "2a790e8d-77b6-4822-db4e-bd230d5e1a13"
      },
      "source": [
        "%cd /content/02456-deep-learning-image-denoising\n",
        "from pathlib import Path\n",
        "from dataset import AutoEncoderDataset\n",
        "from utils import train\n",
        "from utils import val\n",
        "import matplotlib.pyplot as plt\n",
        "import albumentations as albu\n",
        "import segmentation_models_pytorch as smp\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import albumentations.pytorch\n",
        "import numpy as np\n",
        "\n",
        "root_path = Path('/content/gdrive/MyDrive/dermatology_dataset') \n",
        "train_path = root_path / 'train/'\n",
        "val_path = root_path / 'val/'\n",
        "test_path = root_path / 'test/'\n",
        "save_path = root_path / \"experiment_unetplusplus_no_dropout\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/02456-deep-learning-image-denoising\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AY192aPyoxuE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86,
          "referenced_widgets": [
            "3506496467e24fa9a37c82bb6681376b",
            "c1cc030b99a94cb09d470ac6a84626b9",
            "c0c87a1430984ab2956d707ddb488cc4",
            "ab1b608d66bd4beeb563ad149071373a",
            "0dceeab030e1400bbc3b506a37244e4f",
            "0d14a37fa3ea4e6e898c3b2379889cb2",
            "4d52514e22cc438abf26cf5ab7d49591",
            "eaf85ea01c6042fc977f1a7bbbab6a50",
            "3022f80801f243f59803d31bf88ddd06",
            "b60d4fe8139a4318abe35b8a54ad6a03",
            "9796bb373cbd45b480c643ee72f2f0b8"
          ]
        },
        "outputId": "dab5b09c-f37f-43bb-dc6c-fa9efa7c8cab"
      },
      "source": [
        "img_max_size = 512\n",
        "batch_print = 1\n",
        "architecture = smp.UnetPlusPlus\n",
        "encoder = 'timm-regnetx_016'\n",
        "lr = 0.01\n",
        "lr_step_size = 70\n",
        "nb_epochs = 290\n",
        "batch_size = 15\n",
        "val_epoch = 5\n",
        "weight_path = os.path.join(\"/content/weights\", encoder + \"-\" + architecture.__name__)\n",
        "\n",
        "\n",
        "net = architecture(encoder_name=encoder,  # UnetPlusPlus\n",
        "    encoder_weights=\"imagenet\", \n",
        "    in_channels=3, \n",
        "    classes=3,\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-regnet/regnetx_016-65ca972a.pth\" to /root/.cache/torch/hub/checkpoints/regnetx_016-65ca972a.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3506496467e24fa9a37c82bb6681376b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/35.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5STnNSjJoMWP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Elc6DwbbxsM3"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "  net.cuda()\n",
        "  device = \"cuda:0\"\n",
        "else:\n",
        "  device = \"cpu\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R8t1EeaSx1gD"
      },
      "source": [
        "criterion = torch.nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=lr, momentum=0.9)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, lr_step_size, gamma=0.1, last_epoch=-1, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXLGLMLJmcmE"
      },
      "source": [
        "train_transform = [\n",
        "    albu.Blur(blur_limit=10, always_apply=False, p=0.6),\n",
        "    albu.MultiplicativeNoise(multiplier=(0.7, 1.3), p=0.6),\n",
        "    albu.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.4, brightness_by_max=True, always_apply=False, p=0.6),\n",
        "    albu.Downscale(scale_min=0.25, scale_max=0.25, interpolation=0, always_apply=False, p=0.3),\n",
        "    albu.ISONoise(color_shift=(0.08, 0.15), intensity=(0.2, 0.3), always_apply=False, p=0.6),\n",
        "    albu.RandomToneCurve(scale=0.3, p=0.6)\n",
        "]\n",
        "train_transforms = albu.Compose(train_transform)\n",
        "\n",
        "preprocess_transform = [\n",
        "    albu.LongestMaxSize(max_size=img_max_size, always_apply=True),\n",
        "    albu.PadIfNeeded(min_height=img_max_size, min_width=img_max_size, always_apply=True, border_mode=0),    \n",
        "    albu.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, always_apply=False, p=1.0),\n",
        "    albumentations.pytorch.transforms.ToTensorV2()]\n",
        "\n",
        "preprocess_transforms = albu.Compose(preprocess_transform)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmEGD3Glik75"
      },
      "source": [
        "train_dataset = AutoEncoderDataset(train_path, preprocess_transforms, train_transforms)\n",
        "val_dataset = AutoEncoderDataset(val_path, preprocess_transforms, train_transforms)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIR6nXT2llM_",
        "outputId": "fa472478-ae54-4f7c-9ea1-47498880272a"
      },
      "source": [
        "logs = {}\n",
        "logs[\"train\"] = []\n",
        "logs[\"val\"] = []\n",
        "best_loss = np.inf\n",
        "best_idx = 0\n",
        "for i in range(nb_epochs):\n",
        "  print(\"epoch: \", i + 1)\n",
        "  net = train(net, criterion, optimizer, scheduler, train_loader, batch_print, device)\n",
        "  if((i % val_epoch) == 0):\n",
        "    train_loss = val(net, criterion, train_loader, device)\n",
        "    tmp_train = {\"epoch\" : i, \"loss\" : train_loss}\n",
        "    logs[\"train\"].append(tmp_train)\n",
        "    print(\"train loss: \", train_loss)\n",
        "\n",
        "    \n",
        "    val_loss = val(net, criterion, val_loader, device)\n",
        "    tmp_val = {\"epoch\" : i, \"loss\" : val_loss}\n",
        "    logs[\"val\"].append(tmp_val)\n",
        "    print(\"val loss: \", val_loss)\n",
        "    if(val_loss < best_loss):\n",
        "      print(\"best epoch at \", i + 1)\n",
        "      best_loss = val_loss\n",
        "      torch.save(net.state_dict(), os.path.join(save_path, \"best.pth\"))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[    8] loss: 0.159\n",
            "[    9] loss: 0.143\n",
            "[   10] loss: 0.165\n",
            "[   11] loss: 0.185\n",
            "[   12] loss: 0.164\n",
            "[   13] loss: 0.211\n",
            "[   14] loss: 0.124\n",
            "[   15] loss: 0.199\n",
            "[   16] loss: 0.173\n",
            "[   17] loss: 0.137\n",
            "[   18] loss: 0.140\n",
            "[   19] loss: 0.276\n",
            "epoch:  46\n",
            "[    1] loss: 0.140\n",
            "[    2] loss: 0.252\n",
            "[    3] loss: 0.171\n",
            "[    4] loss: 0.153\n",
            "[    5] loss: 0.167\n",
            "[    6] loss: 0.140\n",
            "[    7] loss: 0.160\n",
            "[    8] loss: 0.196\n",
            "[    9] loss: 0.141\n",
            "[   10] loss: 0.190\n",
            "[   11] loss: 0.169\n",
            "[   12] loss: 0.190\n",
            "[   13] loss: 0.221\n",
            "[   14] loss: 0.179\n",
            "[   15] loss: 0.175\n",
            "[   16] loss: 0.133\n",
            "[   17] loss: 0.150\n",
            "[   18] loss: 0.170\n",
            "[   19] loss: 0.175\n",
            "train loss:  0.1518431444761946\n",
            "val loss:  0.22747992351651192\n",
            "epoch:  47\n",
            "[    1] loss: 0.139\n",
            "[    2] loss: 0.146\n",
            "[    3] loss: 0.166\n",
            "[    4] loss: 0.216\n",
            "[    5] loss: 0.138\n",
            "[    6] loss: 0.236\n",
            "[    7] loss: 0.186\n",
            "[    8] loss: 0.231\n",
            "[    9] loss: 0.147\n",
            "[   10] loss: 0.198\n",
            "[   11] loss: 0.212\n",
            "[   12] loss: 0.130\n",
            "[   13] loss: 0.198\n",
            "[   14] loss: 0.207\n",
            "[   15] loss: 0.291\n",
            "[   16] loss: 0.192\n",
            "[   17] loss: 0.158\n",
            "[   18] loss: 0.158\n",
            "[   19] loss: 0.447\n",
            "epoch:  48\n",
            "[    1] loss: 0.166\n",
            "[    2] loss: 0.151\n",
            "[    3] loss: 0.196\n",
            "[    4] loss: 0.232\n",
            "[    5] loss: 0.192\n",
            "[    6] loss: 0.171\n",
            "[    7] loss: 0.184\n",
            "[    8] loss: 0.154\n",
            "[    9] loss: 0.141\n",
            "[   10] loss: 0.232\n",
            "[   11] loss: 0.157\n",
            "[   12] loss: 0.179\n",
            "[   13] loss: 0.191\n",
            "[   14] loss: 0.133\n",
            "[   15] loss: 0.140\n",
            "[   16] loss: 0.259\n",
            "[   17] loss: 0.144\n",
            "[   18] loss: 0.189\n",
            "[   19] loss: 0.207\n",
            "epoch:  49\n",
            "[    1] loss: 0.123\n",
            "[    2] loss: 0.220\n",
            "[    3] loss: 0.195\n",
            "[    4] loss: 0.134\n",
            "[    5] loss: 0.122\n",
            "[    6] loss: 0.125\n",
            "[    7] loss: 0.118\n",
            "[    8] loss: 0.183\n",
            "[    9] loss: 0.144\n",
            "[   10] loss: 0.146\n",
            "[   11] loss: 0.130\n",
            "[   12] loss: 0.166\n",
            "[   13] loss: 0.146\n",
            "[   14] loss: 0.133\n",
            "[   15] loss: 0.216\n",
            "[   16] loss: 0.182\n",
            "[   17] loss: 0.224\n",
            "[   18] loss: 0.175\n",
            "[   19] loss: 0.114\n",
            "epoch:  50\n",
            "[    1] loss: 0.152\n",
            "[    2] loss: 0.197\n",
            "[    3] loss: 0.150\n",
            "[    4] loss: 0.188\n",
            "[    5] loss: 0.158\n",
            "[    6] loss: 0.127\n",
            "[    7] loss: 0.214\n",
            "[    8] loss: 0.198\n",
            "[    9] loss: 0.198\n",
            "[   10] loss: 0.180\n",
            "[   11] loss: 0.148\n",
            "[   12] loss: 0.152\n",
            "[   13] loss: 0.147\n",
            "[   14] loss: 0.149\n",
            "[   15] loss: 0.138\n",
            "[   16] loss: 0.129\n",
            "[   17] loss: 0.192\n",
            "[   18] loss: 0.168\n",
            "[   19] loss: 0.313\n",
            "epoch:  51\n",
            "[    1] loss: 0.159\n",
            "[    2] loss: 0.132\n",
            "[    3] loss: 0.290\n",
            "[    4] loss: 0.235\n",
            "[    5] loss: 0.109\n",
            "[    6] loss: 0.164\n",
            "[    7] loss: 0.139\n",
            "[    8] loss: 0.114\n",
            "[    9] loss: 0.150\n",
            "[   10] loss: 0.181\n",
            "[   11] loss: 0.210\n",
            "[   12] loss: 0.139\n",
            "[   13] loss: 0.162\n",
            "[   14] loss: 0.130\n",
            "[   15] loss: 0.116\n",
            "[   16] loss: 0.210\n",
            "[   17] loss: 0.118\n",
            "[   18] loss: 0.176\n",
            "[   19] loss: 0.283\n",
            "train loss:  0.12876878574709683\n",
            "val loss:  0.18414775282144547\n",
            "epoch:  52\n",
            "[    1] loss: 0.143\n",
            "[    2] loss: 0.223\n",
            "[    3] loss: 0.241\n",
            "[    4] loss: 0.167\n",
            "[    5] loss: 0.135\n",
            "[    6] loss: 0.153\n",
            "[    7] loss: 0.133\n",
            "[    8] loss: 0.199\n",
            "[    9] loss: 0.157\n",
            "[   10] loss: 0.183\n",
            "[   11] loss: 0.121\n",
            "[   12] loss: 0.128\n",
            "[   13] loss: 0.142\n",
            "[   14] loss: 0.140\n",
            "[   15] loss: 0.180\n",
            "[   16] loss: 0.159\n",
            "[   17] loss: 0.159\n",
            "[   18] loss: 0.169\n",
            "[   19] loss: 0.422\n",
            "epoch:  53\n",
            "[    1] loss: 0.180\n",
            "[    2] loss: 0.156\n",
            "[    3] loss: 0.189\n",
            "[    4] loss: 0.121\n",
            "[    5] loss: 0.117\n",
            "[    6] loss: 0.139\n",
            "[    7] loss: 0.139\n",
            "[    8] loss: 0.167\n",
            "[    9] loss: 0.131\n",
            "[   10] loss: 0.195\n",
            "[   11] loss: 0.139\n",
            "[   12] loss: 0.160\n",
            "[   13] loss: 0.193\n",
            "[   14] loss: 0.160\n",
            "[   15] loss: 0.171\n",
            "[   16] loss: 0.121\n",
            "[   17] loss: 0.215\n",
            "[   18] loss: 0.130\n",
            "[   19] loss: 1.151\n",
            "epoch:  54\n",
            "[    1] loss: 0.138\n",
            "[    2] loss: 0.173\n",
            "[    3] loss: 0.146\n",
            "[    4] loss: 0.144\n",
            "[    5] loss: 0.217\n",
            "[    6] loss: 0.162\n",
            "[    7] loss: 0.155\n",
            "[    8] loss: 0.224\n",
            "[    9] loss: 0.194\n",
            "[   10] loss: 0.209\n",
            "[   11] loss: 0.198\n",
            "[   12] loss: 0.173\n",
            "[   13] loss: 0.197\n",
            "[   14] loss: 0.151\n",
            "[   15] loss: 0.129\n",
            "[   16] loss: 0.242\n",
            "[   17] loss: 0.124\n",
            "[   18] loss: 0.211\n",
            "[   19] loss: 0.151\n",
            "epoch:  55\n",
            "[    1] loss: 0.206\n",
            "[    2] loss: 0.255\n",
            "[    3] loss: 0.160\n",
            "[    4] loss: 0.244\n",
            "[    5] loss: 0.197\n",
            "[    6] loss: 0.160\n",
            "[    7] loss: 0.124\n",
            "[    8] loss: 0.139\n",
            "[    9] loss: 0.159\n",
            "[   10] loss: 0.180\n",
            "[   11] loss: 0.161\n",
            "[   12] loss: 0.168\n",
            "[   13] loss: 0.156\n",
            "[   14] loss: 0.235\n",
            "[   15] loss: 0.170\n",
            "[   16] loss: 0.131\n",
            "[   17] loss: 0.240\n",
            "[   18] loss: 0.238\n",
            "[   19] loss: 0.466\n",
            "epoch:  56\n",
            "[    1] loss: 0.215\n",
            "[    2] loss: 0.144\n",
            "[    3] loss: 0.188\n",
            "[    4] loss: 0.150\n",
            "[    5] loss: 0.148\n",
            "[    6] loss: 0.164\n",
            "[    7] loss: 0.126\n",
            "[    8] loss: 0.209\n",
            "[    9] loss: 0.184\n",
            "[   10] loss: 0.173\n",
            "[   11] loss: 0.226\n",
            "[   12] loss: 0.152\n",
            "[   13] loss: 0.126\n",
            "[   14] loss: 0.134\n",
            "[   15] loss: 0.228\n",
            "[   16] loss: 0.147\n",
            "[   17] loss: 0.209\n",
            "[   18] loss: 0.143\n",
            "[   19] loss: 0.443\n",
            "train loss:  0.1526129900423043\n",
            "val loss:  0.20344026200473309\n",
            "epoch:  57\n",
            "[    1] loss: 0.100\n",
            "[    2] loss: 0.201\n",
            "[    3] loss: 0.193\n",
            "[    4] loss: 0.279\n",
            "[    5] loss: 0.114\n",
            "[    6] loss: 0.193\n",
            "[    7] loss: 0.140\n",
            "[    8] loss: 0.250\n",
            "[    9] loss: 0.144\n",
            "[   10] loss: 0.177\n",
            "[   11] loss: 0.151\n",
            "[   12] loss: 0.255\n",
            "[   13] loss: 0.139\n",
            "[   14] loss: 0.127\n",
            "[   15] loss: 0.144\n",
            "[   16] loss: 0.159\n",
            "[   17] loss: 0.197\n",
            "[   18] loss: 0.156\n",
            "[   19] loss: 0.272\n",
            "epoch:  58\n",
            "[    1] loss: 0.167\n",
            "[    2] loss: 0.225\n",
            "[    3] loss: 0.268\n",
            "[    4] loss: 0.246\n",
            "[    5] loss: 0.128\n",
            "[    6] loss: 0.125\n",
            "[    7] loss: 0.163\n",
            "[    8] loss: 0.184\n",
            "[    9] loss: 0.220\n",
            "[   10] loss: 0.195\n",
            "[   11] loss: 0.148\n",
            "[   12] loss: 0.129\n",
            "[   13] loss: 0.186\n",
            "[   14] loss: 0.135\n",
            "[   15] loss: 0.171\n",
            "[   16] loss: 0.180\n",
            "[   17] loss: 0.178\n",
            "[   18] loss: 0.155\n",
            "[   19] loss: 0.278\n",
            "epoch:  59\n",
            "[    1] loss: 0.227\n",
            "[    2] loss: 0.142\n",
            "[    3] loss: 0.167\n",
            "[    4] loss: 0.095\n",
            "[    5] loss: 0.181\n",
            "[    6] loss: 0.163\n",
            "[    7] loss: 0.127\n",
            "[    8] loss: 0.135\n",
            "[    9] loss: 0.171\n",
            "[   10] loss: 0.135\n",
            "[   11] loss: 0.185\n",
            "[   12] loss: 0.363\n",
            "[   13] loss: 0.136\n",
            "[   14] loss: 0.151\n",
            "[   15] loss: 0.123\n",
            "[   16] loss: 0.148\n",
            "[   17] loss: 0.146\n",
            "[   18] loss: 0.151\n",
            "[   19] loss: 0.495\n",
            "epoch:  60\n",
            "[    1] loss: 0.097\n",
            "[    2] loss: 0.131\n",
            "[    3] loss: 0.138\n",
            "[    4] loss: 0.150\n",
            "[    5] loss: 0.150\n",
            "[    6] loss: 0.159\n",
            "[    7] loss: 0.220\n",
            "[    8] loss: 0.189\n",
            "[    9] loss: 0.130\n",
            "[   10] loss: 0.121\n",
            "[   11] loss: 0.135\n",
            "[   12] loss: 0.164\n",
            "[   13] loss: 0.137\n",
            "[   14] loss: 0.134\n",
            "[   15] loss: 0.192\n",
            "[   16] loss: 0.227\n",
            "[   17] loss: 0.190\n",
            "[   18] loss: 0.130\n",
            "[   19] loss: 0.269\n",
            "epoch:  61\n",
            "[    1] loss: 0.104\n",
            "[    2] loss: 0.117\n",
            "[    3] loss: 0.142\n",
            "[    4] loss: 0.112\n",
            "[    5] loss: 0.174\n",
            "[    6] loss: 0.136\n",
            "[    7] loss: 0.157\n",
            "[    8] loss: 0.137\n",
            "[    9] loss: 0.170\n",
            "[   10] loss: 0.151\n",
            "[   11] loss: 0.152\n",
            "[   12] loss: 0.210\n",
            "[   13] loss: 0.121\n",
            "[   14] loss: 0.144\n",
            "[   15] loss: 0.134\n",
            "[   16] loss: 0.161\n",
            "[   17] loss: 0.131\n",
            "[   18] loss: 0.159\n",
            "[   19] loss: 0.230\n",
            "train loss:  0.14492240841226542\n",
            "val loss:  0.23600798472762108\n",
            "epoch:  62\n",
            "[    1] loss: 0.139\n",
            "[    2] loss: 0.146\n",
            "[    3] loss: 0.160\n",
            "[    4] loss: 0.159\n",
            "[    5] loss: 0.163\n",
            "[    6] loss: 0.124\n",
            "[    7] loss: 0.262\n",
            "[    8] loss: 0.156\n",
            "[    9] loss: 0.179\n",
            "[   10] loss: 0.137\n",
            "[   11] loss: 0.157\n",
            "[   12] loss: 0.227\n",
            "[   13] loss: 0.160\n",
            "[   14] loss: 0.197\n",
            "[   15] loss: 0.123\n",
            "[   16] loss: 0.149\n",
            "[   17] loss: 0.171\n",
            "[   18] loss: 0.123\n",
            "[   19] loss: 0.273\n",
            "epoch:  63\n",
            "[    1] loss: 0.124\n",
            "[    2] loss: 0.133\n",
            "[    3] loss: 0.102\n",
            "[    4] loss: 0.130\n",
            "[    5] loss: 0.165\n",
            "[    6] loss: 0.154\n",
            "[    7] loss: 0.114\n",
            "[    8] loss: 0.134\n",
            "[    9] loss: 0.142\n",
            "[   10] loss: 0.124\n",
            "[   11] loss: 0.128\n",
            "[   12] loss: 0.098\n",
            "[   13] loss: 0.107\n",
            "[   14] loss: 0.211\n",
            "[   15] loss: 0.122\n",
            "[   16] loss: 0.154\n",
            "[   17] loss: 0.161\n",
            "[   18] loss: 0.158\n",
            "[   19] loss: 0.326\n",
            "epoch:  64\n",
            "[    1] loss: 0.173\n",
            "[    2] loss: 0.140\n",
            "[    3] loss: 0.199\n",
            "[    4] loss: 0.189\n",
            "[    5] loss: 0.160\n",
            "[    6] loss: 0.147\n",
            "[    7] loss: 0.125\n",
            "[    8] loss: 0.092\n",
            "[    9] loss: 0.192\n",
            "[   10] loss: 0.112\n",
            "[   11] loss: 0.128\n",
            "[   12] loss: 0.182\n",
            "[   13] loss: 0.159\n",
            "[   14] loss: 0.180\n",
            "[   15] loss: 0.155\n",
            "[   16] loss: 0.145\n",
            "[   17] loss: 0.145\n",
            "[   18] loss: 0.198\n",
            "[   19] loss: 0.238\n",
            "epoch:  65\n",
            "[    1] loss: 0.129\n",
            "[    2] loss: 0.140\n",
            "[    3] loss: 0.236\n",
            "[    4] loss: 0.155\n",
            "[    5] loss: 0.155\n",
            "[    6] loss: 0.130\n",
            "[    7] loss: 0.156\n",
            "[    8] loss: 0.163\n",
            "[    9] loss: 0.120\n",
            "[   10] loss: 0.125\n",
            "[   11] loss: 0.166\n",
            "[   12] loss: 0.180\n",
            "[   13] loss: 0.108\n",
            "[   14] loss: 0.166\n",
            "[   15] loss: 0.097\n",
            "[   16] loss: 0.116\n",
            "[   17] loss: 0.157\n",
            "[   18] loss: 0.144\n",
            "[   19] loss: 0.202\n",
            "epoch:  66\n",
            "[    1] loss: 0.127\n",
            "[    2] loss: 0.107\n",
            "[    3] loss: 0.132\n",
            "[    4] loss: 0.188\n",
            "[    5] loss: 0.122\n",
            "[    6] loss: 0.146\n",
            "[    7] loss: 0.104\n",
            "[    8] loss: 0.104\n",
            "[    9] loss: 0.127\n",
            "[   10] loss: 0.124\n",
            "[   11] loss: 0.141\n",
            "[   12] loss: 0.147\n",
            "[   13] loss: 0.124\n",
            "[   14] loss: 0.181\n",
            "[   15] loss: 0.128\n",
            "[   16] loss: 0.103\n",
            "[   17] loss: 0.095\n",
            "[   18] loss: 0.140\n",
            "[   19] loss: 0.205\n",
            "train loss:  0.11370173102135167\n",
            "val loss:  0.17588257789611816\n",
            "epoch:  67\n",
            "[    1] loss: 0.123\n",
            "[    2] loss: 0.136\n",
            "[    3] loss: 0.117\n",
            "[    4] loss: 0.100\n",
            "[    5] loss: 0.129\n",
            "[    6] loss: 0.113\n",
            "[    7] loss: 0.115\n",
            "[    8] loss: 0.134\n",
            "[    9] loss: 0.122\n",
            "[   10] loss: 0.111\n",
            "[   11] loss: 0.109\n",
            "[   12] loss: 0.137\n",
            "[   13] loss: 0.124\n",
            "[   14] loss: 0.120\n",
            "[   15] loss: 0.172\n",
            "[   16] loss: 0.106\n",
            "[   17] loss: 0.132\n",
            "[   18] loss: 0.137\n",
            "[   19] loss: 0.307\n",
            "epoch:  68\n",
            "[    1] loss: 0.166\n",
            "[    2] loss: 0.165\n",
            "[    3] loss: 0.154\n",
            "[    4] loss: 0.204\n",
            "[    5] loss: 0.132\n",
            "[    6] loss: 0.111\n",
            "[    7] loss: 0.124\n",
            "[    8] loss: 0.140\n",
            "[    9] loss: 0.147\n",
            "[   10] loss: 0.106\n",
            "[   11] loss: 0.120\n",
            "[   12] loss: 0.211\n",
            "[   13] loss: 0.245\n",
            "[   14] loss: 0.115\n",
            "[   15] loss: 0.138\n",
            "[   16] loss: 0.156\n",
            "[   17] loss: 0.143\n",
            "[   18] loss: 0.136\n",
            "[   19] loss: 0.144\n",
            "epoch:  69\n",
            "[    1] loss: 0.172\n",
            "[    2] loss: 0.117\n",
            "[    3] loss: 0.131\n",
            "[    4] loss: 0.143\n",
            "[    5] loss: 0.143\n",
            "[    6] loss: 0.155\n",
            "[    7] loss: 0.196\n",
            "[    8] loss: 0.108\n",
            "[    9] loss: 0.134\n",
            "[   10] loss: 0.105\n",
            "[   11] loss: 0.131\n",
            "[   12] loss: 0.147\n",
            "[   13] loss: 0.087\n",
            "[   14] loss: 0.123\n",
            "[   15] loss: 0.144\n",
            "[   16] loss: 0.094\n",
            "[   17] loss: 0.131\n",
            "[   18] loss: 0.219\n",
            "[   19] loss: 0.162\n",
            "epoch:  70\n",
            "[    1] loss: 0.113\n",
            "[    2] loss: 0.127\n",
            "[    3] loss: 0.082\n",
            "[    4] loss: 0.102\n",
            "[    5] loss: 0.137\n",
            "[    6] loss: 0.115\n",
            "[    7] loss: 0.100\n",
            "[    8] loss: 0.120\n",
            "[    9] loss: 0.140\n",
            "[   10] loss: 0.113\n",
            "[   11] loss: 0.148\n",
            "[   12] loss: 0.096\n",
            "[   13] loss: 0.174\n",
            "[   14] loss: 0.144\n",
            "[   15] loss: 0.181\n",
            "[   16] loss: 0.115\n",
            "[   17] loss: 0.122\n",
            "[   18] loss: 0.104\n",
            "[   19] loss: 0.281\n",
            "epoch:  71\n",
            "[    1] loss: 0.107\n",
            "[    2] loss: 0.110\n",
            "[    3] loss: 0.109\n",
            "[    4] loss: 0.120\n",
            "[    5] loss: 0.154\n",
            "[    6] loss: 0.136\n",
            "[    7] loss: 0.088\n",
            "[    8] loss: 0.137\n",
            "[    9] loss: 0.180\n",
            "[   10] loss: 0.099\n",
            "[   11] loss: 0.163\n",
            "[   12] loss: 0.115\n",
            "[   13] loss: 0.139\n",
            "[   14] loss: 0.121\n",
            "[   15] loss: 0.111\n",
            "[   16] loss: 0.132\n",
            "[   17] loss: 0.116\n",
            "[   18] loss: 0.113\n",
            "[   19] loss: 0.528\n",
            "train loss:  0.11096983663189937\n",
            "val loss:  0.16521294973790646\n",
            "epoch:  72\n",
            "[    1] loss: 0.107\n",
            "[    2] loss: 0.138\n",
            "[    3] loss: 0.100\n",
            "[    4] loss: 0.106\n",
            "[    5] loss: 0.116\n",
            "[    6] loss: 0.140\n",
            "[    7] loss: 0.131\n",
            "[    8] loss: 0.090\n",
            "[    9] loss: 0.166\n",
            "[   10] loss: 0.117\n",
            "[   11] loss: 0.112\n",
            "[   12] loss: 0.197\n",
            "[   13] loss: 0.161\n",
            "[   14] loss: 0.142\n",
            "[   15] loss: 0.148\n",
            "[   16] loss: 0.112\n",
            "[   17] loss: 0.110\n",
            "[   18] loss: 0.165\n",
            "[   19] loss: 0.153\n",
            "epoch:  73\n",
            "[    1] loss: 0.163\n",
            "[    2] loss: 0.081\n",
            "[    3] loss: 0.157\n",
            "[    4] loss: 0.134\n",
            "[    5] loss: 0.308\n",
            "[    6] loss: 0.103\n",
            "[    7] loss: 0.095\n",
            "[    8] loss: 0.165\n",
            "[    9] loss: 0.166\n",
            "[   10] loss: 0.138\n",
            "[   11] loss: 0.081\n",
            "[   12] loss: 0.115\n",
            "[   13] loss: 0.134\n",
            "[   14] loss: 0.105\n",
            "[   15] loss: 0.088\n",
            "[   16] loss: 0.100\n",
            "[   17] loss: 0.113\n",
            "[   18] loss: 0.103\n",
            "[   19] loss: 0.816\n",
            "epoch:  74\n",
            "[    1] loss: 0.108\n",
            "[    2] loss: 0.084\n",
            "[    3] loss: 0.110\n",
            "[    4] loss: 0.113\n",
            "[    5] loss: 0.127\n",
            "[    6] loss: 0.090\n",
            "[    7] loss: 0.140\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.114\n",
            "[   10] loss: 0.180\n",
            "[   11] loss: 0.099\n",
            "[   12] loss: 0.110\n",
            "[   13] loss: 0.087\n",
            "[   14] loss: 0.090\n",
            "[   15] loss: 0.114\n",
            "[   16] loss: 0.102\n",
            "[   17] loss: 0.115\n",
            "[   18] loss: 0.188\n",
            "[   19] loss: 0.494\n",
            "epoch:  75\n",
            "[    1] loss: 0.120\n",
            "[    2] loss: 0.101\n",
            "[    3] loss: 0.118\n",
            "[    4] loss: 0.158\n",
            "[    5] loss: 0.120\n",
            "[    6] loss: 0.118\n",
            "[    7] loss: 0.132\n",
            "[    8] loss: 0.138\n",
            "[    9] loss: 0.096\n",
            "[   10] loss: 0.115\n",
            "[   11] loss: 0.105\n",
            "[   12] loss: 0.130\n",
            "[   13] loss: 0.091\n",
            "[   14] loss: 0.090\n",
            "[   15] loss: 0.137\n",
            "[   16] loss: 0.124\n",
            "[   17] loss: 0.112\n",
            "[   18] loss: 0.114\n",
            "[   19] loss: 0.257\n",
            "epoch:  76\n",
            "[    1] loss: 0.085\n",
            "[    2] loss: 0.124\n",
            "[    3] loss: 0.162\n",
            "[    4] loss: 0.118\n",
            "[    5] loss: 0.119\n",
            "[    6] loss: 0.171\n",
            "[    7] loss: 0.081\n",
            "[    8] loss: 0.091\n",
            "[    9] loss: 0.103\n",
            "[   10] loss: 0.115\n",
            "[   11] loss: 0.126\n",
            "[   12] loss: 0.108\n",
            "[   13] loss: 0.131\n",
            "[   14] loss: 0.108\n",
            "[   15] loss: 0.122\n",
            "[   16] loss: 0.123\n",
            "[   17] loss: 0.176\n",
            "[   18] loss: 0.092\n",
            "[   19] loss: 0.217\n",
            "train loss:  0.08969165855909095\n",
            "val loss:  0.16124830767512321\n",
            "epoch:  77\n",
            "[    1] loss: 0.132\n",
            "[    2] loss: 0.130\n",
            "[    3] loss: 0.114\n",
            "[    4] loss: 0.091\n",
            "[    5] loss: 0.092\n",
            "[    6] loss: 0.136\n",
            "[    7] loss: 0.129\n",
            "[    8] loss: 0.138\n",
            "[    9] loss: 0.166\n",
            "[   10] loss: 0.126\n",
            "[   11] loss: 0.124\n",
            "[   12] loss: 0.127\n",
            "[   13] loss: 0.115\n",
            "[   14] loss: 0.096\n",
            "[   15] loss: 0.132\n",
            "[   16] loss: 0.106\n",
            "[   17] loss: 0.089\n",
            "[   18] loss: 0.103\n",
            "[   19] loss: 0.218\n",
            "epoch:  78\n",
            "[    1] loss: 0.124\n",
            "[    2] loss: 0.107\n",
            "[    3] loss: 0.094\n",
            "[    4] loss: 0.111\n",
            "[    5] loss: 0.125\n",
            "[    6] loss: 0.111\n",
            "[    7] loss: 0.104\n",
            "[    8] loss: 0.208\n",
            "[    9] loss: 0.136\n",
            "[   10] loss: 0.124\n",
            "[   11] loss: 0.113\n",
            "[   12] loss: 0.160\n",
            "[   13] loss: 0.121\n",
            "[   14] loss: 0.171\n",
            "[   15] loss: 0.126\n",
            "[   16] loss: 0.081\n",
            "[   17] loss: 0.148\n",
            "[   18] loss: 0.162\n",
            "[   19] loss: 0.261\n",
            "epoch:  79\n",
            "[    1] loss: 0.139\n",
            "[    2] loss: 0.131\n",
            "[    3] loss: 0.133\n",
            "[    4] loss: 0.146\n",
            "[    5] loss: 0.106\n",
            "[    6] loss: 0.107\n",
            "[    7] loss: 0.112\n",
            "[    8] loss: 0.089\n",
            "[    9] loss: 0.150\n",
            "[   10] loss: 0.088\n",
            "[   11] loss: 0.110\n",
            "[   12] loss: 0.085\n",
            "[   13] loss: 0.109\n",
            "[   14] loss: 0.131\n",
            "[   15] loss: 0.113\n",
            "[   16] loss: 0.093\n",
            "[   17] loss: 0.129\n",
            "[   18] loss: 0.129\n",
            "[   19] loss: 0.187\n",
            "epoch:  80\n",
            "[    1] loss: 0.094\n",
            "[    2] loss: 0.093\n",
            "[    3] loss: 0.092\n",
            "[    4] loss: 0.092\n",
            "[    5] loss: 0.137\n",
            "[    6] loss: 0.101\n",
            "[    7] loss: 0.123\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.164\n",
            "[   10] loss: 0.163\n",
            "[   11] loss: 0.093\n",
            "[   12] loss: 0.073\n",
            "[   13] loss: 0.125\n",
            "[   14] loss: 0.199\n",
            "[   15] loss: 0.098\n",
            "[   16] loss: 0.106\n",
            "[   17] loss: 0.119\n",
            "[   18] loss: 0.095\n",
            "[   19] loss: 0.206\n",
            "epoch:  81\n",
            "[    1] loss: 0.126\n",
            "[    2] loss: 0.146\n",
            "[    3] loss: 0.133\n",
            "[    4] loss: 0.093\n",
            "[    5] loss: 0.099\n",
            "[    6] loss: 0.127\n",
            "[    7] loss: 0.153\n",
            "[    8] loss: 0.144\n",
            "[    9] loss: 0.117\n",
            "[   10] loss: 0.131\n",
            "[   11] loss: 0.147\n",
            "[   12] loss: 0.107\n",
            "[   13] loss: 0.093\n",
            "[   14] loss: 0.134\n",
            "[   15] loss: 0.100\n",
            "[   16] loss: 0.103\n",
            "[   17] loss: 0.121\n",
            "[   18] loss: 0.123\n",
            "[   19] loss: 0.297\n",
            "train loss:  0.10790642767268069\n",
            "val loss:  0.16910050995647907\n",
            "epoch:  82\n",
            "[    1] loss: 0.093\n",
            "[    2] loss: 0.155\n",
            "[    3] loss: 0.122\n",
            "[    4] loss: 0.101\n",
            "[    5] loss: 0.127\n",
            "[    6] loss: 0.139\n",
            "[    7] loss: 0.107\n",
            "[    8] loss: 0.144\n",
            "[    9] loss: 0.100\n",
            "[   10] loss: 0.107\n",
            "[   11] loss: 0.100\n",
            "[   12] loss: 0.137\n",
            "[   13] loss: 0.142\n",
            "[   14] loss: 0.117\n",
            "[   15] loss: 0.111\n",
            "[   16] loss: 0.108\n",
            "[   17] loss: 0.124\n",
            "[   18] loss: 0.114\n",
            "[   19] loss: 0.125\n",
            "epoch:  83\n",
            "[    1] loss: 0.159\n",
            "[    2] loss: 0.140\n",
            "[    3] loss: 0.125\n",
            "[    4] loss: 0.090\n",
            "[    5] loss: 0.136\n",
            "[    6] loss: 0.102\n",
            "[    7] loss: 0.109\n",
            "[    8] loss: 0.109\n",
            "[    9] loss: 0.124\n",
            "[   10] loss: 0.134\n",
            "[   11] loss: 0.097\n",
            "[   12] loss: 0.127\n",
            "[   13] loss: 0.104\n",
            "[   14] loss: 0.136\n",
            "[   15] loss: 0.081\n",
            "[   16] loss: 0.106\n",
            "[   17] loss: 0.096\n",
            "[   18] loss: 0.158\n",
            "[   19] loss: 0.199\n",
            "epoch:  84\n",
            "[    1] loss: 0.141\n",
            "[    2] loss: 0.109\n",
            "[    3] loss: 0.161\n",
            "[    4] loss: 0.161\n",
            "[    5] loss: 0.117\n",
            "[    6] loss: 0.128\n",
            "[    7] loss: 0.102\n",
            "[    8] loss: 0.092\n",
            "[    9] loss: 0.078\n",
            "[   10] loss: 0.141\n",
            "[   11] loss: 0.119\n",
            "[   12] loss: 0.129\n",
            "[   13] loss: 0.118\n",
            "[   14] loss: 0.109\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.117\n",
            "[   17] loss: 0.133\n",
            "[   18] loss: 0.158\n",
            "[   19] loss: 0.348\n",
            "epoch:  85\n",
            "[    1] loss: 0.105\n",
            "[    2] loss: 0.130\n",
            "[    3] loss: 0.132\n",
            "[    4] loss: 0.106\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.110\n",
            "[    7] loss: 0.160\n",
            "[    8] loss: 0.141\n",
            "[    9] loss: 0.114\n",
            "[   10] loss: 0.116\n",
            "[   11] loss: 0.107\n",
            "[   12] loss: 0.104\n",
            "[   13] loss: 0.116\n",
            "[   14] loss: 0.095\n",
            "[   15] loss: 0.083\n",
            "[   16] loss: 0.131\n",
            "[   17] loss: 0.146\n",
            "[   18] loss: 0.151\n",
            "[   19] loss: 0.130\n",
            "epoch:  86\n",
            "[    1] loss: 0.097\n",
            "[    2] loss: 0.104\n",
            "[    3] loss: 0.097\n",
            "[    4] loss: 0.149\n",
            "[    5] loss: 0.089\n",
            "[    6] loss: 0.158\n",
            "[    7] loss: 0.135\n",
            "[    8] loss: 0.094\n",
            "[    9] loss: 0.176\n",
            "[   10] loss: 0.109\n",
            "[   11] loss: 0.079\n",
            "[   12] loss: 0.100\n",
            "[   13] loss: 0.115\n",
            "[   14] loss: 0.088\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.124\n",
            "[   17] loss: 0.136\n",
            "[   18] loss: 0.196\n",
            "[   19] loss: 0.209\n",
            "train loss:  0.09927063046351951\n",
            "val loss:  0.18762263469398022\n",
            "epoch:  87\n",
            "[    1] loss: 0.121\n",
            "[    2] loss: 0.106\n",
            "[    3] loss: 0.142\n",
            "[    4] loss: 0.099\n",
            "[    5] loss: 0.160\n",
            "[    6] loss: 0.121\n",
            "[    7] loss: 0.154\n",
            "[    8] loss: 0.093\n",
            "[    9] loss: 0.108\n",
            "[   10] loss: 0.138\n",
            "[   11] loss: 0.093\n",
            "[   12] loss: 0.111\n",
            "[   13] loss: 0.103\n",
            "[   14] loss: 0.082\n",
            "[   15] loss: 0.113\n",
            "[   16] loss: 0.109\n",
            "[   17] loss: 0.106\n",
            "[   18] loss: 0.111\n",
            "[   19] loss: 0.232\n",
            "epoch:  88\n",
            "[    1] loss: 0.090\n",
            "[    2] loss: 0.155\n",
            "[    3] loss: 0.123\n",
            "[    4] loss: 0.118\n",
            "[    5] loss: 0.089\n",
            "[    6] loss: 0.138\n",
            "[    7] loss: 0.108\n",
            "[    8] loss: 0.144\n",
            "[    9] loss: 0.101\n",
            "[   10] loss: 0.170\n",
            "[   11] loss: 0.107\n",
            "[   12] loss: 0.103\n",
            "[   13] loss: 0.090\n",
            "[   14] loss: 0.192\n",
            "[   15] loss: 0.115\n",
            "[   16] loss: 0.115\n",
            "[   17] loss: 0.118\n",
            "[   18] loss: 0.118\n",
            "[   19] loss: 0.194\n",
            "epoch:  89\n",
            "[    1] loss: 0.156\n",
            "[    2] loss: 0.098\n",
            "[    3] loss: 0.089\n",
            "[    4] loss: 0.094\n",
            "[    5] loss: 0.092\n",
            "[    6] loss: 0.122\n",
            "[    7] loss: 0.153\n",
            "[    8] loss: 0.103\n",
            "[    9] loss: 0.109\n",
            "[   10] loss: 0.108\n",
            "[   11] loss: 0.126\n",
            "[   12] loss: 0.162\n",
            "[   13] loss: 0.114\n",
            "[   14] loss: 0.098\n",
            "[   15] loss: 0.114\n",
            "[   16] loss: 0.119\n",
            "[   17] loss: 0.125\n",
            "[   18] loss: 0.145\n",
            "[   19] loss: 0.242\n",
            "epoch:  90\n",
            "[    1] loss: 0.153\n",
            "[    2] loss: 0.097\n",
            "[    3] loss: 0.132\n",
            "[    4] loss: 0.119\n",
            "[    5] loss: 0.105\n",
            "[    6] loss: 0.130\n",
            "[    7] loss: 0.080\n",
            "[    8] loss: 0.117\n",
            "[    9] loss: 0.093\n",
            "[   10] loss: 0.112\n",
            "[   11] loss: 0.110\n",
            "[   12] loss: 0.122\n",
            "[   13] loss: 0.141\n",
            "[   14] loss: 0.129\n",
            "[   15] loss: 0.099\n",
            "[   16] loss: 0.141\n",
            "[   17] loss: 0.147\n",
            "[   18] loss: 0.134\n",
            "[   19] loss: 0.206\n",
            "epoch:  91\n",
            "[    1] loss: 0.135\n",
            "[    2] loss: 0.089\n",
            "[    3] loss: 0.088\n",
            "[    4] loss: 0.086\n",
            "[    5] loss: 0.142\n",
            "[    6] loss: 0.119\n",
            "[    7] loss: 0.139\n",
            "[    8] loss: 0.137\n",
            "[    9] loss: 0.153\n",
            "[   10] loss: 0.154\n",
            "[   11] loss: 0.129\n",
            "[   12] loss: 0.096\n",
            "[   13] loss: 0.105\n",
            "[   14] loss: 0.113\n",
            "[   15] loss: 0.127\n",
            "[   16] loss: 0.123\n",
            "[   17] loss: 0.108\n",
            "[   18] loss: 0.092\n",
            "[   19] loss: 0.214\n",
            "train loss:  0.09963968833086684\n",
            "val loss:  0.17020707204937935\n",
            "epoch:  92\n",
            "[    1] loss: 0.113\n",
            "[    2] loss: 0.106\n",
            "[    3] loss: 0.120\n",
            "[    4] loss: 0.141\n",
            "[    5] loss: 0.110\n",
            "[    6] loss: 0.135\n",
            "[    7] loss: 0.145\n",
            "[    8] loss: 0.065\n",
            "[    9] loss: 0.100\n",
            "[   10] loss: 0.117\n",
            "[   11] loss: 0.176\n",
            "[   12] loss: 0.087\n",
            "[   13] loss: 0.127\n",
            "[   14] loss: 0.084\n",
            "[   15] loss: 0.163\n",
            "[   16] loss: 0.085\n",
            "[   17] loss: 0.165\n",
            "[   18] loss: 0.101\n",
            "[   19] loss: 0.513\n",
            "epoch:  93\n",
            "[    1] loss: 0.113\n",
            "[    2] loss: 0.112\n",
            "[    3] loss: 0.122\n",
            "[    4] loss: 0.110\n",
            "[    5] loss: 0.127\n",
            "[    6] loss: 0.112\n",
            "[    7] loss: 0.144\n",
            "[    8] loss: 0.122\n",
            "[    9] loss: 0.090\n",
            "[   10] loss: 0.101\n",
            "[   11] loss: 0.105\n",
            "[   12] loss: 0.108\n",
            "[   13] loss: 0.117\n",
            "[   14] loss: 0.101\n",
            "[   15] loss: 0.075\n",
            "[   16] loss: 0.094\n",
            "[   17] loss: 0.145\n",
            "[   18] loss: 0.161\n",
            "[   19] loss: 0.490\n",
            "epoch:  94\n",
            "[    1] loss: 0.078\n",
            "[    2] loss: 0.154\n",
            "[    3] loss: 0.121\n",
            "[    4] loss: 0.149\n",
            "[    5] loss: 0.103\n",
            "[    6] loss: 0.102\n",
            "[    7] loss: 0.091\n",
            "[    8] loss: 0.137\n",
            "[    9] loss: 0.124\n",
            "[   10] loss: 0.118\n",
            "[   11] loss: 0.130\n",
            "[   12] loss: 0.119\n",
            "[   13] loss: 0.118\n",
            "[   14] loss: 0.117\n",
            "[   15] loss: 0.111\n",
            "[   16] loss: 0.072\n",
            "[   17] loss: 0.106\n",
            "[   18] loss: 0.138\n",
            "[   19] loss: 0.282\n",
            "epoch:  95\n",
            "[    1] loss: 0.113\n",
            "[    2] loss: 0.116\n",
            "[    3] loss: 0.152\n",
            "[    4] loss: 0.110\n",
            "[    5] loss: 0.141\n",
            "[    6] loss: 0.148\n",
            "[    7] loss: 0.068\n",
            "[    8] loss: 0.079\n",
            "[    9] loss: 0.085\n",
            "[   10] loss: 0.114\n",
            "[   11] loss: 0.140\n",
            "[   12] loss: 0.096\n",
            "[   13] loss: 0.162\n",
            "[   14] loss: 0.122\n",
            "[   15] loss: 0.095\n",
            "[   16] loss: 0.132\n",
            "[   17] loss: 0.101\n",
            "[   18] loss: 0.196\n",
            "[   19] loss: 0.167\n",
            "epoch:  96\n",
            "[    1] loss: 0.104\n",
            "[    2] loss: 0.110\n",
            "[    3] loss: 0.105\n",
            "[    4] loss: 0.091\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.100\n",
            "[    7] loss: 0.110\n",
            "[    8] loss: 0.126\n",
            "[    9] loss: 0.167\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.077\n",
            "[   12] loss: 0.189\n",
            "[   13] loss: 0.107\n",
            "[   14] loss: 0.127\n",
            "[   15] loss: 0.129\n",
            "[   16] loss: 0.108\n",
            "[   17] loss: 0.108\n",
            "[   18] loss: 0.206\n",
            "[   19] loss: 0.165\n",
            "train loss:  0.09204893630436238\n",
            "val loss:  0.17322882637381554\n",
            "epoch:  97\n",
            "[    1] loss: 0.114\n",
            "[    2] loss: 0.118\n",
            "[    3] loss: 0.115\n",
            "[    4] loss: 0.141\n",
            "[    5] loss: 0.087\n",
            "[    6] loss: 0.106\n",
            "[    7] loss: 0.132\n",
            "[    8] loss: 0.132\n",
            "[    9] loss: 0.085\n",
            "[   10] loss: 0.128\n",
            "[   11] loss: 0.083\n",
            "[   12] loss: 0.137\n",
            "[   13] loss: 0.185\n",
            "[   14] loss: 0.150\n",
            "[   15] loss: 0.107\n",
            "[   16] loss: 0.116\n",
            "[   17] loss: 0.095\n",
            "[   18] loss: 0.108\n",
            "[   19] loss: 0.243\n",
            "epoch:  98\n",
            "[    1] loss: 0.119\n",
            "[    2] loss: 0.118\n",
            "[    3] loss: 0.125\n",
            "[    4] loss: 0.107\n",
            "[    5] loss: 0.133\n",
            "[    6] loss: 0.111\n",
            "[    7] loss: 0.134\n",
            "[    8] loss: 0.116\n",
            "[    9] loss: 0.110\n",
            "[   10] loss: 0.113\n",
            "[   11] loss: 0.156\n",
            "[   12] loss: 0.088\n",
            "[   13] loss: 0.201\n",
            "[   14] loss: 0.091\n",
            "[   15] loss: 0.199\n",
            "[   16] loss: 0.154\n",
            "[   17] loss: 0.126\n",
            "[   18] loss: 0.141\n",
            "[   19] loss: 0.701\n",
            "epoch:  99\n",
            "[    1] loss: 0.153\n",
            "[    2] loss: 0.090\n",
            "[    3] loss: 0.101\n",
            "[    4] loss: 0.140\n",
            "[    5] loss: 0.129\n",
            "[    6] loss: 0.107\n",
            "[    7] loss: 0.129\n",
            "[    8] loss: 0.099\n",
            "[    9] loss: 0.093\n",
            "[   10] loss: 0.098\n",
            "[   11] loss: 0.157\n",
            "[   12] loss: 0.099\n",
            "[   13] loss: 0.109\n",
            "[   14] loss: 0.089\n",
            "[   15] loss: 0.113\n",
            "[   16] loss: 0.087\n",
            "[   17] loss: 0.148\n",
            "[   18] loss: 0.120\n",
            "[   19] loss: 0.207\n",
            "epoch:  100\n",
            "[    1] loss: 0.112\n",
            "[    2] loss: 0.094\n",
            "[    3] loss: 0.103\n",
            "[    4] loss: 0.089\n",
            "[    5] loss: 0.139\n",
            "[    6] loss: 0.126\n",
            "[    7] loss: 0.114\n",
            "[    8] loss: 0.120\n",
            "[    9] loss: 0.082\n",
            "[   10] loss: 0.125\n",
            "[   11] loss: 0.151\n",
            "[   12] loss: 0.145\n",
            "[   13] loss: 0.123\n",
            "[   14] loss: 0.102\n",
            "[   15] loss: 0.118\n",
            "[   16] loss: 0.101\n",
            "[   17] loss: 0.111\n",
            "[   18] loss: 0.102\n",
            "[   19] loss: 0.169\n",
            "epoch:  101\n",
            "[    1] loss: 0.115\n",
            "[    2] loss: 0.096\n",
            "[    3] loss: 0.133\n",
            "[    4] loss: 0.097\n",
            "[    5] loss: 0.086\n",
            "[    6] loss: 0.250\n",
            "[    7] loss: 0.109\n",
            "[    8] loss: 0.080\n",
            "[    9] loss: 0.106\n",
            "[   10] loss: 0.103\n",
            "[   11] loss: 0.104\n",
            "[   12] loss: 0.121\n",
            "[   13] loss: 0.167\n",
            "[   14] loss: 0.182\n",
            "[   15] loss: 0.099\n",
            "[   16] loss: 0.167\n",
            "[   17] loss: 0.106\n",
            "[   18] loss: 0.121\n",
            "[   19] loss: 0.269\n",
            "train loss:  0.0972328158095479\n",
            "val loss:  0.19157131016254425\n",
            "epoch:  102\n",
            "[    1] loss: 0.098\n",
            "[    2] loss: 0.091\n",
            "[    3] loss: 0.085\n",
            "[    4] loss: 0.073\n",
            "[    5] loss: 0.127\n",
            "[    6] loss: 0.114\n",
            "[    7] loss: 0.105\n",
            "[    8] loss: 0.107\n",
            "[    9] loss: 0.091\n",
            "[   10] loss: 0.110\n",
            "[   11] loss: 0.110\n",
            "[   12] loss: 0.087\n",
            "[   13] loss: 0.108\n",
            "[   14] loss: 0.098\n",
            "[   15] loss: 0.098\n",
            "[   16] loss: 0.101\n",
            "[   17] loss: 0.146\n",
            "[   18] loss: 0.083\n",
            "[   19] loss: 0.180\n",
            "epoch:  103\n",
            "[    1] loss: 0.130\n",
            "[    2] loss: 0.120\n",
            "[    3] loss: 0.106\n",
            "[    4] loss: 0.087\n",
            "[    5] loss: 0.115\n",
            "[    6] loss: 0.091\n",
            "[    7] loss: 0.108\n",
            "[    8] loss: 0.089\n",
            "[    9] loss: 0.132\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.116\n",
            "[   12] loss: 0.127\n",
            "[   13] loss: 0.129\n",
            "[   14] loss: 0.080\n",
            "[   15] loss: 0.116\n",
            "[   16] loss: 0.072\n",
            "[   17] loss: 0.107\n",
            "[   18] loss: 0.113\n",
            "[   19] loss: 0.195\n",
            "epoch:  104\n",
            "[    1] loss: 0.104\n",
            "[    2] loss: 0.123\n",
            "[    3] loss: 0.093\n",
            "[    4] loss: 0.114\n",
            "[    5] loss: 0.127\n",
            "[    6] loss: 0.120\n",
            "[    7] loss: 0.152\n",
            "[    8] loss: 0.122\n",
            "[    9] loss: 0.098\n",
            "[   10] loss: 0.162\n",
            "[   11] loss: 0.095\n",
            "[   12] loss: 0.131\n",
            "[   13] loss: 0.142\n",
            "[   14] loss: 0.103\n",
            "[   15] loss: 0.129\n",
            "[   16] loss: 0.098\n",
            "[   17] loss: 0.131\n",
            "[   18] loss: 0.142\n",
            "[   19] loss: 0.309\n",
            "epoch:  105\n",
            "[    1] loss: 0.188\n",
            "[    2] loss: 0.117\n",
            "[    3] loss: 0.100\n",
            "[    4] loss: 0.095\n",
            "[    5] loss: 0.110\n",
            "[    6] loss: 0.115\n",
            "[    7] loss: 0.091\n",
            "[    8] loss: 0.090\n",
            "[    9] loss: 0.122\n",
            "[   10] loss: 0.152\n",
            "[   11] loss: 0.148\n",
            "[   12] loss: 0.105\n",
            "[   13] loss: 0.098\n",
            "[   14] loss: 0.145\n",
            "[   15] loss: 0.109\n",
            "[   16] loss: 0.133\n",
            "[   17] loss: 0.135\n",
            "[   18] loss: 0.104\n",
            "[   19] loss: 0.337\n",
            "epoch:  106\n",
            "[    1] loss: 0.099\n",
            "[    2] loss: 0.166\n",
            "[    3] loss: 0.144\n",
            "[    4] loss: 0.081\n",
            "[    5] loss: 0.090\n",
            "[    6] loss: 0.093\n",
            "[    7] loss: 0.125\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.082\n",
            "[   10] loss: 0.101\n",
            "[   11] loss: 0.126\n",
            "[   12] loss: 0.150\n",
            "[   13] loss: 0.107\n",
            "[   14] loss: 0.092\n",
            "[   15] loss: 0.102\n",
            "[   16] loss: 0.108\n",
            "[   17] loss: 0.081\n",
            "[   18] loss: 0.182\n",
            "[   19] loss: 0.185\n",
            "train loss:  0.09517900297856506\n",
            "val loss:  0.16331094689667225\n",
            "epoch:  107\n",
            "[    1] loss: 0.131\n",
            "[    2] loss: 0.090\n",
            "[    3] loss: 0.148\n",
            "[    4] loss: 0.087\n",
            "[    5] loss: 0.153\n",
            "[    6] loss: 0.140\n",
            "[    7] loss: 0.103\n",
            "[    8] loss: 0.117\n",
            "[    9] loss: 0.113\n",
            "[   10] loss: 0.079\n",
            "[   11] loss: 0.075\n",
            "[   12] loss: 0.149\n",
            "[   13] loss: 0.171\n",
            "[   14] loss: 0.156\n",
            "[   15] loss: 0.149\n",
            "[   16] loss: 0.130\n",
            "[   17] loss: 0.090\n",
            "[   18] loss: 0.108\n",
            "[   19] loss: 0.162\n",
            "epoch:  108\n",
            "[    1] loss: 0.093\n",
            "[    2] loss: 0.099\n",
            "[    3] loss: 0.100\n",
            "[    4] loss: 0.125\n",
            "[    5] loss: 0.122\n",
            "[    6] loss: 0.112\n",
            "[    7] loss: 0.098\n",
            "[    8] loss: 0.091\n",
            "[    9] loss: 0.130\n",
            "[   10] loss: 0.110\n",
            "[   11] loss: 0.092\n",
            "[   12] loss: 0.113\n",
            "[   13] loss: 0.103\n",
            "[   14] loss: 0.114\n",
            "[   15] loss: 0.129\n",
            "[   16] loss: 0.101\n",
            "[   17] loss: 0.121\n",
            "[   18] loss: 0.139\n",
            "[   19] loss: 0.111\n",
            "epoch:  109\n",
            "[    1] loss: 0.132\n",
            "[    2] loss: 0.097\n",
            "[    3] loss: 0.108\n",
            "[    4] loss: 0.115\n",
            "[    5] loss: 0.182\n",
            "[    6] loss: 0.116\n",
            "[    7] loss: 0.099\n",
            "[    8] loss: 0.117\n",
            "[    9] loss: 0.091\n",
            "[   10] loss: 0.086\n",
            "[   11] loss: 0.112\n",
            "[   12] loss: 0.143\n",
            "[   13] loss: 0.093\n",
            "[   14] loss: 0.118\n",
            "[   15] loss: 0.115\n",
            "[   16] loss: 0.120\n",
            "[   17] loss: 0.111\n",
            "[   18] loss: 0.100\n",
            "[   19] loss: 0.313\n",
            "epoch:  110\n",
            "[    1] loss: 0.093\n",
            "[    2] loss: 0.099\n",
            "[    3] loss: 0.118\n",
            "[    4] loss: 0.127\n",
            "[    5] loss: 0.088\n",
            "[    6] loss: 0.122\n",
            "[    7] loss: 0.101\n",
            "[    8] loss: 0.087\n",
            "[    9] loss: 0.227\n",
            "[   10] loss: 0.125\n",
            "[   11] loss: 0.104\n",
            "[   12] loss: 0.077\n",
            "[   13] loss: 0.102\n",
            "[   14] loss: 0.110\n",
            "[   15] loss: 0.089\n",
            "[   16] loss: 0.115\n",
            "[   17] loss: 0.116\n",
            "[   18] loss: 0.104\n",
            "[   19] loss: 0.298\n",
            "epoch:  111\n",
            "[    1] loss: 0.109\n",
            "[    2] loss: 0.117\n",
            "[    3] loss: 0.118\n",
            "[    4] loss: 0.101\n",
            "[    5] loss: 0.106\n",
            "[    6] loss: 0.121\n",
            "[    7] loss: 0.120\n",
            "[    8] loss: 0.123\n",
            "[    9] loss: 0.086\n",
            "[   10] loss: 0.132\n",
            "[   11] loss: 0.104\n",
            "[   12] loss: 0.148\n",
            "[   13] loss: 0.103\n",
            "[   14] loss: 0.122\n",
            "[   15] loss: 0.305\n",
            "[   16] loss: 0.122\n",
            "[   17] loss: 0.109\n",
            "[   18] loss: 0.141\n",
            "[   19] loss: 0.101\n",
            "train loss:  0.09193265120334485\n",
            "val loss:  0.19792147167026997\n",
            "epoch:  112\n",
            "[    1] loss: 0.098\n",
            "[    2] loss: 0.133\n",
            "[    3] loss: 0.124\n",
            "[    4] loss: 0.096\n",
            "[    5] loss: 0.099\n",
            "[    6] loss: 0.093\n",
            "[    7] loss: 0.119\n",
            "[    8] loss: 0.137\n",
            "[    9] loss: 0.121\n",
            "[   10] loss: 0.114\n",
            "[   11] loss: 0.178\n",
            "[   12] loss: 0.118\n",
            "[   13] loss: 0.099\n",
            "[   14] loss: 0.091\n",
            "[   15] loss: 0.117\n",
            "[   16] loss: 0.098\n",
            "[   17] loss: 0.125\n",
            "[   18] loss: 0.111\n",
            "[   19] loss: 0.276\n",
            "epoch:  113\n",
            "[    1] loss: 0.132\n",
            "[    2] loss: 0.093\n",
            "[    3] loss: 0.111\n",
            "[    4] loss: 0.173\n",
            "[    5] loss: 0.134\n",
            "[    6] loss: 0.107\n",
            "[    7] loss: 0.083\n",
            "[    8] loss: 0.123\n",
            "[    9] loss: 0.143\n",
            "[   10] loss: 0.149\n",
            "[   11] loss: 0.117\n",
            "[   12] loss: 0.147\n",
            "[   13] loss: 0.111\n",
            "[   14] loss: 0.117\n",
            "[   15] loss: 0.102\n",
            "[   16] loss: 0.163\n",
            "[   17] loss: 0.114\n",
            "[   18] loss: 0.094\n",
            "[   19] loss: 0.153\n",
            "epoch:  114\n",
            "[    1] loss: 0.121\n",
            "[    2] loss: 0.096\n",
            "[    3] loss: 0.101\n",
            "[    4] loss: 0.118\n",
            "[    5] loss: 0.122\n",
            "[    6] loss: 0.134\n",
            "[    7] loss: 0.126\n",
            "[    8] loss: 0.131\n",
            "[    9] loss: 0.115\n",
            "[   10] loss: 0.123\n",
            "[   11] loss: 0.122\n",
            "[   12] loss: 0.090\n",
            "[   13] loss: 0.160\n",
            "[   14] loss: 0.113\n",
            "[   15] loss: 0.091\n",
            "[   16] loss: 0.092\n",
            "[   17] loss: 0.078\n",
            "[   18] loss: 0.162\n",
            "[   19] loss: 0.506\n",
            "epoch:  115\n",
            "[    1] loss: 0.122\n",
            "[    2] loss: 0.113\n",
            "[    3] loss: 0.145\n",
            "[    4] loss: 0.167\n",
            "[    5] loss: 0.129\n",
            "[    6] loss: 0.113\n",
            "[    7] loss: 0.085\n",
            "[    8] loss: 0.136\n",
            "[    9] loss: 0.083\n",
            "[   10] loss: 0.119\n",
            "[   11] loss: 0.133\n",
            "[   12] loss: 0.097\n",
            "[   13] loss: 0.085\n",
            "[   14] loss: 0.103\n",
            "[   15] loss: 0.105\n",
            "[   16] loss: 0.128\n",
            "[   17] loss: 0.115\n",
            "[   18] loss: 0.159\n",
            "[   19] loss: 0.519\n",
            "epoch:  116\n",
            "[    1] loss: 0.151\n",
            "[    2] loss: 0.084\n",
            "[    3] loss: 0.096\n",
            "[    4] loss: 0.110\n",
            "[    5] loss: 0.164\n",
            "[    6] loss: 0.106\n",
            "[    7] loss: 0.108\n",
            "[    8] loss: 0.126\n",
            "[    9] loss: 0.084\n",
            "[   10] loss: 0.086\n",
            "[   11] loss: 0.146\n",
            "[   12] loss: 0.100\n",
            "[   13] loss: 0.122\n",
            "[   14] loss: 0.111\n",
            "[   15] loss: 0.086\n",
            "[   16] loss: 0.102\n",
            "[   17] loss: 0.085\n",
            "[   18] loss: 0.108\n",
            "[   19] loss: 0.356\n",
            "train loss:  0.08922483144766268\n",
            "val loss:  0.17762752063572407\n",
            "epoch:  117\n",
            "[    1] loss: 0.104\n",
            "[    2] loss: 0.122\n",
            "[    3] loss: 0.160\n",
            "[    4] loss: 0.104\n",
            "[    5] loss: 0.114\n",
            "[    6] loss: 0.127\n",
            "[    7] loss: 0.151\n",
            "[    8] loss: 0.107\n",
            "[    9] loss: 0.122\n",
            "[   10] loss: 0.069\n",
            "[   11] loss: 0.082\n",
            "[   12] loss: 0.115\n",
            "[   13] loss: 0.113\n",
            "[   14] loss: 0.189\n",
            "[   15] loss: 0.172\n",
            "[   16] loss: 0.134\n",
            "[   17] loss: 0.093\n",
            "[   18] loss: 0.120\n",
            "[   19] loss: 0.139\n",
            "epoch:  118\n",
            "[    1] loss: 0.106\n",
            "[    2] loss: 0.114\n",
            "[    3] loss: 0.103\n",
            "[    4] loss: 0.080\n",
            "[    5] loss: 0.111\n",
            "[    6] loss: 0.090\n",
            "[    7] loss: 0.098\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.096\n",
            "[   10] loss: 0.137\n",
            "[   11] loss: 0.122\n",
            "[   12] loss: 0.098\n",
            "[   13] loss: 0.129\n",
            "[   14] loss: 0.129\n",
            "[   15] loss: 0.151\n",
            "[   16] loss: 0.101\n",
            "[   17] loss: 0.141\n",
            "[   18] loss: 0.096\n",
            "[   19] loss: 0.275\n",
            "epoch:  119\n",
            "[    1] loss: 0.073\n",
            "[    2] loss: 0.103\n",
            "[    3] loss: 0.125\n",
            "[    4] loss: 0.174\n",
            "[    5] loss: 0.096\n",
            "[    6] loss: 0.154\n",
            "[    7] loss: 0.154\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.107\n",
            "[   10] loss: 0.145\n",
            "[   11] loss: 0.086\n",
            "[   12] loss: 0.125\n",
            "[   13] loss: 0.148\n",
            "[   14] loss: 0.085\n",
            "[   15] loss: 0.108\n",
            "[   16] loss: 0.102\n",
            "[   17] loss: 0.113\n",
            "[   18] loss: 0.098\n",
            "[   19] loss: 0.248\n",
            "epoch:  120\n",
            "[    1] loss: 0.124\n",
            "[    2] loss: 0.087\n",
            "[    3] loss: 0.091\n",
            "[    4] loss: 0.134\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.100\n",
            "[    7] loss: 0.093\n",
            "[    8] loss: 0.134\n",
            "[    9] loss: 0.097\n",
            "[   10] loss: 0.120\n",
            "[   11] loss: 0.115\n",
            "[   12] loss: 0.106\n",
            "[   13] loss: 0.070\n",
            "[   14] loss: 0.143\n",
            "[   15] loss: 0.106\n",
            "[   16] loss: 0.091\n",
            "[   17] loss: 0.103\n",
            "[   18] loss: 0.108\n",
            "[   19] loss: 0.210\n",
            "epoch:  121\n",
            "[    1] loss: 0.080\n",
            "[    2] loss: 0.089\n",
            "[    3] loss: 0.102\n",
            "[    4] loss: 0.087\n",
            "[    5] loss: 0.117\n",
            "[    6] loss: 0.069\n",
            "[    7] loss: 0.119\n",
            "[    8] loss: 0.089\n",
            "[    9] loss: 0.114\n",
            "[   10] loss: 0.087\n",
            "[   11] loss: 0.185\n",
            "[   12] loss: 0.124\n",
            "[   13] loss: 0.161\n",
            "[   14] loss: 0.144\n",
            "[   15] loss: 0.104\n",
            "[   16] loss: 0.159\n",
            "[   17] loss: 0.084\n",
            "[   18] loss: 0.144\n",
            "[   19] loss: 0.080\n",
            "train loss:  0.08571625854272176\n",
            "val loss:  0.17317008040845394\n",
            "epoch:  122\n",
            "[    1] loss: 0.105\n",
            "[    2] loss: 0.086\n",
            "[    3] loss: 0.133\n",
            "[    4] loss: 0.102\n",
            "[    5] loss: 0.155\n",
            "[    6] loss: 0.097\n",
            "[    7] loss: 0.113\n",
            "[    8] loss: 0.089\n",
            "[    9] loss: 0.111\n",
            "[   10] loss: 0.083\n",
            "[   11] loss: 0.121\n",
            "[   12] loss: 0.131\n",
            "[   13] loss: 0.079\n",
            "[   14] loss: 0.098\n",
            "[   15] loss: 0.099\n",
            "[   16] loss: 0.095\n",
            "[   17] loss: 0.170\n",
            "[   18] loss: 0.109\n",
            "[   19] loss: 0.181\n",
            "epoch:  123\n",
            "[    1] loss: 0.111\n",
            "[    2] loss: 0.130\n",
            "[    3] loss: 0.087\n",
            "[    4] loss: 0.096\n",
            "[    5] loss: 0.105\n",
            "[    6] loss: 0.115\n",
            "[    7] loss: 0.134\n",
            "[    8] loss: 0.095\n",
            "[    9] loss: 0.121\n",
            "[   10] loss: 0.091\n",
            "[   11] loss: 0.135\n",
            "[   12] loss: 0.146\n",
            "[   13] loss: 0.134\n",
            "[   14] loss: 0.109\n",
            "[   15] loss: 0.087\n",
            "[   16] loss: 0.112\n",
            "[   17] loss: 0.077\n",
            "[   18] loss: 0.118\n",
            "[   19] loss: 0.118\n",
            "epoch:  124\n",
            "[    1] loss: 0.095\n",
            "[    2] loss: 0.113\n",
            "[    3] loss: 0.100\n",
            "[    4] loss: 0.088\n",
            "[    5] loss: 0.128\n",
            "[    6] loss: 0.092\n",
            "[    7] loss: 0.124\n",
            "[    8] loss: 0.119\n",
            "[    9] loss: 0.077\n",
            "[   10] loss: 0.095\n",
            "[   11] loss: 0.147\n",
            "[   12] loss: 0.096\n",
            "[   13] loss: 0.109\n",
            "[   14] loss: 0.115\n",
            "[   15] loss: 0.103\n",
            "[   16] loss: 0.081\n",
            "[   17] loss: 0.116\n",
            "[   18] loss: 0.169\n",
            "[   19] loss: 0.168\n",
            "epoch:  125\n",
            "[    1] loss: 0.123\n",
            "[    2] loss: 0.125\n",
            "[    3] loss: 0.103\n",
            "[    4] loss: 0.105\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.124\n",
            "[    7] loss: 0.092\n",
            "[    8] loss: 0.134\n",
            "[    9] loss: 0.074\n",
            "[   10] loss: 0.133\n",
            "[   11] loss: 0.119\n",
            "[   12] loss: 0.114\n",
            "[   13] loss: 0.170\n",
            "[   14] loss: 0.096\n",
            "[   15] loss: 0.092\n",
            "[   16] loss: 0.093\n",
            "[   17] loss: 0.115\n",
            "[   18] loss: 0.113\n",
            "[   19] loss: 0.201\n",
            "epoch:  126\n",
            "[    1] loss: 0.093\n",
            "[    2] loss: 0.138\n",
            "[    3] loss: 0.178\n",
            "[    4] loss: 0.082\n",
            "[    5] loss: 0.122\n",
            "[    6] loss: 0.124\n",
            "[    7] loss: 0.090\n",
            "[    8] loss: 0.090\n",
            "[    9] loss: 0.107\n",
            "[   10] loss: 0.115\n",
            "[   11] loss: 0.119\n",
            "[   12] loss: 0.131\n",
            "[   13] loss: 0.097\n",
            "[   14] loss: 0.090\n",
            "[   15] loss: 0.117\n",
            "[   16] loss: 0.079\n",
            "[   17] loss: 0.089\n",
            "[   18] loss: 0.126\n",
            "[   19] loss: 0.399\n",
            "train loss:  0.09040305648437318\n",
            "val loss:  0.16653921268880367\n",
            "epoch:  127\n",
            "[    1] loss: 0.110\n",
            "[    2] loss: 0.114\n",
            "[    3] loss: 0.147\n",
            "[    4] loss: 0.137\n",
            "[    5] loss: 0.096\n",
            "[    6] loss: 0.095\n",
            "[    7] loss: 0.093\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.097\n",
            "[   10] loss: 0.151\n",
            "[   11] loss: 0.083\n",
            "[   12] loss: 0.090\n",
            "[   13] loss: 0.126\n",
            "[   14] loss: 0.099\n",
            "[   15] loss: 0.100\n",
            "[   16] loss: 0.120\n",
            "[   17] loss: 0.090\n",
            "[   18] loss: 0.128\n",
            "[   19] loss: 0.330\n",
            "epoch:  128\n",
            "[    1] loss: 0.096\n",
            "[    2] loss: 0.111\n",
            "[    3] loss: 0.115\n",
            "[    4] loss: 0.096\n",
            "[    5] loss: 0.107\n",
            "[    6] loss: 0.123\n",
            "[    7] loss: 0.085\n",
            "[    8] loss: 0.080\n",
            "[    9] loss: 0.149\n",
            "[   10] loss: 0.091\n",
            "[   11] loss: 0.139\n",
            "[   12] loss: 0.096\n",
            "[   13] loss: 0.123\n",
            "[   14] loss: 0.118\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.146\n",
            "[   17] loss: 0.094\n",
            "[   18] loss: 0.110\n",
            "[   19] loss: 0.170\n",
            "epoch:  129\n",
            "[    1] loss: 0.091\n",
            "[    2] loss: 0.094\n",
            "[    3] loss: 0.077\n",
            "[    4] loss: 0.125\n",
            "[    5] loss: 0.098\n",
            "[    6] loss: 0.108\n",
            "[    7] loss: 0.134\n",
            "[    8] loss: 0.107\n",
            "[    9] loss: 0.100\n",
            "[   10] loss: 0.098\n",
            "[   11] loss: 0.111\n",
            "[   12] loss: 0.115\n",
            "[   13] loss: 0.089\n",
            "[   14] loss: 0.107\n",
            "[   15] loss: 0.140\n",
            "[   16] loss: 0.175\n",
            "[   17] loss: 0.138\n",
            "[   18] loss: 0.112\n",
            "[   19] loss: 0.294\n",
            "epoch:  130\n",
            "[    1] loss: 0.094\n",
            "[    2] loss: 0.111\n",
            "[    3] loss: 0.105\n",
            "[    4] loss: 0.119\n",
            "[    5] loss: 0.182\n",
            "[    6] loss: 0.080\n",
            "[    7] loss: 0.087\n",
            "[    8] loss: 0.176\n",
            "[    9] loss: 0.203\n",
            "[   10] loss: 0.106\n",
            "[   11] loss: 0.118\n",
            "[   12] loss: 0.125\n",
            "[   13] loss: 0.124\n",
            "[   14] loss: 0.076\n",
            "[   15] loss: 0.111\n",
            "[   16] loss: 0.102\n",
            "[   17] loss: 0.121\n",
            "[   18] loss: 0.084\n",
            "[   19] loss: 0.146\n",
            "epoch:  131\n",
            "[    1] loss: 0.094\n",
            "[    2] loss: 0.134\n",
            "[    3] loss: 0.165\n",
            "[    4] loss: 0.111\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.125\n",
            "[    7] loss: 0.117\n",
            "[    8] loss: 0.104\n",
            "[    9] loss: 0.111\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.122\n",
            "[   12] loss: 0.102\n",
            "[   13] loss: 0.094\n",
            "[   14] loss: 0.100\n",
            "[   15] loss: 0.111\n",
            "[   16] loss: 0.094\n",
            "[   17] loss: 0.116\n",
            "[   18] loss: 0.133\n",
            "[   19] loss: 0.067\n",
            "train loss:  0.0917959181482301\n",
            "val loss:  0.17184609174728394\n",
            "epoch:  132\n",
            "[    1] loss: 0.079\n",
            "[    2] loss: 0.122\n",
            "[    3] loss: 0.090\n",
            "[    4] loss: 0.087\n",
            "[    5] loss: 0.086\n",
            "[    6] loss: 0.138\n",
            "[    7] loss: 0.080\n",
            "[    8] loss: 0.128\n",
            "[    9] loss: 0.149\n",
            "[   10] loss: 0.145\n",
            "[   11] loss: 0.176\n",
            "[   12] loss: 0.100\n",
            "[   13] loss: 0.096\n",
            "[   14] loss: 0.117\n",
            "[   15] loss: 0.090\n",
            "[   16] loss: 0.082\n",
            "[   17] loss: 0.116\n",
            "[   18] loss: 0.122\n",
            "[   19] loss: 0.498\n",
            "epoch:  133\n",
            "[    1] loss: 0.125\n",
            "[    2] loss: 0.088\n",
            "[    3] loss: 0.146\n",
            "[    4] loss: 0.114\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.106\n",
            "[    7] loss: 0.124\n",
            "[    8] loss: 0.109\n",
            "[    9] loss: 0.112\n",
            "[   10] loss: 0.141\n",
            "[   11] loss: 0.135\n",
            "[   12] loss: 0.091\n",
            "[   13] loss: 0.095\n",
            "[   14] loss: 0.104\n",
            "[   15] loss: 0.120\n",
            "[   16] loss: 0.091\n",
            "[   17] loss: 0.077\n",
            "[   18] loss: 0.097\n",
            "[   19] loss: 0.147\n",
            "epoch:  134\n",
            "[    1] loss: 0.096\n",
            "[    2] loss: 0.074\n",
            "[    3] loss: 0.087\n",
            "[    4] loss: 0.144\n",
            "[    5] loss: 0.133\n",
            "[    6] loss: 0.103\n",
            "[    7] loss: 0.100\n",
            "[    8] loss: 0.104\n",
            "[    9] loss: 0.140\n",
            "[   10] loss: 0.114\n",
            "[   11] loss: 0.099\n",
            "[   12] loss: 0.087\n",
            "[   13] loss: 0.108\n",
            "[   14] loss: 0.147\n",
            "[   15] loss: 0.120\n",
            "[   16] loss: 0.113\n",
            "[   17] loss: 0.097\n",
            "[   18] loss: 0.103\n",
            "[   19] loss: 0.317\n",
            "epoch:  135\n",
            "[    1] loss: 0.143\n",
            "[    2] loss: 0.123\n",
            "[    3] loss: 0.146\n",
            "[    4] loss: 0.100\n",
            "[    5] loss: 0.092\n",
            "[    6] loss: 0.149\n",
            "[    7] loss: 0.104\n",
            "[    8] loss: 0.110\n",
            "[    9] loss: 0.113\n",
            "[   10] loss: 0.094\n",
            "[   11] loss: 0.109\n",
            "[   12] loss: 0.088\n",
            "[   13] loss: 0.155\n",
            "[   14] loss: 0.084\n",
            "[   15] loss: 0.115\n",
            "[   16] loss: 0.101\n",
            "[   17] loss: 0.114\n",
            "[   18] loss: 0.095\n",
            "[   19] loss: 0.223\n",
            "epoch:  136\n",
            "[    1] loss: 0.095\n",
            "[    2] loss: 0.103\n",
            "[    3] loss: 0.096\n",
            "[    4] loss: 0.108\n",
            "[    5] loss: 0.103\n",
            "[    6] loss: 0.086\n",
            "[    7] loss: 0.097\n",
            "[    8] loss: 0.100\n",
            "[    9] loss: 0.088\n",
            "[   10] loss: 0.125\n",
            "[   11] loss: 0.151\n",
            "[   12] loss: 0.101\n",
            "[   13] loss: 0.168\n",
            "[   14] loss: 0.083\n",
            "[   15] loss: 0.114\n",
            "[   16] loss: 0.092\n",
            "[   17] loss: 0.127\n",
            "[   18] loss: 0.104\n",
            "[   19] loss: 0.161\n",
            "train loss:  0.08588042767608867\n",
            "val loss:  0.1753100734204054\n",
            "epoch:  137\n",
            "[    1] loss: 0.141\n",
            "[    2] loss: 0.122\n",
            "[    3] loss: 0.109\n",
            "[    4] loss: 0.094\n",
            "[    5] loss: 0.169\n",
            "[    6] loss: 0.120\n",
            "[    7] loss: 0.105\n",
            "[    8] loss: 0.136\n",
            "[    9] loss: 0.177\n",
            "[   10] loss: 0.116\n",
            "[   11] loss: 0.121\n",
            "[   12] loss: 0.132\n",
            "[   13] loss: 0.151\n",
            "[   14] loss: 0.101\n",
            "[   15] loss: 0.098\n",
            "[   16] loss: 0.107\n",
            "[   17] loss: 0.130\n",
            "[   18] loss: 0.154\n",
            "[   19] loss: 0.390\n",
            "epoch:  138\n",
            "[    1] loss: 0.118\n",
            "[    2] loss: 0.088\n",
            "[    3] loss: 0.103\n",
            "[    4] loss: 0.120\n",
            "[    5] loss: 0.089\n",
            "[    6] loss: 0.102\n",
            "[    7] loss: 0.102\n",
            "[    8] loss: 0.092\n",
            "[    9] loss: 0.118\n",
            "[   10] loss: 0.107\n",
            "[   11] loss: 0.074\n",
            "[   12] loss: 0.139\n",
            "[   13] loss: 0.178\n",
            "[   14] loss: 0.101\n",
            "[   15] loss: 0.120\n",
            "[   16] loss: 0.089\n",
            "[   17] loss: 0.173\n",
            "[   18] loss: 0.116\n",
            "[   19] loss: 0.090\n",
            "epoch:  139\n",
            "[    1] loss: 0.096\n",
            "[    2] loss: 0.150\n",
            "[    3] loss: 0.112\n",
            "[    4] loss: 0.098\n",
            "[    5] loss: 0.097\n",
            "[    6] loss: 0.124\n",
            "[    7] loss: 0.138\n",
            "[    8] loss: 0.083\n",
            "[    9] loss: 0.115\n",
            "[   10] loss: 0.112\n",
            "[   11] loss: 0.119\n",
            "[   12] loss: 0.184\n",
            "[   13] loss: 0.109\n",
            "[   14] loss: 0.080\n",
            "[   15] loss: 0.098\n",
            "[   16] loss: 0.123\n",
            "[   17] loss: 0.083\n",
            "[   18] loss: 0.143\n",
            "[   19] loss: 0.260\n",
            "epoch:  140\n",
            "[    1] loss: 0.077\n",
            "[    2] loss: 0.092\n",
            "[    3] loss: 0.164\n",
            "[    4] loss: 0.067\n",
            "[    5] loss: 0.125\n",
            "[    6] loss: 0.123\n",
            "[    7] loss: 0.101\n",
            "[    8] loss: 0.113\n",
            "[    9] loss: 0.132\n",
            "[   10] loss: 0.158\n",
            "[   11] loss: 0.125\n",
            "[   12] loss: 0.075\n",
            "[   13] loss: 0.154\n",
            "[   14] loss: 0.169\n",
            "[   15] loss: 0.115\n",
            "[   16] loss: 0.151\n",
            "[   17] loss: 0.127\n",
            "[   18] loss: 0.119\n",
            "[   19] loss: 0.259\n",
            "epoch:  141\n",
            "[    1] loss: 0.089\n",
            "[    2] loss: 0.086\n",
            "[    3] loss: 0.121\n",
            "[    4] loss: 0.085\n",
            "[    5] loss: 0.081\n",
            "[    6] loss: 0.122\n",
            "[    7] loss: 0.123\n",
            "[    8] loss: 0.110\n",
            "[    9] loss: 0.104\n",
            "[   10] loss: 0.096\n",
            "[   11] loss: 0.109\n",
            "[   12] loss: 0.119\n",
            "[   13] loss: 0.098\n",
            "[   14] loss: 0.093\n",
            "[   15] loss: 0.156\n",
            "[   16] loss: 0.086\n",
            "[   17] loss: 0.103\n",
            "[   18] loss: 0.124\n",
            "[   19] loss: 0.321\n",
            "train loss:  0.09631694872480105\n",
            "val loss:  0.17204073816537857\n",
            "epoch:  142\n",
            "[    1] loss: 0.095\n",
            "[    2] loss: 0.090\n",
            "[    3] loss: 0.162\n",
            "[    4] loss: 0.112\n",
            "[    5] loss: 0.072\n",
            "[    6] loss: 0.093\n",
            "[    7] loss: 0.141\n",
            "[    8] loss: 0.092\n",
            "[    9] loss: 0.083\n",
            "[   10] loss: 0.086\n",
            "[   11] loss: 0.135\n",
            "[   12] loss: 0.101\n",
            "[   13] loss: 0.128\n",
            "[   14] loss: 0.111\n",
            "[   15] loss: 0.132\n",
            "[   16] loss: 0.182\n",
            "[   17] loss: 0.107\n",
            "[   18] loss: 0.068\n",
            "[   19] loss: 0.224\n",
            "epoch:  143\n",
            "[    1] loss: 0.128\n",
            "[    2] loss: 0.141\n",
            "[    3] loss: 0.082\n",
            "[    4] loss: 0.124\n",
            "[    5] loss: 0.097\n",
            "[    6] loss: 0.132\n",
            "[    7] loss: 0.119\n",
            "[    8] loss: 0.086\n",
            "[    9] loss: 0.077\n",
            "[   10] loss: 0.108\n",
            "[   11] loss: 0.080\n",
            "[   12] loss: 0.093\n",
            "[   13] loss: 0.194\n",
            "[   14] loss: 0.105\n",
            "[   15] loss: 0.153\n",
            "[   16] loss: 0.085\n",
            "[   17] loss: 0.140\n",
            "[   18] loss: 0.077\n",
            "[   19] loss: 0.226\n",
            "epoch:  144\n",
            "[    1] loss: 0.095\n",
            "[    2] loss: 0.139\n",
            "[    3] loss: 0.130\n",
            "[    4] loss: 0.077\n",
            "[    5] loss: 0.136\n",
            "[    6] loss: 0.068\n",
            "[    7] loss: 0.080\n",
            "[    8] loss: 0.079\n",
            "[    9] loss: 0.131\n",
            "[   10] loss: 0.218\n",
            "[   11] loss: 0.104\n",
            "[   12] loss: 0.089\n",
            "[   13] loss: 0.101\n",
            "[   14] loss: 0.124\n",
            "[   15] loss: 0.085\n",
            "[   16] loss: 0.081\n",
            "[   17] loss: 0.123\n",
            "[   18] loss: 0.176\n",
            "[   19] loss: 0.364\n",
            "epoch:  145\n",
            "[    1] loss: 0.097\n",
            "[    2] loss: 0.102\n",
            "[    3] loss: 0.086\n",
            "[    4] loss: 0.097\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.114\n",
            "[    7] loss: 0.119\n",
            "[    8] loss: 0.083\n",
            "[    9] loss: 0.079\n",
            "[   10] loss: 0.097\n",
            "[   11] loss: 0.114\n",
            "[   12] loss: 0.107\n",
            "[   13] loss: 0.094\n",
            "[   14] loss: 0.093\n",
            "[   15] loss: 0.110\n",
            "[   16] loss: 0.134\n",
            "[   17] loss: 0.089\n",
            "[   18] loss: 0.096\n",
            "[   19] loss: 0.499\n",
            "epoch:  146\n",
            "[    1] loss: 0.094\n",
            "[    2] loss: 0.097\n",
            "[    3] loss: 0.114\n",
            "[    4] loss: 0.118\n",
            "[    5] loss: 0.104\n",
            "[    6] loss: 0.108\n",
            "[    7] loss: 0.105\n",
            "[    8] loss: 0.115\n",
            "[    9] loss: 0.103\n",
            "[   10] loss: 0.112\n",
            "[   11] loss: 0.084\n",
            "[   12] loss: 0.101\n",
            "[   13] loss: 0.105\n",
            "[   14] loss: 0.121\n",
            "[   15] loss: 0.119\n",
            "[   16] loss: 0.138\n",
            "[   17] loss: 0.099\n",
            "[   18] loss: 0.068\n",
            "[   19] loss: 0.321\n",
            "train loss:  0.08650113795610036\n",
            "val loss:  0.21235318668186665\n",
            "epoch:  147\n",
            "[    1] loss: 0.114\n",
            "[    2] loss: 0.117\n",
            "[    3] loss: 0.112\n",
            "[    4] loss: 0.113\n",
            "[    5] loss: 0.160\n",
            "[    6] loss: 0.101\n",
            "[    7] loss: 0.123\n",
            "[    8] loss: 0.122\n",
            "[    9] loss: 0.086\n",
            "[   10] loss: 0.132\n",
            "[   11] loss: 0.118\n",
            "[   12] loss: 0.114\n",
            "[   13] loss: 0.088\n",
            "[   14] loss: 0.160\n",
            "[   15] loss: 0.088\n",
            "[   16] loss: 0.099\n",
            "[   17] loss: 0.148\n",
            "[   18] loss: 0.097\n",
            "[   19] loss: 0.348\n",
            "epoch:  148\n",
            "[    1] loss: 0.085\n",
            "[    2] loss: 0.178\n",
            "[    3] loss: 0.124\n",
            "[    4] loss: 0.097\n",
            "[    5] loss: 0.105\n",
            "[    6] loss: 0.119\n",
            "[    7] loss: 0.091\n",
            "[    8] loss: 0.098\n",
            "[    9] loss: 0.104\n",
            "[   10] loss: 0.109\n",
            "[   11] loss: 0.100\n",
            "[   12] loss: 0.106\n",
            "[   13] loss: 0.064\n",
            "[   14] loss: 0.094\n",
            "[   15] loss: 0.082\n",
            "[   16] loss: 0.104\n",
            "[   17] loss: 0.100\n",
            "[   18] loss: 0.090\n",
            "[   19] loss: 0.436\n",
            "epoch:  149\n",
            "[    1] loss: 0.097\n",
            "[    2] loss: 0.091\n",
            "[    3] loss: 0.084\n",
            "[    4] loss: 0.109\n",
            "[    5] loss: 0.111\n",
            "[    6] loss: 0.148\n",
            "[    7] loss: 0.069\n",
            "[    8] loss: 0.122\n",
            "[    9] loss: 0.118\n",
            "[   10] loss: 0.109\n",
            "[   11] loss: 0.096\n",
            "[   12] loss: 0.140\n",
            "[   13] loss: 0.145\n",
            "[   14] loss: 0.145\n",
            "[   15] loss: 0.126\n",
            "[   16] loss: 0.162\n",
            "[   17] loss: 0.097\n",
            "[   18] loss: 0.085\n",
            "[   19] loss: 0.175\n",
            "epoch:  150\n",
            "[    1] loss: 0.109\n",
            "[    2] loss: 0.118\n",
            "[    3] loss: 0.110\n",
            "[    4] loss: 0.093\n",
            "[    5] loss: 0.071\n",
            "[    6] loss: 0.093\n",
            "[    7] loss: 0.119\n",
            "[    8] loss: 0.089\n",
            "[    9] loss: 0.115\n",
            "[   10] loss: 0.105\n",
            "[   11] loss: 0.107\n",
            "[   12] loss: 0.136\n",
            "[   13] loss: 0.108\n",
            "[   14] loss: 0.078\n",
            "[   15] loss: 0.094\n",
            "[   16] loss: 0.119\n",
            "[   17] loss: 0.121\n",
            "[   18] loss: 0.093\n",
            "[   19] loss: 0.592\n",
            "epoch:  151\n",
            "[    1] loss: 0.081\n",
            "[    2] loss: 0.203\n",
            "[    3] loss: 0.096\n",
            "[    4] loss: 0.129\n",
            "[    5] loss: 0.098\n",
            "[    6] loss: 0.121\n",
            "[    7] loss: 0.128\n",
            "[    8] loss: 0.071\n",
            "[    9] loss: 0.093\n",
            "[   10] loss: 0.148\n",
            "[   11] loss: 0.117\n",
            "[   12] loss: 0.088\n",
            "[   13] loss: 0.095\n",
            "[   14] loss: 0.116\n",
            "[   15] loss: 0.108\n",
            "[   16] loss: 0.115\n",
            "[   17] loss: 0.093\n",
            "[   18] loss: 0.096\n",
            "[   19] loss: 0.447\n",
            "train loss:  0.08254793934140574\n",
            "val loss:  0.17569736577570438\n",
            "epoch:  152\n",
            "[    1] loss: 0.126\n",
            "[    2] loss: 0.100\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.112\n",
            "[    5] loss: 0.125\n",
            "[    6] loss: 0.119\n",
            "[    7] loss: 0.113\n",
            "[    8] loss: 0.133\n",
            "[    9] loss: 0.102\n",
            "[   10] loss: 0.117\n",
            "[   11] loss: 0.097\n",
            "[   12] loss: 0.109\n",
            "[   13] loss: 0.094\n",
            "[   14] loss: 0.092\n",
            "[   15] loss: 0.077\n",
            "[   16] loss: 0.161\n",
            "[   17] loss: 0.111\n",
            "[   18] loss: 0.116\n",
            "[   19] loss: 0.204\n",
            "epoch:  153\n",
            "[    1] loss: 0.100\n",
            "[    2] loss: 0.132\n",
            "[    3] loss: 0.075\n",
            "[    4] loss: 0.130\n",
            "[    5] loss: 0.135\n",
            "[    6] loss: 0.125\n",
            "[    7] loss: 0.109\n",
            "[    8] loss: 0.167\n",
            "[    9] loss: 0.199\n",
            "[   10] loss: 0.124\n",
            "[   11] loss: 0.097\n",
            "[   12] loss: 0.082\n",
            "[   13] loss: 0.091\n",
            "[   14] loss: 0.101\n",
            "[   15] loss: 0.118\n",
            "[   16] loss: 0.118\n",
            "[   17] loss: 0.121\n",
            "[   18] loss: 0.070\n",
            "[   19] loss: 0.223\n",
            "epoch:  154\n",
            "[    1] loss: 0.123\n",
            "[    2] loss: 0.137\n",
            "[    3] loss: 0.090\n",
            "[    4] loss: 0.102\n",
            "[    5] loss: 0.091\n",
            "[    6] loss: 0.079\n",
            "[    7] loss: 0.102\n",
            "[    8] loss: 0.099\n",
            "[    9] loss: 0.102\n",
            "[   10] loss: 0.163\n",
            "[   11] loss: 0.076\n",
            "[   12] loss: 0.124\n",
            "[   13] loss: 0.144\n",
            "[   14] loss: 0.126\n",
            "[   15] loss: 0.100\n",
            "[   16] loss: 0.114\n",
            "[   17] loss: 0.131\n",
            "[   18] loss: 0.129\n",
            "[   19] loss: 0.260\n",
            "epoch:  155\n",
            "[    1] loss: 0.140\n",
            "[    2] loss: 0.133\n",
            "[    3] loss: 0.127\n",
            "[    4] loss: 0.069\n",
            "[    5] loss: 0.081\n",
            "[    6] loss: 0.087\n",
            "[    7] loss: 0.124\n",
            "[    8] loss: 0.119\n",
            "[    9] loss: 0.101\n",
            "[   10] loss: 0.116\n",
            "[   11] loss: 0.121\n",
            "[   12] loss: 0.123\n",
            "[   13] loss: 0.098\n",
            "[   14] loss: 0.134\n",
            "[   15] loss: 0.074\n",
            "[   16] loss: 0.095\n",
            "[   17] loss: 0.128\n",
            "[   18] loss: 0.087\n",
            "[   19] loss: 0.226\n",
            "epoch:  156\n",
            "[    1] loss: 0.089\n",
            "[    2] loss: 0.087\n",
            "[    3] loss: 0.127\n",
            "[    4] loss: 0.083\n",
            "[    5] loss: 0.206\n",
            "[    6] loss: 0.164\n",
            "[    7] loss: 0.129\n",
            "[    8] loss: 0.114\n",
            "[    9] loss: 0.121\n",
            "[   10] loss: 0.088\n",
            "[   11] loss: 0.098\n",
            "[   12] loss: 0.092\n",
            "[   13] loss: 0.117\n",
            "[   14] loss: 0.124\n",
            "[   15] loss: 0.109\n",
            "[   16] loss: 0.079\n",
            "[   17] loss: 0.088\n",
            "[   18] loss: 0.113\n",
            "[   19] loss: 0.328\n",
            "train loss:  0.09928493100382826\n",
            "val loss:  0.1663548517972231\n",
            "epoch:  157\n",
            "[    1] loss: 0.093\n",
            "[    2] loss: 0.103\n",
            "[    3] loss: 0.092\n",
            "[    4] loss: 0.146\n",
            "[    5] loss: 0.116\n",
            "[    6] loss: 0.115\n",
            "[    7] loss: 0.092\n",
            "[    8] loss: 0.174\n",
            "[    9] loss: 0.098\n",
            "[   10] loss: 0.088\n",
            "[   11] loss: 0.092\n",
            "[   12] loss: 0.090\n",
            "[   13] loss: 0.127\n",
            "[   14] loss: 0.121\n",
            "[   15] loss: 0.085\n",
            "[   16] loss: 0.084\n",
            "[   17] loss: 0.133\n",
            "[   18] loss: 0.064\n",
            "[   19] loss: 1.184\n",
            "epoch:  158\n",
            "[    1] loss: 0.181\n",
            "[    2] loss: 0.087\n",
            "[    3] loss: 0.081\n",
            "[    4] loss: 0.101\n",
            "[    5] loss: 0.079\n",
            "[    6] loss: 0.129\n",
            "[    7] loss: 0.084\n",
            "[    8] loss: 0.139\n",
            "[    9] loss: 0.085\n",
            "[   10] loss: 0.126\n",
            "[   11] loss: 0.095\n",
            "[   12] loss: 0.082\n",
            "[   13] loss: 0.089\n",
            "[   14] loss: 0.114\n",
            "[   15] loss: 0.116\n",
            "[   16] loss: 0.145\n",
            "[   17] loss: 0.089\n",
            "[   18] loss: 0.093\n",
            "[   19] loss: 0.243\n",
            "epoch:  159\n",
            "[    1] loss: 0.108\n",
            "[    2] loss: 0.142\n",
            "[    3] loss: 0.088\n",
            "[    4] loss: 0.094\n",
            "[    5] loss: 0.116\n",
            "[    6] loss: 0.090\n",
            "[    7] loss: 0.107\n",
            "[    8] loss: 0.137\n",
            "[    9] loss: 0.114\n",
            "[   10] loss: 0.094\n",
            "[   11] loss: 0.080\n",
            "[   12] loss: 0.075\n",
            "[   13] loss: 0.099\n",
            "[   14] loss: 0.111\n",
            "[   15] loss: 0.116\n",
            "[   16] loss: 0.128\n",
            "[   17] loss: 0.113\n",
            "[   18] loss: 0.113\n",
            "[   19] loss: 0.258\n",
            "epoch:  160\n",
            "[    1] loss: 0.105\n",
            "[    2] loss: 0.124\n",
            "[    3] loss: 0.095\n",
            "[    4] loss: 0.116\n",
            "[    5] loss: 0.120\n",
            "[    6] loss: 0.098\n",
            "[    7] loss: 0.085\n",
            "[    8] loss: 0.093\n",
            "[    9] loss: 0.114\n",
            "[   10] loss: 0.107\n",
            "[   11] loss: 0.097\n",
            "[   12] loss: 0.104\n",
            "[   13] loss: 0.141\n",
            "[   14] loss: 0.091\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.093\n",
            "[   17] loss: 0.090\n",
            "[   18] loss: 0.116\n",
            "[   19] loss: 0.208\n",
            "epoch:  161\n",
            "[    1] loss: 0.134\n",
            "[    2] loss: 0.108\n",
            "[    3] loss: 0.109\n",
            "[    4] loss: 0.100\n",
            "[    5] loss: 0.094\n",
            "[    6] loss: 0.090\n",
            "[    7] loss: 0.092\n",
            "[    8] loss: 0.127\n",
            "[    9] loss: 0.099\n",
            "[   10] loss: 0.118\n",
            "[   11] loss: 0.087\n",
            "[   12] loss: 0.115\n",
            "[   13] loss: 0.085\n",
            "[   14] loss: 0.099\n",
            "[   15] loss: 0.107\n",
            "[   16] loss: 0.071\n",
            "[   17] loss: 0.117\n",
            "[   18] loss: 0.095\n",
            "[   19] loss: 0.428\n",
            "train loss:  0.09359446825350032\n",
            "val loss:  0.17998981475830078\n",
            "epoch:  162\n",
            "[    1] loss: 0.146\n",
            "[    2] loss: 0.092\n",
            "[    3] loss: 0.105\n",
            "[    4] loss: 0.120\n",
            "[    5] loss: 0.148\n",
            "[    6] loss: 0.094\n",
            "[    7] loss: 0.116\n",
            "[    8] loss: 0.091\n",
            "[    9] loss: 0.106\n",
            "[   10] loss: 0.073\n",
            "[   11] loss: 0.097\n",
            "[   12] loss: 0.106\n",
            "[   13] loss: 0.126\n",
            "[   14] loss: 0.110\n",
            "[   15] loss: 0.147\n",
            "[   16] loss: 0.111\n",
            "[   17] loss: 0.153\n",
            "[   18] loss: 0.118\n",
            "[   19] loss: 0.120\n",
            "epoch:  163\n",
            "[    1] loss: 0.073\n",
            "[    2] loss: 0.186\n",
            "[    3] loss: 0.077\n",
            "[    4] loss: 0.105\n",
            "[    5] loss: 0.097\n",
            "[    6] loss: 0.076\n",
            "[    7] loss: 0.091\n",
            "[    8] loss: 0.104\n",
            "[    9] loss: 0.073\n",
            "[   10] loss: 0.156\n",
            "[   11] loss: 0.092\n",
            "[   12] loss: 0.089\n",
            "[   13] loss: 0.086\n",
            "[   14] loss: 0.122\n",
            "[   15] loss: 0.077\n",
            "[   16] loss: 0.107\n",
            "[   17] loss: 0.126\n",
            "[   18] loss: 0.106\n",
            "[   19] loss: 0.200\n",
            "epoch:  164\n",
            "[    1] loss: 0.080\n",
            "[    2] loss: 0.101\n",
            "[    3] loss: 0.146\n",
            "[    4] loss: 0.111\n",
            "[    5] loss: 0.112\n",
            "[    6] loss: 0.096\n",
            "[    7] loss: 0.098\n",
            "[    8] loss: 0.132\n",
            "[    9] loss: 0.123\n",
            "[   10] loss: 0.080\n",
            "[   11] loss: 0.149\n",
            "[   12] loss: 0.100\n",
            "[   13] loss: 0.085\n",
            "[   14] loss: 0.115\n",
            "[   15] loss: 0.085\n",
            "[   16] loss: 0.093\n",
            "[   17] loss: 0.135\n",
            "[   18] loss: 0.088\n",
            "[   19] loss: 0.401\n",
            "epoch:  165\n",
            "[    1] loss: 0.093\n",
            "[    2] loss: 0.111\n",
            "[    3] loss: 0.089\n",
            "[    4] loss: 0.095\n",
            "[    5] loss: 0.134\n",
            "[    6] loss: 0.098\n",
            "[    7] loss: 0.174\n",
            "[    8] loss: 0.089\n",
            "[    9] loss: 0.124\n",
            "[   10] loss: 0.114\n",
            "[   11] loss: 0.093\n",
            "[   12] loss: 0.080\n",
            "[   13] loss: 0.110\n",
            "[   14] loss: 0.090\n",
            "[   15] loss: 0.147\n",
            "[   16] loss: 0.084\n",
            "[   17] loss: 0.091\n",
            "[   18] loss: 0.139\n",
            "[   19] loss: 0.595\n",
            "epoch:  166\n",
            "[    1] loss: 0.096\n",
            "[    2] loss: 0.098\n",
            "[    3] loss: 0.181\n",
            "[    4] loss: 0.084\n",
            "[    5] loss: 0.169\n",
            "[    6] loss: 0.100\n",
            "[    7] loss: 0.100\n",
            "[    8] loss: 0.098\n",
            "[    9] loss: 0.154\n",
            "[   10] loss: 0.133\n",
            "[   11] loss: 0.092\n",
            "[   12] loss: 0.196\n",
            "[   13] loss: 0.094\n",
            "[   14] loss: 0.159\n",
            "[   15] loss: 0.092\n",
            "[   16] loss: 0.102\n",
            "[   17] loss: 0.087\n",
            "[   18] loss: 0.092\n",
            "[   19] loss: 0.278\n",
            "train loss:  0.08518179484149989\n",
            "val loss:  0.18175924569368362\n",
            "epoch:  167\n",
            "[    1] loss: 0.092\n",
            "[    2] loss: 0.115\n",
            "[    3] loss: 0.094\n",
            "[    4] loss: 0.120\n",
            "[    5] loss: 0.122\n",
            "[    6] loss: 0.099\n",
            "[    7] loss: 0.103\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.081\n",
            "[   10] loss: 0.125\n",
            "[   11] loss: 0.108\n",
            "[   12] loss: 0.086\n",
            "[   13] loss: 0.104\n",
            "[   14] loss: 0.175\n",
            "[   15] loss: 0.099\n",
            "[   16] loss: 0.118\n",
            "[   17] loss: 0.083\n",
            "[   18] loss: 0.117\n",
            "[   19] loss: 0.575\n",
            "epoch:  168\n",
            "[    1] loss: 0.147\n",
            "[    2] loss: 0.144\n",
            "[    3] loss: 0.112\n",
            "[    4] loss: 0.068\n",
            "[    5] loss: 0.112\n",
            "[    6] loss: 0.093\n",
            "[    7] loss: 0.093\n",
            "[    8] loss: 0.099\n",
            "[    9] loss: 0.097\n",
            "[   10] loss: 0.110\n",
            "[   11] loss: 0.122\n",
            "[   12] loss: 0.091\n",
            "[   13] loss: 0.139\n",
            "[   14] loss: 0.098\n",
            "[   15] loss: 0.163\n",
            "[   16] loss: 0.101\n",
            "[   17] loss: 0.142\n",
            "[   18] loss: 0.107\n",
            "[   19] loss: 0.171\n",
            "epoch:  169\n",
            "[    1] loss: 0.099\n",
            "[    2] loss: 0.103\n",
            "[    3] loss: 0.131\n",
            "[    4] loss: 0.119\n",
            "[    5] loss: 0.089\n",
            "[    6] loss: 0.111\n",
            "[    7] loss: 0.150\n",
            "[    8] loss: 0.080\n",
            "[    9] loss: 0.084\n",
            "[   10] loss: 0.100\n",
            "[   11] loss: 0.104\n",
            "[   12] loss: 0.118\n",
            "[   13] loss: 0.191\n",
            "[   14] loss: 0.140\n",
            "[   15] loss: 0.106\n",
            "[   16] loss: 0.094\n",
            "[   17] loss: 0.095\n",
            "[   18] loss: 0.121\n",
            "[   19] loss: 0.133\n",
            "epoch:  170\n",
            "[    1] loss: 0.136\n",
            "[    2] loss: 0.093\n",
            "[    3] loss: 0.095\n",
            "[    4] loss: 0.111\n",
            "[    5] loss: 0.082\n",
            "[    6] loss: 0.110\n",
            "[    7] loss: 0.103\n",
            "[    8] loss: 0.111\n",
            "[    9] loss: 0.114\n",
            "[   10] loss: 0.110\n",
            "[   11] loss: 0.106\n",
            "[   12] loss: 0.175\n",
            "[   13] loss: 0.147\n",
            "[   14] loss: 0.116\n",
            "[   15] loss: 0.135\n",
            "[   16] loss: 0.108\n",
            "[   17] loss: 0.084\n",
            "[   18] loss: 0.126\n",
            "[   19] loss: 0.289\n",
            "epoch:  171\n",
            "[    1] loss: 0.129\n",
            "[    2] loss: 0.087\n",
            "[    3] loss: 0.121\n",
            "[    4] loss: 0.119\n",
            "[    5] loss: 0.101\n",
            "[    6] loss: 0.074\n",
            "[    7] loss: 0.193\n",
            "[    8] loss: 0.096\n",
            "[    9] loss: 0.093\n",
            "[   10] loss: 0.109\n",
            "[   11] loss: 0.140\n",
            "[   12] loss: 0.093\n",
            "[   13] loss: 0.082\n",
            "[   14] loss: 0.118\n",
            "[   15] loss: 0.098\n",
            "[   16] loss: 0.080\n",
            "[   17] loss: 0.117\n",
            "[   18] loss: 0.103\n",
            "[   19] loss: 0.513\n",
            "train loss:  0.08528513127170942\n",
            "val loss:  0.1734016016125679\n",
            "epoch:  172\n",
            "[    1] loss: 0.088\n",
            "[    2] loss: 0.101\n",
            "[    3] loss: 0.102\n",
            "[    4] loss: 0.114\n",
            "[    5] loss: 0.095\n",
            "[    6] loss: 0.152\n",
            "[    7] loss: 0.092\n",
            "[    8] loss: 0.113\n",
            "[    9] loss: 0.100\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.094\n",
            "[   12] loss: 0.106\n",
            "[   13] loss: 0.076\n",
            "[   14] loss: 0.157\n",
            "[   15] loss: 0.093\n",
            "[   16] loss: 0.130\n",
            "[   17] loss: 0.126\n",
            "[   18] loss: 0.126\n",
            "[   19] loss: 0.538\n",
            "epoch:  173\n",
            "[    1] loss: 0.089\n",
            "[    2] loss: 0.117\n",
            "[    3] loss: 0.114\n",
            "[    4] loss: 0.116\n",
            "[    5] loss: 0.087\n",
            "[    6] loss: 0.104\n",
            "[    7] loss: 0.138\n",
            "[    8] loss: 0.198\n",
            "[    9] loss: 0.114\n",
            "[   10] loss: 0.093\n",
            "[   11] loss: 0.153\n",
            "[   12] loss: 0.110\n",
            "[   13] loss: 0.090\n",
            "[   14] loss: 0.120\n",
            "[   15] loss: 0.115\n",
            "[   16] loss: 0.126\n",
            "[   17] loss: 0.111\n",
            "[   18] loss: 0.103\n",
            "[   19] loss: 0.278\n",
            "epoch:  174\n",
            "[    1] loss: 0.140\n",
            "[    2] loss: 0.071\n",
            "[    3] loss: 0.095\n",
            "[    4] loss: 0.114\n",
            "[    5] loss: 0.083\n",
            "[    6] loss: 0.125\n",
            "[    7] loss: 0.112\n",
            "[    8] loss: 0.098\n",
            "[    9] loss: 0.126\n",
            "[   10] loss: 0.128\n",
            "[   11] loss: 0.082\n",
            "[   12] loss: 0.085\n",
            "[   13] loss: 0.085\n",
            "[   14] loss: 0.146\n",
            "[   15] loss: 0.077\n",
            "[   16] loss: 0.085\n",
            "[   17] loss: 0.204\n",
            "[   18] loss: 0.102\n",
            "[   19] loss: 0.176\n",
            "epoch:  175\n",
            "[    1] loss: 0.118\n",
            "[    2] loss: 0.136\n",
            "[    3] loss: 0.095\n",
            "[    4] loss: 0.143\n",
            "[    5] loss: 0.115\n",
            "[    6] loss: 0.155\n",
            "[    7] loss: 0.115\n",
            "[    8] loss: 0.120\n",
            "[    9] loss: 0.105\n",
            "[   10] loss: 0.084\n",
            "[   11] loss: 0.134\n",
            "[   12] loss: 0.091\n",
            "[   13] loss: 0.193\n",
            "[   14] loss: 0.083\n",
            "[   15] loss: 0.126\n",
            "[   16] loss: 0.077\n",
            "[   17] loss: 0.124\n",
            "[   18] loss: 0.112\n",
            "[   19] loss: 0.142\n",
            "epoch:  176\n",
            "[    1] loss: 0.086\n",
            "[    2] loss: 0.114\n",
            "[    3] loss: 0.091\n",
            "[    4] loss: 0.115\n",
            "[    5] loss: 0.143\n",
            "[    6] loss: 0.112\n",
            "[    7] loss: 0.086\n",
            "[    8] loss: 0.144\n",
            "[    9] loss: 0.107\n",
            "[   10] loss: 0.121\n",
            "[   11] loss: 0.108\n",
            "[   12] loss: 0.093\n",
            "[   13] loss: 0.147\n",
            "[   14] loss: 0.110\n",
            "[   15] loss: 0.141\n",
            "[   16] loss: 0.119\n",
            "[   17] loss: 0.144\n",
            "[   18] loss: 0.100\n",
            "[   19] loss: 0.243\n",
            "train loss:  0.08902517406215124\n",
            "val loss:  0.18810954689979553\n",
            "epoch:  177\n",
            "[    1] loss: 0.124\n",
            "[    2] loss: 0.092\n",
            "[    3] loss: 0.095\n",
            "[    4] loss: 0.104\n",
            "[    5] loss: 0.084\n",
            "[    6] loss: 0.118\n",
            "[    7] loss: 0.087\n",
            "[    8] loss: 0.119\n",
            "[    9] loss: 0.146\n",
            "[   10] loss: 0.068\n",
            "[   11] loss: 0.129\n",
            "[   12] loss: 0.104\n",
            "[   13] loss: 0.090\n",
            "[   14] loss: 0.151\n",
            "[   15] loss: 0.098\n",
            "[   16] loss: 0.099\n",
            "[   17] loss: 0.124\n",
            "[   18] loss: 0.094\n",
            "[   19] loss: 0.224\n",
            "epoch:  178\n",
            "[    1] loss: 0.142\n",
            "[    2] loss: 0.096\n",
            "[    3] loss: 0.096\n",
            "[    4] loss: 0.107\n",
            "[    5] loss: 0.111\n",
            "[    6] loss: 0.101\n",
            "[    7] loss: 0.073\n",
            "[    8] loss: 0.127\n",
            "[    9] loss: 0.130\n",
            "[   10] loss: 0.090\n",
            "[   11] loss: 0.147\n",
            "[   12] loss: 0.089\n",
            "[   13] loss: 0.135\n",
            "[   14] loss: 0.082\n",
            "[   15] loss: 0.120\n",
            "[   16] loss: 0.094\n",
            "[   17] loss: 0.104\n",
            "[   18] loss: 0.151\n",
            "[   19] loss: 0.197\n",
            "epoch:  179\n",
            "[    1] loss: 0.092\n",
            "[    2] loss: 0.167\n",
            "[    3] loss: 0.088\n",
            "[    4] loss: 0.098\n",
            "[    5] loss: 0.185\n",
            "[    6] loss: 0.106\n",
            "[    7] loss: 0.098\n",
            "[    8] loss: 0.093\n",
            "[    9] loss: 0.094\n",
            "[   10] loss: 0.078\n",
            "[   11] loss: 0.158\n",
            "[   12] loss: 0.100\n",
            "[   13] loss: 0.092\n",
            "[   14] loss: 0.126\n",
            "[   15] loss: 0.090\n",
            "[   16] loss: 0.155\n",
            "[   17] loss: 0.139\n",
            "[   18] loss: 0.160\n",
            "[   19] loss: 0.436\n",
            "epoch:  180\n",
            "[    1] loss: 0.098\n",
            "[    2] loss: 0.103\n",
            "[    3] loss: 0.100\n",
            "[    4] loss: 0.100\n",
            "[    5] loss: 0.095\n",
            "[    6] loss: 0.111\n",
            "[    7] loss: 0.081\n",
            "[    8] loss: 0.090\n",
            "[    9] loss: 0.073\n",
            "[   10] loss: 0.107\n",
            "[   11] loss: 0.113\n",
            "[   12] loss: 0.152\n",
            "[   13] loss: 0.125\n",
            "[   14] loss: 0.086\n",
            "[   15] loss: 0.127\n",
            "[   16] loss: 0.098\n",
            "[   17] loss: 0.100\n",
            "[   18] loss: 0.110\n",
            "[   19] loss: 0.269\n",
            "epoch:  181\n",
            "[    1] loss: 0.147\n",
            "[    2] loss: 0.246\n",
            "[    3] loss: 0.126\n",
            "[    4] loss: 0.111\n",
            "[    5] loss: 0.090\n",
            "[    6] loss: 0.121\n",
            "[    7] loss: 0.084\n",
            "[    8] loss: 0.078\n",
            "[    9] loss: 0.078\n",
            "[   10] loss: 0.100\n",
            "[   11] loss: 0.117\n",
            "[   12] loss: 0.128\n",
            "[   13] loss: 0.157\n",
            "[   14] loss: 0.130\n",
            "[   15] loss: 0.124\n",
            "[   16] loss: 0.226\n",
            "[   17] loss: 0.093\n",
            "[   18] loss: 0.135\n",
            "[   19] loss: 0.347\n",
            "train loss:  0.09841901011874571\n",
            "val loss:  0.19643485359847546\n",
            "epoch:  182\n",
            "[    1] loss: 0.106\n",
            "[    2] loss: 0.141\n",
            "[    3] loss: 0.104\n",
            "[    4] loss: 0.099\n",
            "[    5] loss: 0.127\n",
            "[    6] loss: 0.119\n",
            "[    7] loss: 0.097\n",
            "[    8] loss: 0.120\n",
            "[    9] loss: 0.141\n",
            "[   10] loss: 0.090\n",
            "[   11] loss: 0.104\n",
            "[   12] loss: 0.106\n",
            "[   13] loss: 0.122\n",
            "[   14] loss: 0.146\n",
            "[   15] loss: 0.124\n",
            "[   16] loss: 0.079\n",
            "[   17] loss: 0.086\n",
            "[   18] loss: 0.123\n",
            "[   19] loss: 0.100\n",
            "epoch:  183\n",
            "[    1] loss: 0.092\n",
            "[    2] loss: 0.120\n",
            "[    3] loss: 0.126\n",
            "[    4] loss: 0.125\n",
            "[    5] loss: 0.091\n",
            "[    6] loss: 0.112\n",
            "[    7] loss: 0.103\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.094\n",
            "[   10] loss: 0.096\n",
            "[   11] loss: 0.113\n",
            "[   12] loss: 0.113\n",
            "[   13] loss: 0.084\n",
            "[   14] loss: 0.099\n",
            "[   15] loss: 0.091\n",
            "[   16] loss: 0.088\n",
            "[   17] loss: 0.090\n",
            "[   18] loss: 0.101\n",
            "[   19] loss: 0.164\n",
            "epoch:  184\n",
            "[    1] loss: 0.089\n",
            "[    2] loss: 0.107\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.098\n",
            "[    5] loss: 0.116\n",
            "[    6] loss: 0.089\n",
            "[    7] loss: 0.124\n",
            "[    8] loss: 0.090\n",
            "[    9] loss: 0.121\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.131\n",
            "[   12] loss: 0.105\n",
            "[   13] loss: 0.088\n",
            "[   14] loss: 0.097\n",
            "[   15] loss: 0.109\n",
            "[   16] loss: 0.123\n",
            "[   17] loss: 0.099\n",
            "[   18] loss: 0.111\n",
            "[   19] loss: 0.183\n",
            "epoch:  185\n",
            "[    1] loss: 0.114\n",
            "[    2] loss: 0.095\n",
            "[    3] loss: 0.106\n",
            "[    4] loss: 0.092\n",
            "[    5] loss: 0.080\n",
            "[    6] loss: 0.151\n",
            "[    7] loss: 0.142\n",
            "[    8] loss: 0.083\n",
            "[    9] loss: 0.132\n",
            "[   10] loss: 0.084\n",
            "[   11] loss: 0.112\n",
            "[   12] loss: 0.091\n",
            "[   13] loss: 0.098\n",
            "[   14] loss: 0.127\n",
            "[   15] loss: 0.085\n",
            "[   16] loss: 0.094\n",
            "[   17] loss: 0.117\n",
            "[   18] loss: 0.106\n",
            "[   19] loss: 0.201\n",
            "epoch:  186\n",
            "[    1] loss: 0.113\n",
            "[    2] loss: 0.114\n",
            "[    3] loss: 0.103\n",
            "[    4] loss: 0.121\n",
            "[    5] loss: 0.085\n",
            "[    6] loss: 0.118\n",
            "[    7] loss: 0.103\n",
            "[    8] loss: 0.105\n",
            "[    9] loss: 0.101\n",
            "[   10] loss: 0.110\n",
            "[   11] loss: 0.166\n",
            "[   12] loss: 0.088\n",
            "[   13] loss: 0.087\n",
            "[   14] loss: 0.105\n",
            "[   15] loss: 0.110\n",
            "[   16] loss: 0.111\n",
            "[   17] loss: 0.136\n",
            "[   18] loss: 0.082\n",
            "[   19] loss: 0.237\n",
            "train loss:  0.09350193897262216\n",
            "val loss:  0.17948681116104126\n",
            "epoch:  187\n",
            "[    1] loss: 0.119\n",
            "[    2] loss: 0.101\n",
            "[    3] loss: 0.098\n",
            "[    4] loss: 0.120\n",
            "[    5] loss: 0.132\n",
            "[    6] loss: 0.126\n",
            "[    7] loss: 0.094\n",
            "[    8] loss: 0.086\n",
            "[    9] loss: 0.153\n",
            "[   10] loss: 0.108\n",
            "[   11] loss: 0.108\n",
            "[   12] loss: 0.172\n",
            "[   13] loss: 0.100\n",
            "[   14] loss: 0.101\n",
            "[   15] loss: 0.100\n",
            "[   16] loss: 0.157\n",
            "[   17] loss: 0.114\n",
            "[   18] loss: 0.100\n",
            "[   19] loss: 0.179\n",
            "epoch:  188\n",
            "[    1] loss: 0.098\n",
            "[    2] loss: 0.087\n",
            "[    3] loss: 0.097\n",
            "[    4] loss: 0.108\n",
            "[    5] loss: 0.104\n",
            "[    6] loss: 0.145\n",
            "[    7] loss: 0.072\n",
            "[    8] loss: 0.114\n",
            "[    9] loss: 0.093\n",
            "[   10] loss: 0.089\n",
            "[   11] loss: 0.116\n",
            "[   12] loss: 0.113\n",
            "[   13] loss: 0.086\n",
            "[   14] loss: 0.097\n",
            "[   15] loss: 0.186\n",
            "[   16] loss: 0.105\n",
            "[   17] loss: 0.150\n",
            "[   18] loss: 0.093\n",
            "[   19] loss: 0.457\n",
            "epoch:  189\n",
            "[    1] loss: 0.082\n",
            "[    2] loss: 0.107\n",
            "[    3] loss: 0.118\n",
            "[    4] loss: 0.132\n",
            "[    5] loss: 0.117\n",
            "[    6] loss: 0.102\n",
            "[    7] loss: 0.119\n",
            "[    8] loss: 0.085\n",
            "[    9] loss: 0.184\n",
            "[   10] loss: 0.093\n",
            "[   11] loss: 0.092\n",
            "[   12] loss: 0.152\n",
            "[   13] loss: 0.147\n",
            "[   14] loss: 0.119\n",
            "[   15] loss: 0.090\n",
            "[   16] loss: 0.089\n",
            "[   17] loss: 0.126\n",
            "[   18] loss: 0.092\n",
            "[   19] loss: 0.125\n",
            "epoch:  190\n",
            "[    1] loss: 0.107\n",
            "[    2] loss: 0.120\n",
            "[    3] loss: 0.121\n",
            "[    4] loss: 0.099\n",
            "[    5] loss: 0.124\n",
            "[    6] loss: 0.083\n",
            "[    7] loss: 0.073\n",
            "[    8] loss: 0.111\n",
            "[    9] loss: 0.120\n",
            "[   10] loss: 0.101\n",
            "[   11] loss: 0.113\n",
            "[   12] loss: 0.116\n",
            "[   13] loss: 0.099\n",
            "[   14] loss: 0.101\n",
            "[   15] loss: 0.090\n",
            "[   16] loss: 0.090\n",
            "[   17] loss: 0.101\n",
            "[   18] loss: 0.106\n",
            "[   19] loss: 0.276\n",
            "epoch:  191\n",
            "[    1] loss: 0.117\n",
            "[    2] loss: 0.141\n",
            "[    3] loss: 0.115\n",
            "[    4] loss: 0.099\n",
            "[    5] loss: 0.099\n",
            "[    6] loss: 0.134\n",
            "[    7] loss: 0.105\n",
            "[    8] loss: 0.107\n",
            "[    9] loss: 0.072\n",
            "[   10] loss: 0.116\n",
            "[   11] loss: 0.080\n",
            "[   12] loss: 0.092\n",
            "[   13] loss: 0.227\n",
            "[   14] loss: 0.092\n",
            "[   15] loss: 0.139\n",
            "[   16] loss: 0.117\n",
            "[   17] loss: 0.101\n",
            "[   18] loss: 0.081\n",
            "[   19] loss: 0.178\n",
            "train loss:  0.09526279056444764\n",
            "val loss:  0.18087927997112274\n",
            "epoch:  192\n",
            "[    1] loss: 0.119\n",
            "[    2] loss: 0.101\n",
            "[    3] loss: 0.125\n",
            "[    4] loss: 0.158\n",
            "[    5] loss: 0.132\n",
            "[    6] loss: 0.130\n",
            "[    7] loss: 0.104\n",
            "[    8] loss: 0.101\n",
            "[    9] loss: 0.110\n",
            "[   10] loss: 0.136\n",
            "[   11] loss: 0.120\n",
            "[   12] loss: 0.087\n",
            "[   13] loss: 0.096\n",
            "[   14] loss: 0.105\n",
            "[   15] loss: 0.089\n",
            "[   16] loss: 0.147\n",
            "[   17] loss: 0.103\n",
            "[   18] loss: 0.076\n",
            "[   19] loss: 0.116\n",
            "epoch:  193\n",
            "[    1] loss: 0.098\n",
            "[    2] loss: 0.093\n",
            "[    3] loss: 0.108\n",
            "[    4] loss: 0.099\n",
            "[    5] loss: 0.125\n",
            "[    6] loss: 0.108\n",
            "[    7] loss: 0.130\n",
            "[    8] loss: 0.100\n",
            "[    9] loss: 0.114\n",
            "[   10] loss: 0.109\n",
            "[   11] loss: 0.073\n",
            "[   12] loss: 0.136\n",
            "[   13] loss: 0.092\n",
            "[   14] loss: 0.096\n",
            "[   15] loss: 0.136\n",
            "[   16] loss: 0.093\n",
            "[   17] loss: 0.099\n",
            "[   18] loss: 0.075\n",
            "[   19] loss: 0.203\n",
            "epoch:  194\n",
            "[    1] loss: 0.147\n",
            "[    2] loss: 0.112\n",
            "[    3] loss: 0.103\n",
            "[    4] loss: 0.105\n",
            "[    5] loss: 0.111\n",
            "[    6] loss: 0.132\n",
            "[    7] loss: 0.114\n",
            "[    8] loss: 0.135\n",
            "[    9] loss: 0.089\n",
            "[   10] loss: 0.122\n",
            "[   11] loss: 0.115\n",
            "[   12] loss: 0.132\n",
            "[   13] loss: 0.088\n",
            "[   14] loss: 0.106\n",
            "[   15] loss: 0.107\n",
            "[   16] loss: 0.120\n",
            "[   17] loss: 0.123\n",
            "[   18] loss: 0.100\n",
            "[   19] loss: 0.323\n",
            "epoch:  195\n",
            "[    1] loss: 0.094\n",
            "[    2] loss: 0.096\n",
            "[    3] loss: 0.104\n",
            "[    4] loss: 0.096\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.083\n",
            "[    7] loss: 0.112\n",
            "[    8] loss: 0.086\n",
            "[    9] loss: 0.085\n",
            "[   10] loss: 0.108\n",
            "[   11] loss: 0.080\n",
            "[   12] loss: 0.114\n",
            "[   13] loss: 0.101\n",
            "[   14] loss: 0.096\n",
            "[   15] loss: 0.097\n",
            "[   16] loss: 0.089\n",
            "[   17] loss: 0.104\n",
            "[   18] loss: 0.074\n",
            "[   19] loss: 0.107\n",
            "epoch:  196\n",
            "[    1] loss: 0.138\n",
            "[    2] loss: 0.089\n",
            "[    3] loss: 0.121\n",
            "[    4] loss: 0.103\n",
            "[    5] loss: 0.153\n",
            "[    6] loss: 0.130\n",
            "[    7] loss: 0.099\n",
            "[    8] loss: 0.118\n",
            "[    9] loss: 0.108\n",
            "[   10] loss: 0.099\n",
            "[   11] loss: 0.116\n",
            "[   12] loss: 0.108\n",
            "[   13] loss: 0.127\n",
            "[   14] loss: 0.102\n",
            "[   15] loss: 0.133\n",
            "[   16] loss: 0.087\n",
            "[   17] loss: 0.086\n",
            "[   18] loss: 0.140\n",
            "[   19] loss: 0.155\n",
            "train loss:  0.0852773464832674\n",
            "val loss:  0.1940832193940878\n",
            "epoch:  197\n",
            "[    1] loss: 0.110\n",
            "[    2] loss: 0.088\n",
            "[    3] loss: 0.105\n",
            "[    4] loss: 0.148\n",
            "[    5] loss: 0.098\n",
            "[    6] loss: 0.125\n",
            "[    7] loss: 0.113\n",
            "[    8] loss: 0.094\n",
            "[    9] loss: 0.131\n",
            "[   10] loss: 0.088\n",
            "[   11] loss: 0.155\n",
            "[   12] loss: 0.131\n",
            "[   13] loss: 0.109\n",
            "[   14] loss: 0.103\n",
            "[   15] loss: 0.105\n",
            "[   16] loss: 0.095\n",
            "[   17] loss: 0.102\n",
            "[   18] loss: 0.121\n",
            "[   19] loss: 0.270\n",
            "epoch:  198\n",
            "[    1] loss: 0.091\n",
            "[    2] loss: 0.086\n",
            "[    3] loss: 0.084\n",
            "[    4] loss: 0.112\n",
            "[    5] loss: 0.133\n",
            "[    6] loss: 0.110\n",
            "[    7] loss: 0.143\n",
            "[    8] loss: 0.186\n",
            "[    9] loss: 0.098\n",
            "[   10] loss: 0.110\n",
            "[   11] loss: 0.083\n",
            "[   12] loss: 0.165\n",
            "[   13] loss: 0.140\n",
            "[   14] loss: 0.136\n",
            "[   15] loss: 0.107\n",
            "[   16] loss: 0.109\n",
            "[   17] loss: 0.121\n",
            "[   18] loss: 0.089\n",
            "[   19] loss: 0.230\n",
            "epoch:  199\n",
            "[    1] loss: 0.124\n",
            "[    2] loss: 0.081\n",
            "[    3] loss: 0.137\n",
            "[    4] loss: 0.086\n",
            "[    5] loss: 0.112\n",
            "[    6] loss: 0.090\n",
            "[    7] loss: 0.079\n",
            "[    8] loss: 0.168\n",
            "[    9] loss: 0.098\n",
            "[   10] loss: 0.096\n",
            "[   11] loss: 0.108\n",
            "[   12] loss: 0.097\n",
            "[   13] loss: 0.077\n",
            "[   14] loss: 0.111\n",
            "[   15] loss: 0.092\n",
            "[   16] loss: 0.074\n",
            "[   17] loss: 0.136\n",
            "[   18] loss: 0.155\n",
            "[   19] loss: 0.202\n",
            "epoch:  200\n",
            "[    1] loss: 0.148\n",
            "[    2] loss: 0.177\n",
            "[    3] loss: 0.100\n",
            "[    4] loss: 0.126\n",
            "[    5] loss: 0.117\n",
            "[    6] loss: 0.087\n",
            "[    7] loss: 0.138\n",
            "[    8] loss: 0.103\n",
            "[    9] loss: 0.130\n",
            "[   10] loss: 0.092\n",
            "[   11] loss: 0.173\n",
            "[   12] loss: 0.140\n",
            "[   13] loss: 0.084\n",
            "[   14] loss: 0.079\n",
            "[   15] loss: 0.098\n",
            "[   16] loss: 0.091\n",
            "[   17] loss: 0.105\n",
            "[   18] loss: 0.123\n",
            "[   19] loss: 0.251\n",
            "epoch:  201\n",
            "[    1] loss: 0.128\n",
            "[    2] loss: 0.086\n",
            "[    3] loss: 0.208\n",
            "[    4] loss: 0.098\n",
            "[    5] loss: 0.103\n",
            "[    6] loss: 0.115\n",
            "[    7] loss: 0.105\n",
            "[    8] loss: 0.100\n",
            "[    9] loss: 0.111\n",
            "[   10] loss: 0.111\n",
            "[   11] loss: 0.088\n",
            "[   12] loss: 0.092\n",
            "[   13] loss: 0.084\n",
            "[   14] loss: 0.162\n",
            "[   15] loss: 0.132\n",
            "[   16] loss: 0.088\n",
            "[   17] loss: 0.102\n",
            "[   18] loss: 0.117\n",
            "[   19] loss: 0.165\n",
            "train loss:  0.08604622906183496\n",
            "val loss:  0.17415424436330795\n",
            "epoch:  202\n",
            "[    1] loss: 0.144\n",
            "[    2] loss: 0.104\n",
            "[    3] loss: 0.152\n",
            "[    4] loss: 0.132\n",
            "[    5] loss: 0.114\n",
            "[    6] loss: 0.087\n",
            "[    7] loss: 0.148\n",
            "[    8] loss: 0.099\n",
            "[    9] loss: 0.167\n",
            "[   10] loss: 0.091\n",
            "[   11] loss: 0.118\n",
            "[   12] loss: 0.092\n",
            "[   13] loss: 0.076\n",
            "[   14] loss: 0.106\n",
            "[   15] loss: 0.090\n",
            "[   16] loss: 0.125\n",
            "[   17] loss: 0.108\n",
            "[   18] loss: 0.077\n",
            "[   19] loss: 0.079\n",
            "epoch:  203\n",
            "[    1] loss: 0.105\n",
            "[    2] loss: 0.115\n",
            "[    3] loss: 0.143\n",
            "[    4] loss: 0.096\n",
            "[    5] loss: 0.088\n",
            "[    6] loss: 0.094\n",
            "[    7] loss: 0.159\n",
            "[    8] loss: 0.113\n",
            "[    9] loss: 0.094\n",
            "[   10] loss: 0.131\n",
            "[   11] loss: 0.093\n",
            "[   12] loss: 0.120\n",
            "[   13] loss: 0.112\n",
            "[   14] loss: 0.138\n",
            "[   15] loss: 0.107\n",
            "[   16] loss: 0.106\n",
            "[   17] loss: 0.105\n",
            "[   18] loss: 0.099\n",
            "[   19] loss: 0.164\n",
            "epoch:  204\n",
            "[    1] loss: 0.083\n",
            "[    2] loss: 0.112\n",
            "[    3] loss: 0.090\n",
            "[    4] loss: 0.096\n",
            "[    5] loss: 0.111\n",
            "[    6] loss: 0.118\n",
            "[    7] loss: 0.107\n",
            "[    8] loss: 0.107\n",
            "[    9] loss: 0.098\n",
            "[   10] loss: 0.103\n",
            "[   11] loss: 0.091\n",
            "[   12] loss: 0.124\n",
            "[   13] loss: 0.141\n",
            "[   14] loss: 0.083\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.136\n",
            "[   17] loss: 0.162\n",
            "[   18] loss: 0.106\n",
            "[   19] loss: 0.395\n",
            "epoch:  205\n",
            "[    1] loss: 0.132\n",
            "[    2] loss: 0.160\n",
            "[    3] loss: 0.159\n",
            "[    4] loss: 0.108\n",
            "[    5] loss: 0.085\n",
            "[    6] loss: 0.083\n",
            "[    7] loss: 0.101\n",
            "[    8] loss: 0.107\n",
            "[    9] loss: 0.102\n",
            "[   10] loss: 0.130\n",
            "[   11] loss: 0.110\n",
            "[   12] loss: 0.124\n",
            "[   13] loss: 0.105\n",
            "[   14] loss: 0.074\n",
            "[   15] loss: 0.102\n",
            "[   16] loss: 0.102\n",
            "[   17] loss: 0.134\n",
            "[   18] loss: 0.129\n",
            "[   19] loss: 0.153\n",
            "epoch:  206\n",
            "[    1] loss: 0.109\n",
            "[    2] loss: 0.113\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.113\n",
            "[    5] loss: 0.093\n",
            "[    6] loss: 0.154\n",
            "[    7] loss: 0.104\n",
            "[    8] loss: 0.088\n",
            "[    9] loss: 0.128\n",
            "[   10] loss: 0.181\n",
            "[   11] loss: 0.107\n",
            "[   12] loss: 0.139\n",
            "[   13] loss: 0.112\n",
            "[   14] loss: 0.077\n",
            "[   15] loss: 0.105\n",
            "[   16] loss: 0.103\n",
            "[   17] loss: 0.113\n",
            "[   18] loss: 0.097\n",
            "[   19] loss: 0.511\n",
            "train loss:  0.09169216426637243\n",
            "val loss:  0.17426101677119732\n",
            "epoch:  207\n",
            "[    1] loss: 0.117\n",
            "[    2] loss: 0.085\n",
            "[    3] loss: 0.105\n",
            "[    4] loss: 0.126\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.111\n",
            "[    7] loss: 0.098\n",
            "[    8] loss: 0.118\n",
            "[    9] loss: 0.110\n",
            "[   10] loss: 0.084\n",
            "[   11] loss: 0.102\n",
            "[   12] loss: 0.115\n",
            "[   13] loss: 0.086\n",
            "[   14] loss: 0.073\n",
            "[   15] loss: 0.108\n",
            "[   16] loss: 0.145\n",
            "[   17] loss: 0.114\n",
            "[   18] loss: 0.098\n",
            "[   19] loss: 0.243\n",
            "epoch:  208\n",
            "[    1] loss: 0.104\n",
            "[    2] loss: 0.136\n",
            "[    3] loss: 0.091\n",
            "[    4] loss: 0.095\n",
            "[    5] loss: 0.132\n",
            "[    6] loss: 0.134\n",
            "[    7] loss: 0.150\n",
            "[    8] loss: 0.139\n",
            "[    9] loss: 0.136\n",
            "[   10] loss: 0.087\n",
            "[   11] loss: 0.087\n",
            "[   12] loss: 0.088\n",
            "[   13] loss: 0.101\n",
            "[   14] loss: 0.098\n",
            "[   15] loss: 0.086\n",
            "[   16] loss: 0.086\n",
            "[   17] loss: 0.136\n",
            "[   18] loss: 0.116\n",
            "[   19] loss: 0.195\n",
            "epoch:  209\n",
            "[    1] loss: 0.113\n",
            "[    2] loss: 0.154\n",
            "[    3] loss: 0.094\n",
            "[    4] loss: 0.073\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.100\n",
            "[    7] loss: 0.107\n",
            "[    8] loss: 0.100\n",
            "[    9] loss: 0.101\n",
            "[   10] loss: 0.086\n",
            "[   11] loss: 0.127\n",
            "[   12] loss: 0.146\n",
            "[   13] loss: 0.200\n",
            "[   14] loss: 0.086\n",
            "[   15] loss: 0.103\n",
            "[   16] loss: 0.110\n",
            "[   17] loss: 0.078\n",
            "[   18] loss: 0.089\n",
            "[   19] loss: 0.201\n",
            "epoch:  210\n",
            "[    1] loss: 0.104\n",
            "[    2] loss: 0.092\n",
            "[    3] loss: 0.122\n",
            "[    4] loss: 0.073\n",
            "[    5] loss: 0.091\n",
            "[    6] loss: 0.093\n",
            "[    7] loss: 0.088\n",
            "[    8] loss: 0.094\n",
            "[    9] loss: 0.149\n",
            "[   10] loss: 0.131\n",
            "[   11] loss: 0.122\n",
            "[   12] loss: 0.115\n",
            "[   13] loss: 0.100\n",
            "[   14] loss: 0.124\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.123\n",
            "[   17] loss: 0.135\n",
            "[   18] loss: 0.089\n",
            "[   19] loss: 0.295\n",
            "epoch:  211\n",
            "[    1] loss: 0.099\n",
            "[    2] loss: 0.092\n",
            "[    3] loss: 0.101\n",
            "[    4] loss: 0.111\n",
            "[    5] loss: 0.112\n",
            "[    6] loss: 0.084\n",
            "[    7] loss: 0.108\n",
            "[    8] loss: 0.065\n",
            "[    9] loss: 0.084\n",
            "[   10] loss: 0.161\n",
            "[   11] loss: 0.185\n",
            "[   12] loss: 0.131\n",
            "[   13] loss: 0.126\n",
            "[   14] loss: 0.102\n",
            "[   15] loss: 0.152\n",
            "[   16] loss: 0.101\n",
            "[   17] loss: 0.082\n",
            "[   18] loss: 0.082\n",
            "[   19] loss: 0.234\n",
            "train loss:  0.0915274568285574\n",
            "val loss:  0.17478013783693314\n",
            "epoch:  212\n",
            "[    1] loss: 0.099\n",
            "[    2] loss: 0.105\n",
            "[    3] loss: 0.162\n",
            "[    4] loss: 0.086\n",
            "[    5] loss: 0.104\n",
            "[    6] loss: 0.122\n",
            "[    7] loss: 0.087\n",
            "[    8] loss: 0.076\n",
            "[    9] loss: 0.122\n",
            "[   10] loss: 0.095\n",
            "[   11] loss: 0.106\n",
            "[   12] loss: 0.162\n",
            "[   13] loss: 0.099\n",
            "[   14] loss: 0.077\n",
            "[   15] loss: 0.145\n",
            "[   16] loss: 0.090\n",
            "[   17] loss: 0.102\n",
            "[   18] loss: 0.092\n",
            "[   19] loss: 0.168\n",
            "epoch:  213\n",
            "[    1] loss: 0.085\n",
            "[    2] loss: 0.096\n",
            "[    3] loss: 0.096\n",
            "[    4] loss: 0.115\n",
            "[    5] loss: 0.147\n",
            "[    6] loss: 0.121\n",
            "[    7] loss: 0.121\n",
            "[    8] loss: 0.125\n",
            "[    9] loss: 0.110\n",
            "[   10] loss: 0.099\n",
            "[   11] loss: 0.136\n",
            "[   12] loss: 0.132\n",
            "[   13] loss: 0.132\n",
            "[   14] loss: 0.103\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.093\n",
            "[   17] loss: 0.115\n",
            "[   18] loss: 0.083\n",
            "[   19] loss: 0.226\n",
            "epoch:  214\n",
            "[    1] loss: 0.095\n",
            "[    2] loss: 0.142\n",
            "[    3] loss: 0.091\n",
            "[    4] loss: 0.068\n",
            "[    5] loss: 0.089\n",
            "[    6] loss: 0.107\n",
            "[    7] loss: 0.071\n",
            "[    8] loss: 0.119\n",
            "[    9] loss: 0.126\n",
            "[   10] loss: 0.188\n",
            "[   11] loss: 0.126\n",
            "[   12] loss: 0.127\n",
            "[   13] loss: 0.098\n",
            "[   14] loss: 0.095\n",
            "[   15] loss: 0.115\n",
            "[   16] loss: 0.125\n",
            "[   17] loss: 0.088\n",
            "[   18] loss: 0.089\n",
            "[   19] loss: 0.319\n",
            "epoch:  215\n",
            "[    1] loss: 0.114\n",
            "[    2] loss: 0.100\n",
            "[    3] loss: 0.094\n",
            "[    4] loss: 0.133\n",
            "[    5] loss: 0.079\n",
            "[    6] loss: 0.079\n",
            "[    7] loss: 0.110\n",
            "[    8] loss: 0.125\n",
            "[    9] loss: 0.096\n",
            "[   10] loss: 0.125\n",
            "[   11] loss: 0.077\n",
            "[   12] loss: 0.110\n",
            "[   13] loss: 0.080\n",
            "[   14] loss: 0.120\n",
            "[   15] loss: 0.130\n",
            "[   16] loss: 0.089\n",
            "[   17] loss: 0.122\n",
            "[   18] loss: 0.131\n",
            "[   19] loss: 0.398\n",
            "epoch:  216\n",
            "[    1] loss: 0.063\n",
            "[    2] loss: 0.126\n",
            "[    3] loss: 0.113\n",
            "[    4] loss: 0.118\n",
            "[    5] loss: 0.088\n",
            "[    6] loss: 0.132\n",
            "[    7] loss: 0.091\n",
            "[    8] loss: 0.097\n",
            "[    9] loss: 0.092\n",
            "[   10] loss: 0.131\n",
            "[   11] loss: 0.103\n",
            "[   12] loss: 0.128\n",
            "[   13] loss: 0.089\n",
            "[   14] loss: 0.108\n",
            "[   15] loss: 0.081\n",
            "[   16] loss: 0.082\n",
            "[   17] loss: 0.147\n",
            "[   18] loss: 0.112\n",
            "[   19] loss: 0.109\n",
            "train loss:  0.08623341008034699\n",
            "val loss:  0.17652052640914917\n",
            "epoch:  217\n",
            "[    1] loss: 0.110\n",
            "[    2] loss: 0.082\n",
            "[    3] loss: 0.134\n",
            "[    4] loss: 0.118\n",
            "[    5] loss: 0.096\n",
            "[    6] loss: 0.107\n",
            "[    7] loss: 0.071\n",
            "[    8] loss: 0.092\n",
            "[    9] loss: 0.111\n",
            "[   10] loss: 0.107\n",
            "[   11] loss: 0.119\n",
            "[   12] loss: 0.144\n",
            "[   13] loss: 0.123\n",
            "[   14] loss: 0.091\n",
            "[   15] loss: 0.116\n",
            "[   16] loss: 0.119\n",
            "[   17] loss: 0.134\n",
            "[   18] loss: 0.106\n",
            "[   19] loss: 0.343\n",
            "epoch:  218\n",
            "[    1] loss: 0.112\n",
            "[    2] loss: 0.104\n",
            "[    3] loss: 0.072\n",
            "[    4] loss: 0.090\n",
            "[    5] loss: 0.100\n",
            "[    6] loss: 0.103\n",
            "[    7] loss: 0.122\n",
            "[    8] loss: 0.127\n",
            "[    9] loss: 0.074\n",
            "[   10] loss: 0.121\n",
            "[   11] loss: 0.109\n",
            "[   12] loss: 0.091\n",
            "[   13] loss: 0.126\n",
            "[   14] loss: 0.100\n",
            "[   15] loss: 0.096\n",
            "[   16] loss: 0.114\n",
            "[   17] loss: 0.142\n",
            "[   18] loss: 0.116\n",
            "[   19] loss: 0.191\n",
            "epoch:  219\n",
            "[    1] loss: 0.145\n",
            "[    2] loss: 0.106\n",
            "[    3] loss: 0.108\n",
            "[    4] loss: 0.089\n",
            "[    5] loss: 0.115\n",
            "[    6] loss: 0.123\n",
            "[    7] loss: 0.082\n",
            "[    8] loss: 0.083\n",
            "[    9] loss: 0.126\n",
            "[   10] loss: 0.095\n",
            "[   11] loss: 0.110\n",
            "[   12] loss: 0.150\n",
            "[   13] loss: 0.116\n",
            "[   14] loss: 0.104\n",
            "[   15] loss: 0.107\n",
            "[   16] loss: 0.094\n",
            "[   17] loss: 0.079\n",
            "[   18] loss: 0.103\n",
            "[   19] loss: 0.195\n",
            "epoch:  220\n",
            "[    1] loss: 0.125\n",
            "[    2] loss: 0.111\n",
            "[    3] loss: 0.134\n",
            "[    4] loss: 0.123\n",
            "[    5] loss: 0.080\n",
            "[    6] loss: 0.100\n",
            "[    7] loss: 0.095\n",
            "[    8] loss: 0.098\n",
            "[    9] loss: 0.087\n",
            "[   10] loss: 0.090\n",
            "[   11] loss: 0.078\n",
            "[   12] loss: 0.120\n",
            "[   13] loss: 0.138\n",
            "[   14] loss: 0.074\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.086\n",
            "[   17] loss: 0.087\n",
            "[   18] loss: 0.125\n",
            "[   19] loss: 0.272\n",
            "epoch:  221\n",
            "[    1] loss: 0.102\n",
            "[    2] loss: 0.100\n",
            "[    3] loss: 0.101\n",
            "[    4] loss: 0.165\n",
            "[    5] loss: 0.127\n",
            "[    6] loss: 0.129\n",
            "[    7] loss: 0.094\n",
            "[    8] loss: 0.131\n",
            "[    9] loss: 0.103\n",
            "[   10] loss: 0.085\n",
            "[   11] loss: 0.098\n",
            "[   12] loss: 0.117\n",
            "[   13] loss: 0.107\n",
            "[   14] loss: 0.123\n",
            "[   15] loss: 0.124\n",
            "[   16] loss: 0.081\n",
            "[   17] loss: 0.103\n",
            "[   18] loss: 0.127\n",
            "[   19] loss: 0.096\n",
            "train loss:  0.0867500494727317\n",
            "val loss:  0.17544939927756786\n",
            "epoch:  222\n",
            "[    1] loss: 0.122\n",
            "[    2] loss: 0.154\n",
            "[    3] loss: 0.162\n",
            "[    4] loss: 0.085\n",
            "[    5] loss: 0.130\n",
            "[    6] loss: 0.132\n",
            "[    7] loss: 0.088\n",
            "[    8] loss: 0.159\n",
            "[    9] loss: 0.086\n",
            "[   10] loss: 0.100\n",
            "[   11] loss: 0.118\n",
            "[   12] loss: 0.107\n",
            "[   13] loss: 0.118\n",
            "[   14] loss: 0.154\n",
            "[   15] loss: 0.108\n",
            "[   16] loss: 0.071\n",
            "[   17] loss: 0.099\n",
            "[   18] loss: 0.173\n",
            "[   19] loss: 0.202\n",
            "epoch:  223\n",
            "[    1] loss: 0.107\n",
            "[    2] loss: 0.104\n",
            "[    3] loss: 0.081\n",
            "[    4] loss: 0.077\n",
            "[    5] loss: 0.170\n",
            "[    6] loss: 0.098\n",
            "[    7] loss: 0.107\n",
            "[    8] loss: 0.111\n",
            "[    9] loss: 0.115\n",
            "[   10] loss: 0.093\n",
            "[   11] loss: 0.084\n",
            "[   12] loss: 0.112\n",
            "[   13] loss: 0.090\n",
            "[   14] loss: 0.098\n",
            "[   15] loss: 0.119\n",
            "[   16] loss: 0.112\n",
            "[   17] loss: 0.099\n",
            "[   18] loss: 0.092\n",
            "[   19] loss: 0.188\n",
            "epoch:  224\n",
            "[    1] loss: 0.114\n",
            "[    2] loss: 0.100\n",
            "[    3] loss: 0.103\n",
            "[    4] loss: 0.122\n",
            "[    5] loss: 0.103\n",
            "[    6] loss: 0.126\n",
            "[    7] loss: 0.116\n",
            "[    8] loss: 0.122\n",
            "[    9] loss: 0.082\n",
            "[   10] loss: 0.110\n",
            "[   11] loss: 0.123\n",
            "[   12] loss: 0.102\n",
            "[   13] loss: 0.130\n",
            "[   14] loss: 0.082\n",
            "[   15] loss: 0.130\n",
            "[   16] loss: 0.096\n",
            "[   17] loss: 0.097\n",
            "[   18] loss: 0.104\n",
            "[   19] loss: 0.176\n",
            "epoch:  225\n",
            "[    1] loss: 0.076\n",
            "[    2] loss: 0.085\n",
            "[    3] loss: 0.090\n",
            "[    4] loss: 0.097\n",
            "[    5] loss: 0.138\n",
            "[    6] loss: 0.075\n",
            "[    7] loss: 0.071\n",
            "[    8] loss: 0.108\n",
            "[    9] loss: 0.137\n",
            "[   10] loss: 0.134\n",
            "[   11] loss: 0.151\n",
            "[   12] loss: 0.100\n",
            "[   13] loss: 0.118\n",
            "[   14] loss: 0.088\n",
            "[   15] loss: 0.156\n",
            "[   16] loss: 0.182\n",
            "[   17] loss: 0.096\n",
            "[   18] loss: 0.097\n",
            "[   19] loss: 0.576\n",
            "epoch:  226\n",
            "[    1] loss: 0.120\n",
            "[    2] loss: 0.120\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.093\n",
            "[    5] loss: 0.150\n",
            "[    6] loss: 0.102\n",
            "[    7] loss: 0.146\n",
            "[    8] loss: 0.102\n",
            "[    9] loss: 0.072\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.107\n",
            "[   12] loss: 0.163\n",
            "[   13] loss: 0.091\n",
            "[   14] loss: 0.151\n",
            "[   15] loss: 0.171\n",
            "[   16] loss: 0.097\n",
            "[   17] loss: 0.104\n",
            "[   18] loss: 0.124\n",
            "[   19] loss: 0.357\n",
            "train loss:  0.09194213182062787\n",
            "val loss:  0.180328831076622\n",
            "epoch:  227\n",
            "[    1] loss: 0.112\n",
            "[    2] loss: 0.110\n",
            "[    3] loss: 0.091\n",
            "[    4] loss: 0.141\n",
            "[    5] loss: 0.107\n",
            "[    6] loss: 0.091\n",
            "[    7] loss: 0.083\n",
            "[    8] loss: 0.106\n",
            "[    9] loss: 0.112\n",
            "[   10] loss: 0.131\n",
            "[   11] loss: 0.105\n",
            "[   12] loss: 0.121\n",
            "[   13] loss: 0.097\n",
            "[   14] loss: 0.099\n",
            "[   15] loss: 0.105\n",
            "[   16] loss: 0.131\n",
            "[   17] loss: 0.102\n",
            "[   18] loss: 0.083\n",
            "[   19] loss: 0.140\n",
            "epoch:  228\n",
            "[    1] loss: 0.117\n",
            "[    2] loss: 0.088\n",
            "[    3] loss: 0.180\n",
            "[    4] loss: 0.092\n",
            "[    5] loss: 0.091\n",
            "[    6] loss: 0.082\n",
            "[    7] loss: 0.127\n",
            "[    8] loss: 0.085\n",
            "[    9] loss: 0.101\n",
            "[   10] loss: 0.097\n",
            "[   11] loss: 0.120\n",
            "[   12] loss: 0.098\n",
            "[   13] loss: 0.102\n",
            "[   14] loss: 0.126\n",
            "[   15] loss: 0.146\n",
            "[   16] loss: 0.086\n",
            "[   17] loss: 0.116\n",
            "[   18] loss: 0.091\n",
            "[   19] loss: 0.287\n",
            "epoch:  229\n",
            "[    1] loss: 0.121\n",
            "[    2] loss: 0.073\n",
            "[    3] loss: 0.131\n",
            "[    4] loss: 0.117\n",
            "[    5] loss: 0.114\n",
            "[    6] loss: 0.125\n",
            "[    7] loss: 0.124\n",
            "[    8] loss: 0.113\n",
            "[    9] loss: 0.104\n",
            "[   10] loss: 0.097\n",
            "[   11] loss: 0.082\n",
            "[   12] loss: 0.104\n",
            "[   13] loss: 0.100\n",
            "[   14] loss: 0.098\n",
            "[   15] loss: 0.098\n",
            "[   16] loss: 0.130\n",
            "[   17] loss: 0.093\n",
            "[   18] loss: 0.114\n",
            "[   19] loss: 0.372\n",
            "epoch:  230\n",
            "[    1] loss: 0.177\n",
            "[    2] loss: 0.086\n",
            "[    3] loss: 0.109\n",
            "[    4] loss: 0.083\n",
            "[    5] loss: 0.084\n",
            "[    6] loss: 0.113\n",
            "[    7] loss: 0.095\n",
            "[    8] loss: 0.088\n",
            "[    9] loss: 0.121\n",
            "[   10] loss: 0.082\n",
            "[   11] loss: 0.165\n",
            "[   12] loss: 0.134\n",
            "[   13] loss: 0.102\n",
            "[   14] loss: 0.093\n",
            "[   15] loss: 0.153\n",
            "[   16] loss: 0.105\n",
            "[   17] loss: 0.112\n",
            "[   18] loss: 0.112\n",
            "[   19] loss: 0.250\n",
            "epoch:  231\n",
            "[    1] loss: 0.113\n",
            "[    2] loss: 0.142\n",
            "[    3] loss: 0.098\n",
            "[    4] loss: 0.109\n",
            "[    5] loss: 0.187\n",
            "[    6] loss: 0.156\n",
            "[    7] loss: 0.130\n",
            "[    8] loss: 0.157\n",
            "[    9] loss: 0.106\n",
            "[   10] loss: 0.075\n",
            "[   11] loss: 0.176\n",
            "[   12] loss: 0.160\n",
            "[   13] loss: 0.106\n",
            "[   14] loss: 0.079\n",
            "[   15] loss: 0.102\n",
            "[   16] loss: 0.092\n",
            "[   17] loss: 0.097\n",
            "[   18] loss: 0.129\n",
            "[   19] loss: 0.258\n",
            "train loss:  0.08675849914331646\n",
            "val loss:  0.1725298147648573\n",
            "epoch:  232\n",
            "[    1] loss: 0.087\n",
            "[    2] loss: 0.097\n",
            "[    3] loss: 0.087\n",
            "[    4] loss: 0.079\n",
            "[    5] loss: 0.115\n",
            "[    6] loss: 0.103\n",
            "[    7] loss: 0.097\n",
            "[    8] loss: 0.124\n",
            "[    9] loss: 0.147\n",
            "[   10] loss: 0.135\n",
            "[   11] loss: 0.081\n",
            "[   12] loss: 0.110\n",
            "[   13] loss: 0.105\n",
            "[   14] loss: 0.090\n",
            "[   15] loss: 0.120\n",
            "[   16] loss: 0.089\n",
            "[   17] loss: 0.107\n",
            "[   18] loss: 0.135\n",
            "[   19] loss: 0.375\n",
            "epoch:  233\n",
            "[    1] loss: 0.090\n",
            "[    2] loss: 0.098\n",
            "[    3] loss: 0.097\n",
            "[    4] loss: 0.073\n",
            "[    5] loss: 0.107\n",
            "[    6] loss: 0.102\n",
            "[    7] loss: 0.112\n",
            "[    8] loss: 0.135\n",
            "[    9] loss: 0.080\n",
            "[   10] loss: 0.119\n",
            "[   11] loss: 0.108\n",
            "[   12] loss: 0.100\n",
            "[   13] loss: 0.095\n",
            "[   14] loss: 0.114\n",
            "[   15] loss: 0.121\n",
            "[   16] loss: 0.110\n",
            "[   17] loss: 0.134\n",
            "[   18] loss: 0.155\n",
            "[   19] loss: 0.099\n",
            "epoch:  234\n",
            "[    1] loss: 0.097\n",
            "[    2] loss: 0.121\n",
            "[    3] loss: 0.098\n",
            "[    4] loss: 0.106\n",
            "[    5] loss: 0.097\n",
            "[    6] loss: 0.099\n",
            "[    7] loss: 0.113\n",
            "[    8] loss: 0.101\n",
            "[    9] loss: 0.099\n",
            "[   10] loss: 0.090\n",
            "[   11] loss: 0.130\n",
            "[   12] loss: 0.099\n",
            "[   13] loss: 0.116\n",
            "[   14] loss: 0.124\n",
            "[   15] loss: 0.092\n",
            "[   16] loss: 0.099\n",
            "[   17] loss: 0.111\n",
            "[   18] loss: 0.091\n",
            "[   19] loss: 0.169\n",
            "epoch:  235\n",
            "[    1] loss: 0.141\n",
            "[    2] loss: 0.075\n",
            "[    3] loss: 0.121\n",
            "[    4] loss: 0.123\n",
            "[    5] loss: 0.106\n",
            "[    6] loss: 0.140\n",
            "[    7] loss: 0.121\n",
            "[    8] loss: 0.111\n",
            "[    9] loss: 0.098\n",
            "[   10] loss: 0.143\n",
            "[   11] loss: 0.102\n",
            "[   12] loss: 0.092\n",
            "[   13] loss: 0.152\n",
            "[   14] loss: 0.080\n",
            "[   15] loss: 0.086\n",
            "[   16] loss: 0.132\n",
            "[   17] loss: 0.092\n",
            "[   18] loss: 0.071\n",
            "[   19] loss: 0.464\n",
            "epoch:  236\n",
            "[    1] loss: 0.113\n",
            "[    2] loss: 0.100\n",
            "[    3] loss: 0.073\n",
            "[    4] loss: 0.089\n",
            "[    5] loss: 0.127\n",
            "[    6] loss: 0.161\n",
            "[    7] loss: 0.155\n",
            "[    8] loss: 0.115\n",
            "[    9] loss: 0.100\n",
            "[   10] loss: 0.108\n",
            "[   11] loss: 0.130\n",
            "[   12] loss: 0.111\n",
            "[   13] loss: 0.098\n",
            "[   14] loss: 0.092\n",
            "[   15] loss: 0.100\n",
            "[   16] loss: 0.083\n",
            "[   17] loss: 0.098\n",
            "[   18] loss: 0.090\n",
            "[   19] loss: 0.178\n",
            "train loss:  0.08791239326819777\n",
            "val loss:  0.19862059876322746\n",
            "epoch:  237\n",
            "[    1] loss: 0.173\n",
            "[    2] loss: 0.110\n",
            "[    3] loss: 0.104\n",
            "[    4] loss: 0.101\n",
            "[    5] loss: 0.119\n",
            "[    6] loss: 0.093\n",
            "[    7] loss: 0.074\n",
            "[    8] loss: 0.168\n",
            "[    9] loss: 0.112\n",
            "[   10] loss: 0.104\n",
            "[   11] loss: 0.083\n",
            "[   12] loss: 0.089\n",
            "[   13] loss: 0.112\n",
            "[   14] loss: 0.094\n",
            "[   15] loss: 0.076\n",
            "[   16] loss: 0.100\n",
            "[   17] loss: 0.078\n",
            "[   18] loss: 0.130\n",
            "[   19] loss: 0.157\n",
            "epoch:  238\n",
            "[    1] loss: 0.074\n",
            "[    2] loss: 0.120\n",
            "[    3] loss: 0.111\n",
            "[    4] loss: 0.105\n",
            "[    5] loss: 0.140\n",
            "[    6] loss: 0.125\n",
            "[    7] loss: 0.162\n",
            "[    8] loss: 0.088\n",
            "[    9] loss: 0.084\n",
            "[   10] loss: 0.121\n",
            "[   11] loss: 0.088\n",
            "[   12] loss: 0.106\n",
            "[   13] loss: 0.096\n",
            "[   14] loss: 0.145\n",
            "[   15] loss: 0.128\n",
            "[   16] loss: 0.100\n",
            "[   17] loss: 0.076\n",
            "[   18] loss: 0.088\n",
            "[   19] loss: 0.224\n",
            "epoch:  239\n",
            "[    1] loss: 0.094\n",
            "[    2] loss: 0.101\n",
            "[    3] loss: 0.082\n",
            "[    4] loss: 0.139\n",
            "[    5] loss: 0.167\n",
            "[    6] loss: 0.094\n",
            "[    7] loss: 0.143\n",
            "[    8] loss: 0.167\n",
            "[    9] loss: 0.093\n",
            "[   10] loss: 0.159\n",
            "[   11] loss: 0.095\n",
            "[   12] loss: 0.082\n",
            "[   13] loss: 0.238\n",
            "[   14] loss: 0.116\n",
            "[   15] loss: 0.114\n",
            "[   16] loss: 0.081\n",
            "[   17] loss: 0.089\n",
            "[   18] loss: 0.157\n",
            "[   19] loss: 0.273\n",
            "epoch:  240\n",
            "[    1] loss: 0.079\n",
            "[    2] loss: 0.127\n",
            "[    3] loss: 0.121\n",
            "[    4] loss: 0.095\n",
            "[    5] loss: 0.102\n",
            "[    6] loss: 0.086\n",
            "[    7] loss: 0.120\n",
            "[    8] loss: 0.151\n",
            "[    9] loss: 0.126\n",
            "[   10] loss: 0.077\n",
            "[   11] loss: 0.085\n",
            "[   12] loss: 0.105\n",
            "[   13] loss: 0.109\n",
            "[   14] loss: 0.098\n",
            "[   15] loss: 0.105\n",
            "[   16] loss: 0.108\n",
            "[   17] loss: 0.109\n",
            "[   18] loss: 0.119\n",
            "[   19] loss: 0.138\n",
            "epoch:  241\n",
            "[    1] loss: 0.091\n",
            "[    2] loss: 0.153\n",
            "[    3] loss: 0.126\n",
            "[    4] loss: 0.104\n",
            "[    5] loss: 0.083\n",
            "[    6] loss: 0.081\n",
            "[    7] loss: 0.126\n",
            "[    8] loss: 0.111\n",
            "[    9] loss: 0.114\n",
            "[   10] loss: 0.143\n",
            "[   11] loss: 0.098\n",
            "[   12] loss: 0.121\n",
            "[   13] loss: 0.099\n",
            "[   14] loss: 0.087\n",
            "[   15] loss: 0.147\n",
            "[   16] loss: 0.137\n",
            "[   17] loss: 0.097\n",
            "[   18] loss: 0.118\n",
            "[   19] loss: 0.270\n",
            "train loss:  0.09012475730303456\n",
            "val loss:  0.158513268455863\n",
            "best epoch at  241\n",
            "epoch:  242\n",
            "[    1] loss: 0.116\n",
            "[    2] loss: 0.105\n",
            "[    3] loss: 0.113\n",
            "[    4] loss: 0.117\n",
            "[    5] loss: 0.085\n",
            "[    6] loss: 0.104\n",
            "[    7] loss: 0.076\n",
            "[    8] loss: 0.093\n",
            "[    9] loss: 0.104\n",
            "[   10] loss: 0.107\n",
            "[   11] loss: 0.135\n",
            "[   12] loss: 0.101\n",
            "[   13] loss: 0.095\n",
            "[   14] loss: 0.114\n",
            "[   15] loss: 0.116\n",
            "[   16] loss: 0.187\n",
            "[   17] loss: 0.112\n",
            "[   18] loss: 0.080\n",
            "[   19] loss: 0.314\n",
            "epoch:  243\n",
            "[    1] loss: 0.145\n",
            "[    2] loss: 0.101\n",
            "[    3] loss: 0.177\n",
            "[    4] loss: 0.079\n",
            "[    5] loss: 0.083\n",
            "[    6] loss: 0.163\n",
            "[    7] loss: 0.076\n",
            "[    8] loss: 0.102\n",
            "[    9] loss: 0.145\n",
            "[   10] loss: 0.090\n",
            "[   11] loss: 0.127\n",
            "[   12] loss: 0.103\n",
            "[   13] loss: 0.130\n",
            "[   14] loss: 0.111\n",
            "[   15] loss: 0.092\n",
            "[   16] loss: 0.110\n",
            "[   17] loss: 0.142\n",
            "[   18] loss: 0.144\n",
            "[   19] loss: 0.113\n",
            "epoch:  244\n",
            "[    1] loss: 0.111\n",
            "[    2] loss: 0.118\n",
            "[    3] loss: 0.165\n",
            "[    4] loss: 0.097\n",
            "[    5] loss: 0.144\n",
            "[    6] loss: 0.146\n",
            "[    7] loss: 0.081\n",
            "[    8] loss: 0.075\n",
            "[    9] loss: 0.107\n",
            "[   10] loss: 0.093\n",
            "[   11] loss: 0.091\n",
            "[   12] loss: 0.099\n",
            "[   13] loss: 0.094\n",
            "[   14] loss: 0.123\n",
            "[   15] loss: 0.099\n",
            "[   16] loss: 0.156\n",
            "[   17] loss: 0.089\n",
            "[   18] loss: 0.117\n",
            "[   19] loss: 0.246\n",
            "epoch:  245\n",
            "[    1] loss: 0.112\n",
            "[    2] loss: 0.086\n",
            "[    3] loss: 0.097\n",
            "[    4] loss: 0.147\n",
            "[    5] loss: 0.088\n",
            "[    6] loss: 0.118\n",
            "[    7] loss: 0.084\n",
            "[    8] loss: 0.082\n",
            "[    9] loss: 0.097\n",
            "[   10] loss: 0.140\n",
            "[   11] loss: 0.141\n",
            "[   12] loss: 0.128\n",
            "[   13] loss: 0.088\n",
            "[   14] loss: 0.120\n",
            "[   15] loss: 0.070\n",
            "[   16] loss: 0.122\n",
            "[   17] loss: 0.124\n",
            "[   18] loss: 0.096\n",
            "[   19] loss: 0.398\n",
            "epoch:  246\n",
            "[    1] loss: 0.087\n",
            "[    2] loss: 0.094\n",
            "[    3] loss: 0.158\n",
            "[    4] loss: 0.086\n",
            "[    5] loss: 0.129\n",
            "[    6] loss: 0.112\n",
            "[    7] loss: 0.106\n",
            "[    8] loss: 0.085\n",
            "[    9] loss: 0.101\n",
            "[   10] loss: 0.136\n",
            "[   11] loss: 0.079\n",
            "[   12] loss: 0.083\n",
            "[   13] loss: 0.122\n",
            "[   14] loss: 0.106\n",
            "[   15] loss: 0.130\n",
            "[   16] loss: 0.097\n",
            "[   17] loss: 0.090\n",
            "[   18] loss: 0.090\n",
            "[   19] loss: 0.109\n",
            "train loss:  0.09246425848344669\n",
            "val loss:  0.17921606451272964\n",
            "epoch:  247\n",
            "[    1] loss: 0.102\n",
            "[    2] loss: 0.095\n",
            "[    3] loss: 0.091\n",
            "[    4] loss: 0.101\n",
            "[    5] loss: 0.094\n",
            "[    6] loss: 0.107\n",
            "[    7] loss: 0.087\n",
            "[    8] loss: 0.106\n",
            "[    9] loss: 0.177\n",
            "[   10] loss: 0.124\n",
            "[   11] loss: 0.111\n",
            "[   12] loss: 0.107\n",
            "[   13] loss: 0.134\n",
            "[   14] loss: 0.094\n",
            "[   15] loss: 0.108\n",
            "[   16] loss: 0.145\n",
            "[   17] loss: 0.106\n",
            "[   18] loss: 0.117\n",
            "[   19] loss: 0.253\n",
            "epoch:  248\n",
            "[    1] loss: 0.105\n",
            "[    2] loss: 0.132\n",
            "[    3] loss: 0.076\n",
            "[    4] loss: 0.120\n",
            "[    5] loss: 0.128\n",
            "[    6] loss: 0.147\n",
            "[    7] loss: 0.137\n",
            "[    8] loss: 0.137\n",
            "[    9] loss: 0.090\n",
            "[   10] loss: 0.167\n",
            "[   11] loss: 0.097\n",
            "[   12] loss: 0.096\n",
            "[   13] loss: 0.115\n",
            "[   14] loss: 0.124\n",
            "[   15] loss: 0.095\n",
            "[   16] loss: 0.097\n",
            "[   17] loss: 0.101\n",
            "[   18] loss: 0.124\n",
            "[   19] loss: 0.282\n",
            "epoch:  249\n",
            "[    1] loss: 0.098\n",
            "[    2] loss: 0.117\n",
            "[    3] loss: 0.127\n",
            "[    4] loss: 0.159\n",
            "[    5] loss: 0.096\n",
            "[    6] loss: 0.137\n",
            "[    7] loss: 0.132\n",
            "[    8] loss: 0.117\n",
            "[    9] loss: 0.095\n",
            "[   10] loss: 0.137\n",
            "[   11] loss: 0.119\n",
            "[   12] loss: 0.131\n",
            "[   13] loss: 0.129\n",
            "[   14] loss: 0.109\n",
            "[   15] loss: 0.150\n",
            "[   16] loss: 0.109\n",
            "[   17] loss: 0.072\n",
            "[   18] loss: 0.083\n",
            "[   19] loss: 0.690\n",
            "epoch:  250\n",
            "[    1] loss: 0.137\n",
            "[    2] loss: 0.092\n",
            "[    3] loss: 0.124\n",
            "[    4] loss: 0.147\n",
            "[    5] loss: 0.158\n",
            "[    6] loss: 0.094\n",
            "[    7] loss: 0.088\n",
            "[    8] loss: 0.107\n",
            "[    9] loss: 0.100\n",
            "[   10] loss: 0.112\n",
            "[   11] loss: 0.086\n",
            "[   12] loss: 0.097\n",
            "[   13] loss: 0.074\n",
            "[   14] loss: 0.093\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.072\n",
            "[   17] loss: 0.102\n",
            "[   18] loss: 0.081\n",
            "[   19] loss: 0.235\n",
            "epoch:  251\n",
            "[    1] loss: 0.075\n",
            "[    2] loss: 0.095\n",
            "[    3] loss: 0.086\n",
            "[    4] loss: 0.108\n",
            "[    5] loss: 0.090\n",
            "[    6] loss: 0.119\n",
            "[    7] loss: 0.114\n",
            "[    8] loss: 0.083\n",
            "[    9] loss: 0.121\n",
            "[   10] loss: 0.097\n",
            "[   11] loss: 0.127\n",
            "[   12] loss: 0.089\n",
            "[   13] loss: 0.143\n",
            "[   14] loss: 0.093\n",
            "[   15] loss: 0.109\n",
            "[   16] loss: 0.105\n",
            "[   17] loss: 0.106\n",
            "[   18] loss: 0.105\n",
            "[   19] loss: 0.204\n",
            "train loss:  0.08858017391963478\n",
            "val loss:  0.16932971961796284\n",
            "epoch:  252\n",
            "[    1] loss: 0.106\n",
            "[    2] loss: 0.133\n",
            "[    3] loss: 0.112\n",
            "[    4] loss: 0.107\n",
            "[    5] loss: 0.096\n",
            "[    6] loss: 0.071\n",
            "[    7] loss: 0.100\n",
            "[    8] loss: 0.099\n",
            "[    9] loss: 0.086\n",
            "[   10] loss: 0.093\n",
            "[   11] loss: 0.096\n",
            "[   12] loss: 0.104\n",
            "[   13] loss: 0.110\n",
            "[   14] loss: 0.129\n",
            "[   15] loss: 0.130\n",
            "[   16] loss: 0.093\n",
            "[   17] loss: 0.118\n",
            "[   18] loss: 0.123\n",
            "[   19] loss: 0.363\n",
            "epoch:  253\n",
            "[    1] loss: 0.092\n",
            "[    2] loss: 0.097\n",
            "[    3] loss: 0.107\n",
            "[    4] loss: 0.120\n",
            "[    5] loss: 0.118\n",
            "[    6] loss: 0.112\n",
            "[    7] loss: 0.102\n",
            "[    8] loss: 0.103\n",
            "[    9] loss: 0.164\n",
            "[   10] loss: 0.079\n",
            "[   11] loss: 0.122\n",
            "[   12] loss: 0.111\n",
            "[   13] loss: 0.127\n",
            "[   14] loss: 0.096\n",
            "[   15] loss: 0.175\n",
            "[   16] loss: 0.138\n",
            "[   17] loss: 0.095\n",
            "[   18] loss: 0.138\n",
            "[   19] loss: 0.460\n",
            "epoch:  254\n",
            "[    1] loss: 0.085\n",
            "[    2] loss: 0.111\n",
            "[    3] loss: 0.072\n",
            "[    4] loss: 0.126\n",
            "[    5] loss: 0.097\n",
            "[    6] loss: 0.118\n",
            "[    7] loss: 0.089\n",
            "[    8] loss: 0.119\n",
            "[    9] loss: 0.099\n",
            "[   10] loss: 0.120\n",
            "[   11] loss: 0.114\n",
            "[   12] loss: 0.107\n",
            "[   13] loss: 0.185\n",
            "[   14] loss: 0.067\n",
            "[   15] loss: 0.094\n",
            "[   16] loss: 0.093\n",
            "[   17] loss: 0.113\n",
            "[   18] loss: 0.078\n",
            "[   19] loss: 0.103\n",
            "epoch:  255\n",
            "[    1] loss: 0.106\n",
            "[    2] loss: 0.113\n",
            "[    3] loss: 0.077\n",
            "[    4] loss: 0.108\n",
            "[    5] loss: 0.102\n",
            "[    6] loss: 0.129\n",
            "[    7] loss: 0.108\n",
            "[    8] loss: 0.105\n",
            "[    9] loss: 0.114\n",
            "[   10] loss: 0.106\n",
            "[   11] loss: 0.094\n",
            "[   12] loss: 0.085\n",
            "[   13] loss: 0.124\n",
            "[   14] loss: 0.091\n",
            "[   15] loss: 0.277\n",
            "[   16] loss: 0.126\n",
            "[   17] loss: 0.102\n",
            "[   18] loss: 0.145\n",
            "[   19] loss: 0.137\n",
            "epoch:  256\n",
            "[    1] loss: 0.107\n",
            "[    2] loss: 0.088\n",
            "[    3] loss: 0.100\n",
            "[    4] loss: 0.169\n",
            "[    5] loss: 0.096\n",
            "[    6] loss: 0.083\n",
            "[    7] loss: 0.096\n",
            "[    8] loss: 0.091\n",
            "[    9] loss: 0.100\n",
            "[   10] loss: 0.119\n",
            "[   11] loss: 0.120\n",
            "[   12] loss: 0.101\n",
            "[   13] loss: 0.101\n",
            "[   14] loss: 0.089\n",
            "[   15] loss: 0.094\n",
            "[   16] loss: 0.122\n",
            "[   17] loss: 0.094\n",
            "[   18] loss: 0.098\n",
            "[   19] loss: 0.159\n",
            "train loss:  0.08786284419543602\n",
            "val loss:  0.18017838150262833\n",
            "epoch:  257\n",
            "[    1] loss: 0.119\n",
            "[    2] loss: 0.084\n",
            "[    3] loss: 0.109\n",
            "[    4] loss: 0.106\n",
            "[    5] loss: 0.099\n",
            "[    6] loss: 0.097\n",
            "[    7] loss: 0.108\n",
            "[    8] loss: 0.133\n",
            "[    9] loss: 0.092\n",
            "[   10] loss: 0.109\n",
            "[   11] loss: 0.122\n",
            "[   12] loss: 0.104\n",
            "[   13] loss: 0.097\n",
            "[   14] loss: 0.099\n",
            "[   15] loss: 0.113\n",
            "[   16] loss: 0.116\n",
            "[   17] loss: 0.095\n",
            "[   18] loss: 0.122\n",
            "[   19] loss: 0.135\n",
            "epoch:  258\n",
            "[    1] loss: 0.090\n",
            "[    2] loss: 0.140\n",
            "[    3] loss: 0.139\n",
            "[    4] loss: 0.078\n",
            "[    5] loss: 0.083\n",
            "[    6] loss: 0.101\n",
            "[    7] loss: 0.087\n",
            "[    8] loss: 0.115\n",
            "[    9] loss: 0.128\n",
            "[   10] loss: 0.112\n",
            "[   11] loss: 0.105\n",
            "[   12] loss: 0.076\n",
            "[   13] loss: 0.100\n",
            "[   14] loss: 0.084\n",
            "[   15] loss: 0.123\n",
            "[   16] loss: 0.134\n",
            "[   17] loss: 0.128\n",
            "[   18] loss: 0.119\n",
            "[   19] loss: 0.386\n",
            "epoch:  259\n",
            "[    1] loss: 0.190\n",
            "[    2] loss: 0.099\n",
            "[    3] loss: 0.137\n",
            "[    4] loss: 0.072\n",
            "[    5] loss: 0.136\n",
            "[    6] loss: 0.135\n",
            "[    7] loss: 0.109\n",
            "[    8] loss: 0.149\n",
            "[    9] loss: 0.120\n",
            "[   10] loss: 0.118\n",
            "[   11] loss: 0.082\n",
            "[   12] loss: 0.081\n",
            "[   13] loss: 0.143\n",
            "[   14] loss: 0.137\n",
            "[   15] loss: 0.122\n",
            "[   16] loss: 0.096\n",
            "[   17] loss: 0.114\n",
            "[   18] loss: 0.104\n",
            "[   19] loss: 0.248\n",
            "epoch:  260\n",
            "[    1] loss: 0.093\n",
            "[    2] loss: 0.101\n",
            "[    3] loss: 0.109\n",
            "[    4] loss: 0.109\n",
            "[    5] loss: 0.150\n",
            "[    6] loss: 0.089\n",
            "[    7] loss: 0.085\n",
            "[    8] loss: 0.088\n",
            "[    9] loss: 0.095\n",
            "[   10] loss: 0.079\n",
            "[   11] loss: 0.112\n",
            "[   12] loss: 0.135\n",
            "[   13] loss: 0.101\n",
            "[   14] loss: 0.093\n",
            "[   15] loss: 0.108\n",
            "[   16] loss: 0.077\n",
            "[   17] loss: 0.085\n",
            "[   18] loss: 0.150\n",
            "[   19] loss: 0.160\n",
            "epoch:  261\n",
            "[    1] loss: 0.109\n",
            "[    2] loss: 0.110\n",
            "[    3] loss: 0.123\n",
            "[    4] loss: 0.097\n",
            "[    5] loss: 0.098\n",
            "[    6] loss: 0.100\n",
            "[    7] loss: 0.088\n",
            "[    8] loss: 0.114\n",
            "[    9] loss: 0.071\n",
            "[   10] loss: 0.076\n",
            "[   11] loss: 0.146\n",
            "[   12] loss: 0.130\n",
            "[   13] loss: 0.116\n",
            "[   14] loss: 0.092\n",
            "[   15] loss: 0.097\n",
            "[   16] loss: 0.094\n",
            "[   17] loss: 0.186\n",
            "[   18] loss: 0.121\n",
            "[   19] loss: 0.551\n",
            "train loss:  0.09747536765301928\n",
            "val loss:  0.18970323726534843\n",
            "epoch:  262\n",
            "[    1] loss: 0.092\n",
            "[    2] loss: 0.107\n",
            "[    3] loss: 0.159\n",
            "[    4] loss: 0.140\n",
            "[    5] loss: 0.083\n",
            "[    6] loss: 0.155\n",
            "[    7] loss: 0.116\n",
            "[    8] loss: 0.112\n",
            "[    9] loss: 0.099\n",
            "[   10] loss: 0.112\n",
            "[   11] loss: 0.081\n",
            "[   12] loss: 0.110\n",
            "[   13] loss: 0.123\n",
            "[   14] loss: 0.145\n",
            "[   15] loss: 0.096\n",
            "[   16] loss: 0.111\n",
            "[   17] loss: 0.129\n",
            "[   18] loss: 0.116\n",
            "[   19] loss: 0.155\n",
            "epoch:  263\n",
            "[    1] loss: 0.091\n",
            "[    2] loss: 0.101\n",
            "[    3] loss: 0.092\n",
            "[    4] loss: 0.084\n",
            "[    5] loss: 0.115\n",
            "[    6] loss: 0.215\n",
            "[    7] loss: 0.151\n",
            "[    8] loss: 0.144\n",
            "[    9] loss: 0.121\n",
            "[   10] loss: 0.123\n",
            "[   11] loss: 0.090\n",
            "[   12] loss: 0.085\n",
            "[   13] loss: 0.111\n",
            "[   14] loss: 0.076\n",
            "[   15] loss: 0.085\n",
            "[   16] loss: 0.067\n",
            "[   17] loss: 0.122\n",
            "[   18] loss: 0.108\n",
            "[   19] loss: 0.165\n",
            "epoch:  264\n",
            "[    1] loss: 0.116\n",
            "[    2] loss: 0.098\n",
            "[    3] loss: 0.115\n",
            "[    4] loss: 0.077\n",
            "[    5] loss: 0.084\n",
            "[    6] loss: 0.103\n",
            "[    7] loss: 0.097\n",
            "[    8] loss: 0.106\n",
            "[    9] loss: 0.105\n",
            "[   10] loss: 0.087\n",
            "[   11] loss: 0.082\n",
            "[   12] loss: 0.093\n",
            "[   13] loss: 0.123\n",
            "[   14] loss: 0.124\n",
            "[   15] loss: 0.097\n",
            "[   16] loss: 0.139\n",
            "[   17] loss: 0.127\n",
            "[   18] loss: 0.129\n",
            "[   19] loss: 0.162\n",
            "epoch:  265\n",
            "[    1] loss: 0.123\n",
            "[    2] loss: 0.116\n",
            "[    3] loss: 0.104\n",
            "[    4] loss: 0.100\n",
            "[    5] loss: 0.092\n",
            "[    6] loss: 0.182\n",
            "[    7] loss: 0.088\n",
            "[    8] loss: 0.101\n",
            "[    9] loss: 0.071\n",
            "[   10] loss: 0.097\n",
            "[   11] loss: 0.155\n",
            "[   12] loss: 0.098\n",
            "[   13] loss: 0.105\n",
            "[   14] loss: 0.100\n",
            "[   15] loss: 0.109\n",
            "[   16] loss: 0.120\n",
            "[   17] loss: 0.070\n",
            "[   18] loss: 0.095\n",
            "[   19] loss: 0.129\n",
            "epoch:  266\n",
            "[    1] loss: 0.116\n",
            "[    2] loss: 0.080\n",
            "[    3] loss: 0.079\n",
            "[    4] loss: 0.100\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.107\n",
            "[    7] loss: 0.097\n",
            "[    8] loss: 0.129\n",
            "[    9] loss: 0.118\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.137\n",
            "[   12] loss: 0.130\n",
            "[   13] loss: 0.152\n",
            "[   14] loss: 0.203\n",
            "[   15] loss: 0.081\n",
            "[   16] loss: 0.127\n",
            "[   17] loss: 0.081\n",
            "[   18] loss: 0.097\n",
            "[   19] loss: 0.164\n",
            "train loss:  0.08525911220075454\n",
            "val loss:  0.15583702735602856\n",
            "best epoch at  266\n",
            "epoch:  267\n",
            "[    1] loss: 0.159\n",
            "[    2] loss: 0.111\n",
            "[    3] loss: 0.102\n",
            "[    4] loss: 0.113\n",
            "[    5] loss: 0.099\n",
            "[    6] loss: 0.089\n",
            "[    7] loss: 0.084\n",
            "[    8] loss: 0.127\n",
            "[    9] loss: 0.089\n",
            "[   10] loss: 0.097\n",
            "[   11] loss: 0.076\n",
            "[   12] loss: 0.216\n",
            "[   13] loss: 0.108\n",
            "[   14] loss: 0.145\n",
            "[   15] loss: 0.105\n",
            "[   16] loss: 0.137\n",
            "[   17] loss: 0.176\n",
            "[   18] loss: 0.104\n",
            "[   19] loss: 0.129\n",
            "epoch:  268\n",
            "[    1] loss: 0.117\n",
            "[    2] loss: 0.129\n",
            "[    3] loss: 0.116\n",
            "[    4] loss: 0.110\n",
            "[    5] loss: 0.115\n",
            "[    6] loss: 0.090\n",
            "[    7] loss: 0.127\n",
            "[    8] loss: 0.126\n",
            "[    9] loss: 0.108\n",
            "[   10] loss: 0.140\n",
            "[   11] loss: 0.142\n",
            "[   12] loss: 0.108\n",
            "[   13] loss: 0.128\n",
            "[   14] loss: 0.070\n",
            "[   15] loss: 0.117\n",
            "[   16] loss: 0.087\n",
            "[   17] loss: 0.096\n",
            "[   18] loss: 0.094\n",
            "[   19] loss: 0.287\n",
            "epoch:  269\n",
            "[    1] loss: 0.095\n",
            "[    2] loss: 0.118\n",
            "[    3] loss: 0.120\n",
            "[    4] loss: 0.100\n",
            "[    5] loss: 0.087\n",
            "[    6] loss: 0.077\n",
            "[    7] loss: 0.087\n",
            "[    8] loss: 0.137\n",
            "[    9] loss: 0.083\n",
            "[   10] loss: 0.115\n",
            "[   11] loss: 0.093\n",
            "[   12] loss: 0.119\n",
            "[   13] loss: 0.136\n",
            "[   14] loss: 0.102\n",
            "[   15] loss: 0.132\n",
            "[   16] loss: 0.131\n",
            "[   17] loss: 0.118\n",
            "[   18] loss: 0.092\n",
            "[   19] loss: 0.176\n",
            "epoch:  270\n",
            "[    1] loss: 0.107\n",
            "[    2] loss: 0.149\n",
            "[    3] loss: 0.107\n",
            "[    4] loss: 0.111\n",
            "[    5] loss: 0.106\n",
            "[    6] loss: 0.118\n",
            "[    7] loss: 0.100\n",
            "[    8] loss: 0.117\n",
            "[    9] loss: 0.105\n",
            "[   10] loss: 0.171\n",
            "[   11] loss: 0.117\n",
            "[   12] loss: 0.068\n",
            "[   13] loss: 0.149\n",
            "[   14] loss: 0.095\n",
            "[   15] loss: 0.112\n",
            "[   16] loss: 0.095\n",
            "[   17] loss: 0.115\n",
            "[   18] loss: 0.090\n",
            "[   19] loss: 0.136\n",
            "epoch:  271\n",
            "[    1] loss: 0.110\n",
            "[    2] loss: 0.125\n",
            "[    3] loss: 0.139\n",
            "[    4] loss: 0.108\n",
            "[    5] loss: 0.099\n",
            "[    6] loss: 0.086\n",
            "[    7] loss: 0.089\n",
            "[    8] loss: 0.088\n",
            "[    9] loss: 0.120\n",
            "[   10] loss: 0.105\n",
            "[   11] loss: 0.113\n",
            "[   12] loss: 0.117\n",
            "[   13] loss: 0.069\n",
            "[   14] loss: 0.094\n",
            "[   15] loss: 0.115\n",
            "[   16] loss: 0.096\n",
            "[   17] loss: 0.108\n",
            "[   18] loss: 0.096\n",
            "[   19] loss: 0.210\n",
            "train loss:  0.0855198098510942\n",
            "val loss:  0.17544636316597462\n",
            "epoch:  272\n",
            "[    1] loss: 0.189\n",
            "[    2] loss: 0.112\n",
            "[    3] loss: 0.123\n",
            "[    4] loss: 0.098\n",
            "[    5] loss: 0.109\n",
            "[    6] loss: 0.245\n",
            "[    7] loss: 0.082\n",
            "[    8] loss: 0.083\n",
            "[    9] loss: 0.092\n",
            "[   10] loss: 0.100\n",
            "[   11] loss: 0.124\n",
            "[   12] loss: 0.097\n",
            "[   13] loss: 0.165\n",
            "[   14] loss: 0.101\n",
            "[   15] loss: 0.076\n",
            "[   16] loss: 0.112\n",
            "[   17] loss: 0.139\n",
            "[   18] loss: 0.103\n",
            "[   19] loss: 0.182\n",
            "epoch:  273\n",
            "[    1] loss: 0.155\n",
            "[    2] loss: 0.080\n",
            "[    3] loss: 0.110\n",
            "[    4] loss: 0.110\n",
            "[    5] loss: 0.108\n",
            "[    6] loss: 0.095\n",
            "[    7] loss: 0.210\n",
            "[    8] loss: 0.154\n",
            "[    9] loss: 0.126\n",
            "[   10] loss: 0.116\n",
            "[   11] loss: 0.076\n",
            "[   12] loss: 0.106\n",
            "[   13] loss: 0.148\n",
            "[   14] loss: 0.076\n",
            "[   15] loss: 0.135\n",
            "[   16] loss: 0.080\n",
            "[   17] loss: 0.077\n",
            "[   18] loss: 0.090\n",
            "[   19] loss: 0.148\n",
            "epoch:  274\n",
            "[    1] loss: 0.086\n",
            "[    2] loss: 0.087\n",
            "[    3] loss: 0.108\n",
            "[    4] loss: 0.097\n",
            "[    5] loss: 0.102\n",
            "[    6] loss: 0.121\n",
            "[    7] loss: 0.106\n",
            "[    8] loss: 0.158\n",
            "[    9] loss: 0.110\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.116\n",
            "[   12] loss: 0.113\n",
            "[   13] loss: 0.092\n",
            "[   14] loss: 0.089\n",
            "[   15] loss: 0.118\n",
            "[   16] loss: 0.150\n",
            "[   17] loss: 0.104\n",
            "[   18] loss: 0.112\n",
            "[   19] loss: 0.112\n",
            "epoch:  275\n",
            "[    1] loss: 0.074\n",
            "[    2] loss: 0.089\n",
            "[    3] loss: 0.075\n",
            "[    4] loss: 0.073\n",
            "[    5] loss: 0.109\n",
            "[    6] loss: 0.123\n",
            "[    7] loss: 0.146\n",
            "[    8] loss: 0.102\n",
            "[    9] loss: 0.111\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.104\n",
            "[   12] loss: 0.109\n",
            "[   13] loss: 0.137\n",
            "[   14] loss: 0.089\n",
            "[   15] loss: 0.101\n",
            "[   16] loss: 0.084\n",
            "[   17] loss: 0.107\n",
            "[   18] loss: 0.085\n",
            "[   19] loss: 0.264\n",
            "epoch:  276\n",
            "[    1] loss: 0.099\n",
            "[    2] loss: 0.087\n",
            "[    3] loss: 0.097\n",
            "[    4] loss: 0.091\n",
            "[    5] loss: 0.077\n",
            "[    6] loss: 0.091\n",
            "[    7] loss: 0.106\n",
            "[    8] loss: 0.141\n",
            "[    9] loss: 0.106\n",
            "[   10] loss: 0.129\n",
            "[   11] loss: 0.096\n",
            "[   12] loss: 0.126\n",
            "[   13] loss: 0.106\n",
            "[   14] loss: 0.145\n",
            "[   15] loss: 0.130\n",
            "[   16] loss: 0.101\n",
            "[   17] loss: 0.076\n",
            "[   18] loss: 0.108\n",
            "[   19] loss: 0.103\n",
            "train loss:  0.08611325615578715\n",
            "val loss:  0.17549079097807407\n",
            "epoch:  277\n",
            "[    1] loss: 0.118\n",
            "[    2] loss: 0.092\n",
            "[    3] loss: 0.106\n",
            "[    4] loss: 0.099\n",
            "[    5] loss: 0.116\n",
            "[    6] loss: 0.096\n",
            "[    7] loss: 0.096\n",
            "[    8] loss: 0.127\n",
            "[    9] loss: 0.113\n",
            "[   10] loss: 0.103\n",
            "[   11] loss: 0.082\n",
            "[   12] loss: 0.087\n",
            "[   13] loss: 0.103\n",
            "[   14] loss: 0.140\n",
            "[   15] loss: 0.166\n",
            "[   16] loss: 0.086\n",
            "[   17] loss: 0.087\n",
            "[   18] loss: 0.101\n",
            "[   19] loss: 0.255\n",
            "epoch:  278\n",
            "[    1] loss: 0.107\n",
            "[    2] loss: 0.101\n",
            "[    3] loss: 0.122\n",
            "[    4] loss: 0.101\n",
            "[    5] loss: 0.156\n",
            "[    6] loss: 0.148\n",
            "[    7] loss: 0.101\n",
            "[    8] loss: 0.106\n",
            "[    9] loss: 0.106\n",
            "[   10] loss: 0.143\n",
            "[   11] loss: 0.204\n",
            "[   12] loss: 0.105\n",
            "[   13] loss: 0.135\n",
            "[   14] loss: 0.111\n",
            "[   15] loss: 0.137\n",
            "[   16] loss: 0.094\n",
            "[   17] loss: 0.107\n",
            "[   18] loss: 0.081\n",
            "[   19] loss: 0.300\n",
            "epoch:  279\n",
            "[    1] loss: 0.103\n",
            "[    2] loss: 0.076\n",
            "[    3] loss: 0.087\n",
            "[    4] loss: 0.129\n",
            "[    5] loss: 0.093\n",
            "[    6] loss: 0.146\n",
            "[    7] loss: 0.111\n",
            "[    8] loss: 0.111\n",
            "[    9] loss: 0.106\n",
            "[   10] loss: 0.259\n",
            "[   11] loss: 0.125\n",
            "[   12] loss: 0.128\n",
            "[   13] loss: 0.134\n",
            "[   14] loss: 0.107\n",
            "[   15] loss: 0.144\n",
            "[   16] loss: 0.115\n",
            "[   17] loss: 0.114\n",
            "[   18] loss: 0.087\n",
            "[   19] loss: 0.126\n",
            "epoch:  280\n",
            "[    1] loss: 0.103\n",
            "[    2] loss: 0.104\n",
            "[    3] loss: 0.089\n",
            "[    4] loss: 0.142\n",
            "[    5] loss: 0.085\n",
            "[    6] loss: 0.106\n",
            "[    7] loss: 0.068\n",
            "[    8] loss: 0.153\n",
            "[    9] loss: 0.097\n",
            "[   10] loss: 0.139\n",
            "[   11] loss: 0.109\n",
            "[   12] loss: 0.097\n",
            "[   13] loss: 0.080\n",
            "[   14] loss: 0.131\n",
            "[   15] loss: 0.114\n",
            "[   16] loss: 0.097\n",
            "[   17] loss: 0.095\n",
            "[   18] loss: 0.114\n",
            "[   19] loss: 0.152\n",
            "epoch:  281\n",
            "[    1] loss: 0.086\n",
            "[    2] loss: 0.100\n",
            "[    3] loss: 0.149\n",
            "[    4] loss: 0.113\n",
            "[    5] loss: 0.085\n",
            "[    6] loss: 0.080\n",
            "[    7] loss: 0.106\n",
            "[    8] loss: 0.131\n",
            "[    9] loss: 0.116\n",
            "[   10] loss: 0.085\n",
            "[   11] loss: 0.102\n",
            "[   12] loss: 0.126\n",
            "[   13] loss: 0.135\n",
            "[   14] loss: 0.097\n",
            "[   15] loss: 0.095\n",
            "[   16] loss: 0.132\n",
            "[   17] loss: 0.116\n",
            "[   18] loss: 0.104\n",
            "[   19] loss: 0.145\n",
            "train loss:  0.09279214336043771\n",
            "val loss:  0.1789360921829939\n",
            "epoch:  282\n",
            "[    1] loss: 0.110\n",
            "[    2] loss: 0.097\n",
            "[    3] loss: 0.140\n",
            "[    4] loss: 0.142\n",
            "[    5] loss: 0.123\n",
            "[    6] loss: 0.093\n",
            "[    7] loss: 0.108\n",
            "[    8] loss: 0.102\n",
            "[    9] loss: 0.098\n",
            "[   10] loss: 0.102\n",
            "[   11] loss: 0.150\n",
            "[   12] loss: 0.094\n",
            "[   13] loss: 0.157\n",
            "[   14] loss: 0.115\n",
            "[   15] loss: 0.094\n",
            "[   16] loss: 0.159\n",
            "[   17] loss: 0.109\n",
            "[   18] loss: 0.108\n",
            "[   19] loss: 0.188\n",
            "epoch:  283\n",
            "[    1] loss: 0.168\n",
            "[    2] loss: 0.083\n",
            "[    3] loss: 0.107\n",
            "[    4] loss: 0.137\n",
            "[    5] loss: 0.118\n",
            "[    6] loss: 0.102\n",
            "[    7] loss: 0.074\n",
            "[    8] loss: 0.116\n",
            "[    9] loss: 0.113\n",
            "[   10] loss: 0.106\n",
            "[   11] loss: 0.096\n",
            "[   12] loss: 0.113\n",
            "[   13] loss: 0.097\n",
            "[   14] loss: 0.122\n",
            "[   15] loss: 0.108\n",
            "[   16] loss: 0.118\n",
            "[   17] loss: 0.122\n",
            "[   18] loss: 0.091\n",
            "[   19] loss: 0.199\n",
            "epoch:  284\n",
            "[    1] loss: 0.107\n",
            "[    2] loss: 0.119\n",
            "[    3] loss: 0.158\n",
            "[    4] loss: 0.102\n",
            "[    5] loss: 0.096\n",
            "[    6] loss: 0.127\n",
            "[    7] loss: 0.072\n",
            "[    8] loss: 0.108\n",
            "[    9] loss: 0.076\n",
            "[   10] loss: 0.180\n",
            "[   11] loss: 0.098\n",
            "[   12] loss: 0.073\n",
            "[   13] loss: 0.104\n",
            "[   14] loss: 0.108\n",
            "[   15] loss: 0.158\n",
            "[   16] loss: 0.116\n",
            "[   17] loss: 0.126\n",
            "[   18] loss: 0.116\n",
            "[   19] loss: 0.215\n",
            "epoch:  285\n",
            "[    1] loss: 0.117\n",
            "[    2] loss: 0.092\n",
            "[    3] loss: 0.144\n",
            "[    4] loss: 0.073\n",
            "[    5] loss: 0.093\n",
            "[    6] loss: 0.093\n",
            "[    7] loss: 0.169\n",
            "[    8] loss: 0.106\n",
            "[    9] loss: 0.125\n",
            "[   10] loss: 0.111\n",
            "[   11] loss: 0.106\n",
            "[   12] loss: 0.148\n",
            "[   13] loss: 0.130\n",
            "[   14] loss: 0.093\n",
            "[   15] loss: 0.098\n",
            "[   16] loss: 0.140\n",
            "[   17] loss: 0.151\n",
            "[   18] loss: 0.085\n",
            "[   19] loss: 0.181\n",
            "epoch:  286\n",
            "[    1] loss: 0.152\n",
            "[    2] loss: 0.102\n",
            "[    3] loss: 0.140\n",
            "[    4] loss: 0.099\n",
            "[    5] loss: 0.131\n",
            "[    6] loss: 0.089\n",
            "[    7] loss: 0.104\n",
            "[    8] loss: 0.129\n",
            "[    9] loss: 0.093\n",
            "[   10] loss: 0.140\n",
            "[   11] loss: 0.096\n",
            "[   12] loss: 0.131\n",
            "[   13] loss: 0.099\n",
            "[   14] loss: 0.109\n",
            "[   15] loss: 0.110\n",
            "[   16] loss: 0.102\n",
            "[   17] loss: 0.105\n",
            "[   18] loss: 0.075\n",
            "[   19] loss: 0.160\n",
            "train loss:  0.09285225971218418\n",
            "val loss:  0.1733917836099863\n",
            "epoch:  287\n",
            "[    1] loss: 0.088\n",
            "[    2] loss: 0.129\n",
            "[    3] loss: 0.130\n",
            "[    4] loss: 0.133\n",
            "[    5] loss: 0.143\n",
            "[    6] loss: 0.094\n",
            "[    7] loss: 0.107\n",
            "[    8] loss: 0.126\n",
            "[    9] loss: 0.120\n",
            "[   10] loss: 0.148\n",
            "[   11] loss: 0.109\n",
            "[   12] loss: 0.098\n",
            "[   13] loss: 0.123\n",
            "[   14] loss: 0.110\n",
            "[   15] loss: 0.096\n",
            "[   16] loss: 0.169\n",
            "[   17] loss: 0.110\n",
            "[   18] loss: 0.117\n",
            "[   19] loss: 0.322\n",
            "epoch:  288\n",
            "[    1] loss: 0.126\n",
            "[    2] loss: 0.099\n",
            "[    3] loss: 0.101\n",
            "[    4] loss: 0.100\n",
            "[    5] loss: 0.149\n",
            "[    6] loss: 0.120\n",
            "[    7] loss: 0.101\n",
            "[    8] loss: 0.102\n",
            "[    9] loss: 0.094\n",
            "[   10] loss: 0.092\n",
            "[   11] loss: 0.098\n",
            "[   12] loss: 0.086\n",
            "[   13] loss: 0.143\n",
            "[   14] loss: 0.093\n",
            "[   15] loss: 0.102\n",
            "[   16] loss: 0.111\n",
            "[   17] loss: 0.159\n",
            "[   18] loss: 0.108\n",
            "[   19] loss: 0.535\n",
            "epoch:  289\n",
            "[    1] loss: 0.122\n",
            "[    2] loss: 0.095\n",
            "[    3] loss: 0.110\n",
            "[    4] loss: 0.084\n",
            "[    5] loss: 0.110\n",
            "[    6] loss: 0.134\n",
            "[    7] loss: 0.098\n",
            "[    8] loss: 0.103\n",
            "[    9] loss: 0.104\n",
            "[   10] loss: 0.076\n",
            "[   11] loss: 0.080\n",
            "[   12] loss: 0.128\n",
            "[   13] loss: 0.147\n",
            "[   14] loss: 0.085\n",
            "[   15] loss: 0.136\n",
            "[   16] loss: 0.077\n",
            "[   17] loss: 0.074\n",
            "[   18] loss: 0.102\n",
            "[   19] loss: 0.269\n",
            "epoch:  290\n",
            "[    1] loss: 0.132\n",
            "[    2] loss: 0.104\n",
            "[    3] loss: 0.123\n",
            "[    4] loss: 0.123\n",
            "[    5] loss: 0.097\n",
            "[    6] loss: 0.113\n",
            "[    7] loss: 0.110\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2G2aIj04AYR"
      },
      "source": [
        "np.save(os.path.join(save_path, \"logs.npy\"), logs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B8kvMbkg5kZl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}